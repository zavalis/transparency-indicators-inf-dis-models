<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2022-03-11T00:48:14Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:8522122" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:8522122</identifier>
        <datestamp>2021-10-18</datestamp>
        <setSpec>pheelsevier</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3-mathml3.xsd" article-type="research-article" dtd-version="1.3">
          <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
            <restricted-by>pmc</restricted-by>
          </processing-meta>
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Knowl Based Syst</journal-id>
              <journal-id journal-id-type="iso-abbrev">Knowl Based Syst</journal-id>
              <journal-title-group>
                <journal-title>Knowledge-Based Systems</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0950-7051</issn>
              <issn pub-type="epub">1872-7409</issn>
              <publisher>
                <publisher-name>Elsevier B.V.</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC8522122</article-id>
              <article-id pub-id-type="pmcid">PMC8522122</article-id>
              <article-id pub-id-type="pmc-uid">8522122</article-id>
              <article-id pub-id-type="pmid">34690447</article-id>
              <article-id pub-id-type="pii">S0950-7051(21)00679-1</article-id>
              <article-id pub-id-type="doi">10.1016/j.knosys.2021.107417</article-id>
              <article-id pub-id-type="publisher-id">107417</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Hybrid deep learning of social media big data for predicting the evolution of COVID-19 transmission</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" id="au000001">
                  <name>
                    <surname>Chew</surname>
                    <given-names>Alvin Wei Ze</given-names>
                  </name>
                  <xref rid="aff1" ref-type="aff">a</xref>
                  <xref rid="fn1" ref-type="fn">1</xref>
                </contrib>
                <contrib contrib-type="author" id="au000002">
                  <name>
                    <surname>Pan</surname>
                    <given-names>Yue</given-names>
                  </name>
                  <xref rid="aff2" ref-type="aff">b</xref>
                  <xref rid="fn1" ref-type="fn">1</xref>
                </contrib>
                <contrib contrib-type="author" id="au000003">
                  <name>
                    <surname>Wang</surname>
                    <given-names>Ying</given-names>
                  </name>
                  <xref rid="aff3" ref-type="aff">c</xref>
                </contrib>
                <contrib contrib-type="author" id="au000004">
                  <name>
                    <surname>Zhang</surname>
                    <given-names>Limao</given-names>
                  </name>
                  <xref rid="aff3" ref-type="aff">c</xref>
                  <xref rid="cor1" ref-type="corresp">⁎</xref>
                </contrib>
                <aff id="aff1"><label>a</label>Bentley Systems Research Office, 1 Harbourfront Pl, HarbourFront Tower One, Singapore 098633, Singapore</aff>
                <aff id="aff2"><label>b</label>Shanghai Key Laboratory for Digital Maintenance of Buildings and Infrastructure, Department of Civil Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai, China</aff>
                <aff id="aff3"><label>c</label>School of Civil and Environmental Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore 639798, Singapore</aff>
              </contrib-group>
              <author-notes>
                <corresp id="cor1"><label>⁎</label>Corresponding author.</corresp>
                <fn id="fn1">
                  <label>1</label>
                  <p id="d1e2741">These authors contributed to the work equally and should be regarded as co-first authors.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="pmc-release">
                <day>24</day>
                <month>8</month>
                <year>2021</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
              <pub-date pub-type="ppub">
                <day>5</day>
                <month>12</month>
                <year>2021</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>24</day>
                <month>8</month>
                <year>2021</year>
              </pub-date>
              <volume>233</volume>
              <fpage>107417</fpage>
              <lpage>107417</lpage>
              <history>
                <date date-type="received">
                  <day>1</day>
                  <month>3</month>
                  <year>2021</year>
                </date>
                <date date-type="rev-recd">
                  <day>14</day>
                  <month>7</month>
                  <year>2021</year>
                </date>
                <date date-type="accepted">
                  <day>18</day>
                  <month>8</month>
                  <year>2021</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2021 Elsevier B.V. All rights reserved.</copyright-statement>
                <copyright-year>2021</copyright-year>
                <copyright-holder>Elsevier B.V.</copyright-holder>
                <license>
                  <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
                </license>
              </permissions>
              <abstract id="d1e2747">
                <p>In this study, a hybrid deep-learning model termed as ODANN, built upon neural networks (NN) coupled with data assimilation and natural language processing (NLP) features extraction methods, has been constructed to concurrently process daily COVID-19 time-series records and large volumes of COVID-19 related Twitter data, as representative of the global community’s aggregated emotional responses towards the current pandemic, to model the growth rate in the number of confirmed COVID-19 cases globally via a proposed G parameter. Overall, there were 3 key components to ODANN’s development phase, namely: (i) data hydration and pre-processing were performed on COVID-19 related Twitter data ranging between 23 January 2020 and 10 May 2020, which amounted to over 100 million Tweets written in English language; (ii) multiple NLP features extraction methods were subsequently leveraged to encode the hydrated Twitter data into useful semantic word vectors for training ODANN under an optimal set of hyperparameters; and (iii) historical time-series data of defined characteristics were also assimilated into ODANN’s selected hidden layer(s) to model the G parameter daily with a lead-time of 1 day. By far, our experimental results demonstrated that by adopting a rolling time-window size of 5 days, with respect to the number of historical time-series records for assimilating different data features, enabled ODANN to outperform other traditional time-series models and recent studies, in terms of the computed RMSE and MAE scores attained from the model’s testing step. Overall, the summarized results from ODANN demonstrated its competitive edge in modelling and forecasting the growth rate in the number of COVID-19 cases globally.</p>
              </abstract>
              <kwd-group id="d1e2753">
                <title>Keywords</title>
                <kwd>COVID-19</kwd>
                <kwd>Natural language processing</kwd>
                <kwd>Time-series prediction</kwd>
                <kwd>Deep learning</kwd>
                <kwd>Big-data</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec id="sec1">
              <label>1</label>
              <title>Introduction</title>
              <p id="d1e2778">Coronavirus disease 2019 (COVID-19) can easily be spread via coughs or sneeze droplets by entering one’s body through their eyes, nose, and mouth <xref rid="b1" ref-type="bibr">[1]</xref>. When a person communicates with an infected person within a distance of 1.8 m, the infection likelihood of the former individual augments <xref rid="b2" ref-type="bibr">[2]</xref>. The highly infectious nature of the virus has since accelerated its spread globally. On January 30, 2020, the World Health Organization (WHO) declared the COVID-19 outbreak as a Public Health Emergency of International Concern. During March 2020, the WHO officially announced COVID-19 as a global pandemic, hence aggravating healthcare and travel concerns globally. Due to its highly infectious nature, the number of confirmed COVID-19 active and death cases have continued to increase till today, even as vaccination rates gradually ramp up in many countries, hence posing intractable challenges to the global health, environment, and economy <xref rid="b3" ref-type="bibr">[3]</xref>, <xref rid="b4" ref-type="bibr">[4]</xref>. Within a short time span, COVID-19 has already been considered to be one of the biggest crises faced by humanity in the 21st century <xref rid="b5" ref-type="bibr">[5]</xref>.</p>
              <p id="d1e2797">At present, the global community is closely tracking the evolution of COVID-19 by reporting the daily number of infected persons, recovered persons, and deaths, resulting in available datasets on the internet for research investigations to better understand the unknown virus qualitatively and quantitatively. An important problem statement can thus be formulated, namely: how can we estimate the number of confirmed cases globally, with suitable lead-time and a level of confidence, by analysing historical data pertaining to the growth rate in the number of confirmed COVID-19 cases together with relevant social- environment factors. By modelling and forecasting the spread of COVID-19 over time on a global scale, decision-makers around the world can then undertake appropriate preventative and response measures which include, but not limited to, social distancing, self-quarantine, travel restriction, lockdown, etc., to better manage the virus spread over time. To build towards this target objective, statistical and mathematical models can serve as useful predictive tools to model the temporal spread of COVID-19 globally.</p>
              <p id="d1e2809">Regression analysis, which is a very conventional, but yet useful, technique in the machine learning (ML) domain, is generally useful for time-series predictions in many diverse domains. Hence, it can be considered helpful to decision-makers to forecast the spread of COVID-19 over time on the basis that there is some level of correlation between the virus’ transmission rate and a range of controlled and/or uncontrolled factors <xref rid="b6" ref-type="bibr">[6]</xref>. For example, the well-known auto-regression integrated moving average (ARIMA) model is expected to estimate the trend and seasonal profiles of the virus spread <xref rid="b7" ref-type="bibr">[7]</xref>. On the other hand, the susceptible–infectious-removed (SIR) epidemiological model, which is built upon the compartments of susceptible (S), infected (I), and removed (R) individuals, may also be useful to model the transmission rate of COVID-19 <xref rid="b8" ref-type="bibr">[8]</xref>. However, SIR and its variants set the rates of transmission and removal to be constants, hence hindering their abilities to adapt to uncontrollable external conditions/factors <xref rid="b9" ref-type="bibr">[9]</xref>. At present, the global transmission rate of COVID-19 exhibits complex patterns which are closely related to a range of social, governmental, and environmental factors. The large multitude of factors, however, can be difficult to be leverage effectively, without any significant data pre-processing, to model and forecast the number of confirmed COVID-19 cases over time. Besides the above-mentioned traditional quantitative methods available, deep learning approaches can also serve as good alternatives by learning from historical data and other forms of big-data information to maximize the prediction model’s accuracy in forecasting the transmission rate of COVID-19, which in turn can be useful to support clinical and academic research for COVID-19 in the foreseeable future <xref rid="b10" ref-type="bibr">[10]</xref>.</p>
              <p id="d1e2840">One important data source pertaining to COVID-19 can be derived from social media platforms which can be helpful to investigate the society’s emotional responses towards the current pandemic <xref rid="b11" ref-type="bibr">[11]</xref>, <xref rid="b12" ref-type="bibr">[12]</xref>. For example, Twitter enables netizens to post messages and retweet contents pertaining to all forms of issues occurring globally. The platform also provides users instant access to millions of short messages (public responses, opinions, etc.) towards any specific topic of their interest <xref rid="b13" ref-type="bibr">[13]</xref>. In today’s context, there are currently more than 300 million monthly active users on Twitter and an average of 500 million tweets are being made daily <xref rid="b14" ref-type="bibr">[14]</xref>. An advantage of using Twitter information lies in its real-time deployment and publicly available information, which are tagged by different time zones and geographic locations, for easy access in most parts of the world. Not surprisingly, due to the large volume of data available on Twitter’s platform, there have already been research works done in leveraging Twitter data for disease surveillance, which can subsequently provide important insights into public health conditions and return instant feedback to healthcare professionals and the different stakeholders.</p>
              <p id="d1e2854">ML methods have since been incorporated into Twitter-based healthcare framework to monitor, analyse, and predict the outbreak of different types of diseases in the near real-time context <xref rid="b15" ref-type="bibr">[15]</xref>. For example, Signorini et al. <xref rid="b16" ref-type="bibr">[16]</xref> examined embedded information in H1N1-related tweets using support vector regression (SVR) to quantify the public sentiments towards the flu virus in the United States and estimate the weekly influenza-like illness level with a reasonably low error percentage of 0.28% on average. Hirose and Wang <xref rid="b17" ref-type="bibr">[17]</xref> applied multiple linear regression (MLP) methods with ridge regularization on Twitter data combined with other influenza-like data features derived from the Centres for Disease Control and Prevention (CDC). The model predicted the spread of influenza with a resulting root mean square error (RMSE) of 0.002 on average. Santos and Matos <xref rid="b18" ref-type="bibr">[18]</xref> built a Naïve Bayes classifier and MLP models to analyse tweets and web search queries to forecast the incidence rate of influenza in Portugal, where the prediction results have an average correlation ratio of 0.849 with the available ground truth data. It has also been demonstrated that analysing Twitter data offers valuable opportunities to track influenza’s infection rate in near real-time and to quantitatively understand the potential upward/downward trends. The predictions can subsequently be useful to generate early warnings to improve both clinical and public health responses. Apart from the conventional flu virus, Twitter data has also been leveraged to forecast the outbreak trends of other infectious diseases such as Zika virus, Malaria, Ebola, and others <xref rid="b19" ref-type="bibr">[19]</xref>. The above-mentioned studies have thus indicated the possibility of combining Twitter data with suitable ML models to investigate the infection rate of COVID-19 globally.</p>
              <p id="d1e2911">In this paper, we propose a novel prediction model, termed as optimized data assimilated neural network (ODANN), which unifies the use of natural language processing (NLP) <xref rid="b20" ref-type="bibr">[20]</xref>, for generating useful input features layer for neural networks (NN), and data assimilation component for concatenating time-series data into optimal location(s) of the NN’s hidden layers to forecast the global growth rate in the total number of confirmed COVID-19 cases with a good level of predictive accuracy. Modelling of the cases’ growth rate is achieved via a G parameter which considers the total reported number of confirmed COVID-19 cases on a rolling-forward daily basis. In the model development of ODANN, features extraction methods via NLP are leveraged to extract high-level numerical features from vast volumes of Twitter data (millions in exceedance) associated with COVID-19 since late January 2020. The encoded features from the available Twitter data are then assimilated with historical time-series records for the proposed G parameter by optimally concatenating the features with selected hidden layers of a personalized deep NN model to forecast the G parameter with a defined lead-time of 1 day. At present, our study demonstrates that the resulting time-series predictions for the G parameter from our proposed ODANN model are generally more accurate than the corresponding results derived from other classical time-series prediction models on the same dataset being investigated. The overall novelty of ODANN lies in its end-to-end model framework capable of processing large volumes of Twitter data, as representative of the general community’s emotional responses towards COVID-19, to construct useful input features to train deep learning models. The hidden layers of the deep NNs are also carefully optimized, in terms of the selected number of neurons and their placement locations, to assimilate with other important time-series data features to maximize the resulting model’s predictive accuracy to forecast the proposed G parameter with a lead-time of 1 day. We are hopeful that ODANN can serve as an alternative prediction model to assist the research community to enhance existing mechanism studies for COVID-19, and subsequently provide useful insights into the nuanced relationship(s) between the global spread of COVID-19 over time and the community’s aggregated emotional responses towards the virus.</p>
              <p id="d1e2917">This paper is structured as follows. Section <xref rid="sec2" ref-type="sec">2</xref> reviews the previous research works done, using ML and/or deep learning (DL), to analyse Twitter data for modelling the temporal transmission behaviour of COVID-19. In Section <xref rid="sec3" ref-type="sec">3</xref>, we describe our proposed engineering workflow for ODANN in detail, followed by running the constructed prediction model via several computational experiments (model training, validation, and testing) in Section <xref rid="sec4" ref-type="sec">4</xref> to evaluate the model’s predictive accuracy on a defined time-series dataset for the G parameter and COVID-19 related Twitter data. In Section <xref rid="sec5" ref-type="sec">5</xref>, we compare the results derived from ODANN with that of other conventional time-series prediction models, coupled with analysing the models’ predictive robustness in handling missing data conditions. In addition, we also compare ODANN’s predictive capability with other relevant research studies for the same research objective as outlined in this study. Finally, Section <xref rid="sec6" ref-type="sec">6</xref> succinctly summarizes the key results derived from our analysis using ODANN by far, and the future works to be involved in the same direction.</p>
            </sec>
            <sec id="sec2">
              <label>2</label>
              <title>Literature review</title>
              <p id="d1e2950">The growing availability of COVID-19 related data provides useful quantitative and qualitative information for researchers to leverage on for modelling the virus’ transmission behaviour over time. With the advantages provided by machine learning (e.g., scalability, faster computations, etc.) in the field of healthcare <xref rid="b21" ref-type="bibr">[21]</xref>, researchers across the world have been developing various ML-based frameworks to manage the unprecedented COVID-19 crisis. For example, Google’s DeepMind applied an improved AlphaFold system to model the uncharacterized protein structure related to SARS-CoV-2 and generate accurate 3D models to quantitatively understand complex functions of the underlying proteins. Beck et al. <xref rid="b22" ref-type="bibr">[22]</xref> designed a drug-target interaction prediction model via deep learning (DL) to efficiently identify the existing drug candidates that can be re-purposed to disrupt viral proteins in the COVID-19 virus.</p>
              <p id="d1e2969">On the clinical scale, deep convolutional neural networks (DCNN) are developed to automatically identify COVID-19’s infections and regions of interest (ROI) via medical images in an economical and efficient manner <xref rid="b23" ref-type="bibr">[23]</xref>, which can be especially helpful for the less-developed communities. Pre-trained DL models such as ResNet, U-Net, VB-Net, and others, have also been employed for X-ray or CT image segmentation in COVID-19 applications <xref rid="b23" ref-type="bibr">[23]</xref>, which have generated promising results in distinguishing COVID-19 from community-acquired pneumonia by segmenting and locating infected lung regions and lesions, and hence tracking and evaluating the virus’ severity and evolution over time <xref rid="b24" ref-type="bibr">[24]</xref>. On the other hand, as numerical data pertaining to the number of suspected, confirmed, cured, and death COVID-19 cases, passengers travel trajectories, etc., are being shared widely on the internet daily, traditional and novel ML methods can be applied to learn from the vastly available information to forecast the transmission of COVID-19 <xref rid="b25" ref-type="bibr">[25]</xref>.</p>
              <p id="d1e2988">For example, researchers have developed multiple prediction models to forecast the virus’ future trend behaviour and evaluate the impacts of COVID-19 by estimating key indicators/features associated with the virus which include, but not limited to, its prevalence, mortality rate, recovery rate, etc. For example, Rustam et al. <xref rid="b26" ref-type="bibr">[26]</xref> trained four ML regression models, namely the linear regression (LR), least absolute shrinkage and selection operator (LASSO), support vector machine (SVM), and exponential smoothing (ES), to perform reliable forecasting on the global numbers of confirmed, recovered and death cases using a 10-days lead-time. The authors, however, underlined the importance to augment the amount of training data to improve their models’ resulting predictions. Yesilkanat <xref rid="b27" ref-type="bibr">[27]</xref> adopted the random forest (RF) algorithm to estimate the daily increase in the number of confirmed COVID-19 cases globally with estimated coefficient of determination values ranging between 0.843 and 0.995. Researchers <xref rid="b28" ref-type="bibr">[28]</xref>, <xref rid="b29" ref-type="bibr">[29]</xref>, <xref rid="b30" ref-type="bibr">[30]</xref> have also leveraged on long-short-term-memory (LSTM) neural network, which is capable to model long sequential time-series data, and capture long-term dependencies at the same time, to deliver high-quality prediction results on the number of confirmed COVID-19 cases over time. However, the present daily numbers of COVID-19 cases and deaths may not sufficiently large to maximize the forecasting capability of LSTM model. Going forward, it is thus desirable to apply alternative DL methods to fully leverage on the widely available COVID-19 related data to bridge the gap between intelligent computing and COVID-19 prognosis, which can subsequently be useful to assist the different stakeholders to better manage the current pandemic.</p>
              <p id="d1e3002">The wealth of publicly available Twitter data can be actively collected via Twitter’s streaming API and Tweepy on a daily basis <xref rid="b31" ref-type="bibr">[31]</xref>, <xref rid="b32" ref-type="bibr">[32]</xref>, which thus offers opportunities for large-scale data mining of the global community’s emotional responses towards COVID-19 and subsequently enabling us to quantitatively investigate any relationship between the trending emotions and the proposed G parameter daily with defined lead-times on a global scale. As an example, Lwin et al. <xref rid="b33" ref-type="bibr">[33]</xref> examined the public’s emotions with over 20 million Twitter posts worldwide during the early outbreak of COVID-19 globally, where the reported findings can be useful to support government measures to maintain the general public’s mental wellbeing in the present pandemic situation. Park et al. <xref rid="b34" ref-type="bibr">[34]</xref> performed network and context analysis using keywords from COVID-19 associated Twitter data, in Korean language, to track information, discover conversation patterns, and capture the public’s interests and perceptions towards COVID-19. Since Twitter data can provide aggregated insights into individuals’ emotional responses (or attitudes) towards the global transmission rate of COVID-19, the large data availability can thus be numerically processed to forecast the proposed G parameter by modelling the dimensional effects due to the community’s aggregated emotions over time. An important pointer to highlight is that we would expect the raw Twitter data to contain ‘noises’ such as wrong spellings, punctuations, and differing expressions for underlying common messages. Hence, significant data pre-processing, combined with NLP features extraction methods, is required to minimize the inherent ‘noises’ and encapsulate the nuanced relationship between the different words available and the respective context, and vice versa. An interesting, but physically intuitive, observation is that the rising number of confirmed COVID-19 cases globally is correlated to Twitter data in the negative emotional realm as the community generally become anxious about the deteriorating social conditions since the virus’s inception <xref rid="b35" ref-type="bibr">[35]</xref>. The key research question is how we can maximize the value of Twitter data to potentially quantify the complex relationship between the global transmission patterns of COVID-19, i.e., the G parameter, and the relevant emotional responses from the global community.</p>
              <p id="d1e3021">While large volumes of Twitter data, reflecting the emotional state of the public towards COVID-19, can easily be accessed, the available data requires significant data pre-processing to minimize the inherent noises as discussed. In this aspect, Zheng et al. <xref rid="b36" ref-type="bibr">[36]</xref> combined semantic features derived from online news and reports into LSTM neural networks, which could effectively minimize the error in estimating the overall infection rate via an improved susceptible–infected (ISI) model. At the same time, Hazarika and Gupta <xref rid="b37" ref-type="bibr">[37]</xref> also recently verified that higher data dimensions, which encompass the social, economic, or/and environmental conditions, derived from the pre-processing step can be useful to improve the prediction model’s resulting accuracy in forecasting the transmission rate of COVID-19 globally.</p>
              <p id="d1e3031">The above-discussed information from the present literature generally underlines the rationality and feasibility of combining large-scale Twitter data and historical time-series records of defined data characteristics for useful COVID-19 related predictions. Hence, this study proposes the ODANN model which consists of an engineering workflow process to systematically extract high-level encoded features from pre-processed COVID-19 related Twitter data, followed by optimally assimilating the model’s hidden layers with relevant historical time-series records to model and forecast the proposed G parameter over time. To the best of our knowledge, the formulated workflow of ODANN has not been developed in the literature by far, and we are thus hopeful that the ODANN can serve as an alternative prediction model to build towards reliable predictions in the number of confirmed COVID-19 cases globally on a rolling-forward daily basis using social media big-data with defined lead-times. The forecasting results could potentially assist the different stakeholders to implement appropriate measures to manage COVID-19 on a whole as the virus has been widely expected to become an endemic in the foreseeable future.</p>
            </sec>
            <sec id="sec3">
              <label>3</label>
              <title>Methodology</title>
              <p id="d1e3038"><xref rid="fig1" ref-type="fig">Fig. 1</xref> illustrates the overall workflow for ODANN’s developmental phase which involves the following systematic tasks, namely: (1) processing more than 100 million of COVID-19 related Twitter data, written in English language, via suitable NLP features extraction methods for constructing useful numerically encoded features; (2) training of deep NN (DNN) model by leveraging on the encoded features derived from the preceding Task 1, and assimilating with historical time-series records for the proposed G parameter and/or other data features during the model training step; and (3) comparing results derived from a trained ODANN prediction model, with its optimized hyperparameters, with other traditional time-series prediction models to forecast the same G parameter on a daily basis with a defined lead-time. At the same time, a pseudo-code in Algorithm 1 is provided to summarize the key computational steps involved in ODANN’s proposed workflow.</p>
              <p id="d1e3043">
                <fig id="dfig1">
                  <graphic xlink:href="fx1_lrg"/>
                </fig>
              </p>
              <p id="d1e3048">
                <fig id="fig1">
                  <label>Fig. 1</label>
                  <caption>
                    <p>ODANN’s workflow to integrate Twitter data and other historical time-series records to model and forecast G parameter over time, coupled with comparison of model’s resulting accuracy with alternative time-series prediction methods.</p>
                  </caption>
                  <graphic xlink:href="gr1_lrg"/>
                </fig>
              </p>
              <sec id="sec3.1">
                <label>3.1</label>
                <title>Data hydration and pre-processing</title>
                <p id="d1e3056">Since the inception of COVID-19 in Wuhan, China, in December 2019, over 100 million of related Tweets, have been made on the internet propagating the keywords of <bold>“covid”</bold>, <bold>“coronavirus”</bold>, <bold>“ncov19”</bold>, <bold>“ncov2019”</bold> and etc. The millions of Tweets, written in the English language, were extracted from an open-source Kaggle source (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/lopezbec/covid19-tweets-dataset" id="interref1">https://www.kaggle.com/lopezbec/covid19-tweets-dataset</ext-link>) for the modelling analysis in this study. In the selected open-source dataset, the total amount of Tweets written in English constitutes to around 60% of the total available Tweets collated. We analysed the available Tweets for the period between 23 January 2020 and 10 May 2020 for extensiveness, while also accounting for the computational resources (memory storage) limitation in our present study. We note that the amount of COVID-19 Tweets increased exponentially on Twitter’s platform from March 2020 and thereafter as illustrated in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. The training phase of ODANN was performed using Azure’s NC12sV2 N-Series virtual machine (VM) which has 12vCPUs, 224GiB RAM and 2 in-built Tesla P100 GPU cards.</p>
                <p id="d1e3086">The aggregated pool of Tweets (made in English language) was considered to model the proposed G-parameter over time on a global scale, instead of localizing into specific countries, for the following reasons:</p>
                <p id="d1e3088">
                  <list list-type="simple" id="d1e3090">
                    <list-item id="lst1">
                      <label>•</label>
                      <p id="d1e3094">In the extracted Twitter data, only the geographic information of the re-Tweets can be obtained, however, with minimum information about the Tweets contributors in terms of age, race, etc. The geographic information of the re-Tweets revealed the dominant terms of <bold>“global”, “worldwide”, “around the world”, “everywhere” and “</bold>
<bold>linkedin:”</bold> as summarized in <xref rid="tbl1" ref-type="table">Table 1</xref>. Hence, this observation enables us to first investigate any aggregated correlation between the complexity of the Tweets made by the global community and the proposed G-parameter which quantifies the growth rate in the confirmed number of COVID-19 cases on the global scale.</p>
                    </list-item>
                    <list-item id="lst2">
                      <label>•</label>
                      <p id="d1e3108">The top 10 countries (since 2020) having the highest numbers of reported confirmed COVID-19 cases included USA, India, United Kingdom, Brazil, France, etc., were also among the list of top contributors to the re-Tweets as shown in <xref rid="tbl1" ref-type="table">Table 1</xref>. This thus underlines our approach to first aggregate all available Tweets, without any prior filtering with respect to any countries, to correlate with the G- parameter over time on the global scale.</p>
                    </list-item>
                  </list>
                </p>
                <p id="d1e3114">As discussed earlier, the large amount of Tweet data available serves as useful data to analyse and quantify the aggregated emotional responses of the general community towards COVID-19 on the global scale. The proposed G parameter, which relates to the global growth rate in the total number of confirmed COVID-19 cases on a rolling-forward daily basis, can be expressed as follows: <disp-formula id="fd1"><label>(1)</label><mml:math id="d1e3136" display="block" altimg="si1.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>100</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e3189" display="inline" altimg="si2.svg"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the global number of confirmed COVID-19 cases at time <inline-formula><mml:math id="d1e3199" display="inline" altimg="si3.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="d1e3209" display="inline" altimg="si4.svg"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> represents the global number of COVID-19 cases at time <inline-formula><mml:math id="d1e3224" display="inline" altimg="si5.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> from the previous day. For example, the G value on 31 January 2020 (<inline-formula><mml:math id="d1e3239" display="inline" altimg="si6.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>) is computed by dividing the difference between the respective number of COVID-19 confirmed cases on 31 January (<inline-formula><mml:math id="d1e3245" display="inline" altimg="si6.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>) and 30 January 2020 (<inline-formula><mml:math id="d1e3252" display="inline" altimg="si8.svg"><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) by the number of COVID-19 confirmed cases from the latter’s date. The number of confirmed COVID-19 cases recorded on the global context, since 31 December 2019, were also extracted from another open-source database (<ext-link ext-link-type="uri" xlink:href="https://ourworldindata.org/coronavirus-data" id="interref2">https://ourworldindata.org/coronavirus-data</ext-link>). Our analysis instead models the daily growth rate, on a rolling-forward basis, in the proposed G parameter for the period between 25 January 2020 and 11 May 2020 (see <xref rid="fig3" ref-type="fig">Fig. 3</xref>). Note that we started on 25 January 2020 to forecast the G value in order to maintain a lead-time of 1 day in leveraging the pre-processed Twitter data from the previous day, i.e. 23 January 2020, for the modelling step. We also note that we stored the full historical time-records for the G parameter since 18 January 2020 for the analysis via a rolling time-window size for data assimilation component in ODANN which will be further discussed later.<fig id="fig2"><label>Fig. 2</label><caption><p>Number of the collected Tweets between 23 January 2020 and 10 May 2020 in this study.</p></caption><graphic xlink:href="gr2_lrg"/></fig><table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Top 30 contributors of COVID-19 related re-Tweets in the extracted dataset between 23 January 2020 and 10 May 2020.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Location name</th><th align="left">Quantity</th></tr></thead><tbody><tr><td align="left">Washington, DC</td><td align="left">1 898 553</td></tr><tr><td align="left">New York, NY</td><td align="left">1 049 229</td></tr><tr><td align="left">India</td><td align="left">704 448</td></tr><tr><td align="left"><bold>Global</bold><xref rid="tblfn1a" ref-type="table-fn">a</xref></td><td align="left">635 534</td></tr><tr><td align="left">London</td><td align="left">570 490</td></tr><tr><td align="left">Washington, D.C.</td><td align="left">539 826</td></tr><tr><td align="left">France</td><td align="left">462 137</td></tr><tr><td align="left">New York</td><td align="left">461 072</td></tr><tr><td align="left">United States</td><td align="left">454 821</td></tr><tr><td align="left">New York city</td><td align="left">426 335</td></tr><tr><td align="left">Madrid</td><td align="left">397 464</td></tr><tr><td align="left">Brasil</td><td align="left">372 637</td></tr><tr><td align="left">London, UK</td><td align="left">342 182</td></tr><tr><td align="left">Paris, France</td><td align="left">340 845</td></tr><tr><td align="left">London, England</td><td align="left">337 515</td></tr><tr><td align="left">Los angeles, CA</td><td align="left">318 437</td></tr><tr><td align="left">USA</td><td align="left">255 963</td></tr><tr><td align="left">Venezuela</td><td align="left">252 693</td></tr><tr><td align="left">Paris</td><td align="left">246 379</td></tr><tr><td align="left">United Kingdom</td><td align="left">240 403</td></tr><tr><td align="left">New York, USA</td><td align="left">235 346</td></tr><tr><td align="left">New Delhi, India</td><td align="left">234 865</td></tr><tr><td align="left">Argentina</td><td align="left">230 770</td></tr><tr><td align="left"><bold>Worldwide</bold><xref rid="tblfn1a" ref-type="table-fn">a</xref></td><td align="left">222 662</td></tr><tr><td align="left"><bold>Linkedin:</bold><xref rid="tblfn1a" ref-type="table-fn">a</xref></td><td align="left">221 702</td></tr><tr><td align="left">Buenos Aires, Argentina</td><td align="left">219 081</td></tr><tr><td align="left"><bold>Around the world</bold><xref rid="tblfn1a" ref-type="table-fn">a</xref></td><td align="left">209 210</td></tr><tr><td align="left">New Delhi</td><td align="left">198 667</td></tr><tr><td align="left"><bold>Everywhere</bold><xref rid="tblfn1a" ref-type="table-fn">a</xref></td><td align="left">195 892</td></tr><tr><td align="left">São Paulo, Brasil</td><td align="left">191 823</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1a"><label>a</label><p id="d1e570">Keywords which indicate the contributors of COVID-19 re-Tweets on a global aspect.</p></fn></table-wrap-foot></table-wrap></p>
                <p id="d1e3268">Hydration of the known Tweet IDs involves extracting the important text information, which contributes to the features extraction component in this study. Data hydration is subsequently performed using an open-source command-line Tool, <italic>Twarc</italic> which is also programmed in Python language. Each Tweet data is represented as a JSON object that can be extracted using Twitter API by requiring the user to provide 4 key parameters, namely: (a) <italic>consumer_key</italic>; (b) <italic>consumer_secret</italic>; (c) <italic>access_token</italic>; (d) <italic>access_token_secret</italic>, in our personalized data hydration algorithm for Twitter data. The in-built <italic>Twarc</italic> library can automatically handle Twitter API’s <ext-link ext-link-type="uri" xlink:href="https://dev.twitter.com/rest/public/rate-limiting" id="interref3">rate limits</ext-link> where the present quantity limit is 45,000 tweets for every 15 min. In the present Twitter dataset extracted for the analysis, there were approximately 1 million Tweet IDs for every date since 23 January 2020 and the average hydration time takes around 6 h with our present computational resources available. Hence, the total time required to hydrate the total available Tweet IDs, ranging between 23 January 2020 and 10 May 2020, took around 45 days from the start of this study.<fig id="fig3"><label>Fig. 3</label><caption><p>Plot of temporal variation of G values between 25 January 2020 (Day 1) and 11 May (Day 108) 2020.</p></caption><graphic xlink:href="gr3_lrg"/></fig></p>
                <p id="d1e3294">After hydration, the extracted text data in English language undergoes a series of pre-processing steps to remove their inherent “noises” prior to constructing their respective vectorized semantic word representations. The constructed vectors from each processed Tweet data will subsequently be analysed via ODANN’s deep learning algorithms to model the proposed G parameter from Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>. Systematically, the following data pre-processing steps are performed for each of the extracted Tweet data (with a unique Tweet ID), namely:</p>
                <p id="d1e3310">
                  <list list-type="simple" id="d1e3312">
                    <list-item id="lst3">
                      <label>i.</label>
                      <p id="d1e3316">Tokenize each Tweet sentence, as corresponding to each unique Tweet ID, into individual unique words/vocabularies</p>
                    </list-item>
                    <list-item id="lst4">
                      <label>ii.</label>
                      <p id="d1e3321">Examine every individual word against a known pool of stop words as derived from the open-source Natural Language Toolkit (NLTK) library which is also programmed in Python language. If any individual word is identified as a stop word, remove them accordingly, else retain them for further analysis</p>
                    </list-item>
                    <list-item id="lst5">
                      <label>iii.</label>
                      <p id="d1e3326">Remove all punctuations</p>
                    </list-item>
                    <list-item id="lst6">
                      <label>iv.</label>
                      <p id="d1e3331">Remove all other unknown symbols</p>
                    </list-item>
                    <list-item id="lst7">
                      <label>v.</label>
                      <p id="d1e3336">Combine all remaining words to form a new sentence for the specific Tweet ID</p>
                    </list-item>
                  </list>
                </p>
              </sec>
              <sec id="sec3.2">
                <label>3.2</label>
                <title>Feature extraction methods via natural language processing</title>
                <p id="d1e3343">As highlighted earlier, each date, starting from 23 January 2020, had an average of 1 million unique Tweets (including re-Tweets) text data which contributed to their respective corpus (i.e., collection of words/sentences) for the assigned date. The combined corpus of processed Tweet data derived, as ranging between 23 January 2020 and 10 May 2020, will then be further encoded via suitable NLP feature extraction methods to build their corresponding semantic word representations of defined vector sizes. The NLP features extraction algorithms adopted in this study are namely: (a) TfidfVectorizer; (b) Word2Vec – Continuous Bag of Words (CBOW); (c) Word2Vec – Skipgram. In the following, the significance and possible shortcomings for each feature extraction method are briefly described.</p>
                <sec id="sec3.2.1">
                  <label>3.2.1</label>
                  <title>TfidfVectorizer</title>
                  <p id="d1e3350">TfidfVectorizer stands for <italic>“Term Frequency – Inverse Document Frequency”</italic> which represents the components of the respective resulting scores assigned to each unique word in the original pool of corpus <xref rid="b38" ref-type="bibr">[38]</xref>. The inherent <italic>Term Frequency</italic> function computes the frequency of each unique word that exists in the corpus, while the <italic>Inverse Document Frequency</italic> component downscales the respective weights of common words within the same corpus. The latter can be useful to handle imbalanced dataset where the frequency of certain words can far exceed that of others. In general, the TfidfVectorizer algorithm first tokenizes the corpus into individual words, followed by learning the pool of tokenized vocabularies and then inverting the frequency weights of the respective words for further analytics.</p>
                  <p id="d1e3365">The current combined corpus of processed Tweet texts, as ranging between 23 January 2020 and 10 May 2020, contains 94,237 unique words of which each word has its own one-hot encoded representation. However, this method consists of several shortcomings which include: (a) inability to effectively learn and quantify the semantic relationship among the connected words within the corpus or across the different texts for semantic analysis purpose; and (b) the need to re-train the prediction model if new words are introduced into the original corpus, hence constricting the model’s scalability.</p>
                </sec>
                <sec id="sec3.2.2">
                  <label>3.2.2</label>
                  <title>Word2Vec (CBOW and Skipgram)</title>
                  <p id="d1e3372">Word2Vec is a powerful and efficient unsupervised machine learning algorithm, as developed by Google in 2013, which can create neural word embeddings for large corpus. Its inherent algorithm contains an in-built two-layer neural network (shallow–deep network) which processes the input corpus data by forming the word representation for the different words. The output from the neural network model is a set of feature vectors which represent the neighbouring words in the same corpus. It is worth noting that Word2Vec can be classified as a shallow–deep neural network model and works best with big text data where the model can learn the complex relationship among the different words inside the corpus. The derived feature vectors can be processed further by using them as input features to train other machine learning algorithms for predictive analytics or simply queried for semantic analysis purposes. In general, Word2Vec behaves in a similar format as autoencoders by first encoding each word in the concerned corpus and then train the model by mapping the vectorized representation of each word to that of the other surrounding words in the context of the same corpus. This whole process can generally be performed via CBOW or Skipgram algorithms which were also previously developed by Google’s research team <xref rid="b39" ref-type="bibr">[39]</xref>, <xref rid="b40" ref-type="bibr">[40]</xref>.</p>
                  <p id="d1e3403">In the CBOW model, the distributed representations of the context (i.e., surrounding words), built upon a defined window size, are combined accordingly to predict the word in the middle. On the contrary, the Skipgram model maps the distributed representation of the input word by the user to predict its relevant context. CBOW can generally train much faster than Skipgram for the same corpus as the shallow–deep neural network is learning to map the context of the corpus to each available unique word, hence the fitting process is expected to be quicker with more features available in corpus’ context as the input layer. In addition, CBOW is expected to derive more effective representations for words that occur more frequently. Skipgram, however, requires a longer training time with large text data, but can better represents less frequently occurring or rare words/sentences in the corpus. The hyper-parameters adopted for both CBOW and Skipgram in this study for analysing the corpus of processed Tweet text are summarized in <xref rid="tbl2" ref-type="table">Table 2</xref> for model reproducibility. We note that the selected values for the min-count and window-size hyper-parameters for CBOW and Skipgram algorithms are based upon recommendations from the literature <xref rid="b39" ref-type="bibr">[39]</xref>, <xref rid="b40" ref-type="bibr">[40]</xref>. Hence, the focus is to evaluate the varying orders of magnitudes in the vector-size for the word representation, i.e., embedding layer, for the combined corpus of extracted Tweets in the English language as part of the features extraction analysis.</p>
                  <p id="d1e3430">
                    <table-wrap position="anchor" id="tbl2">
                      <label>Table 2</label>
                      <caption>
                        <p>Summary of hyperparameters’ values for Word2Vec algorithms (CBOW, Skipgram) adopted in this study.</p>
                      </caption>
                      <table frame="hsides" rules="groups">
                        <thead>
                          <tr>
                            <th align="left">Hyper-parameters</th>
                            <th align="left">CBOW</th>
                            <th align="left">Skipgram</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left">min-count</td>
                            <td align="left">3</td>
                            <td align="left">3</td>
                          </tr>
                          <tr>
                            <td align="left">window-size</td>
                            <td align="left">5</td>
                            <td align="left">7</td>
                          </tr>
                          <tr>
                            <td align="left">vector-size</td>
                            <td align="left">100, 500, 5000</td>
                            <td align="left">100, 500, 5000</td>
                          </tr>
                          <tr>
                            <td align="left">no_of_workers</td>
                            <td align="left">5</td>
                            <td align="left">5</td>
                          </tr>
                        </tbody>
                      </table>
                    </table-wrap>
                  </p>
                </sec>
              </sec>
              <sec id="sec3.3">
                <label>3.3</label>
                <title>Development of ODANN prediction model</title>
                <p id="d1e3438">The encoded word representation, as derived from the earlier discussed NLP features extraction methods (TfidfVectorizer, CBOW, Skipgram,), are then leveraged as the high-level input features layer for modelling the G parameter over time (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>, <xref rid="fig5" ref-type="fig">Fig. 5</xref>) via deep learning methods. This study adopted a personalized deep neural network (DNN) model with and without data assimilation (ODANN) component based upon a rolling time-window size for modelling and forecasting the G-parameter over time since 25 January 2020 on the global scale.</p>
                <p id="d1e3444">
                  <fig id="fig4">
                    <label>Fig. 4</label>
                    <caption>
                      <p>Illustration of DNN model without data assimilation component to model proposed G parameter.</p>
                    </caption>
                    <graphic xlink:href="gr4_lrg"/>
                  </fig>
                </p>
                <sec id="sec3.3.1">
                  <label>3.3.1</label>
                  <title>DNN without data assimilation</title>
                  <p id="d1e3452">The input features layers, as constructed from the Word2Vec algorithms, adopted a pre-defined vector size for training a simple DNN model (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>), which has multiple hidden layers of neurons inclusive of the final output layer of 1 neuron for modelling the proposed G parameter on a daily basis (see details in <xref rid="tbl3" ref-type="table">Table 3</xref>). Again, we note that the selected vector size for the input layer, ranging among 100, 500, and 5000, does not represent the number of unique words/vocabularies in the analysed corpus as discussed earlier. A series of trial and error was thus performed to determine the optimal size of model’s input layer which can achieve the highest possible prediction accuracy between the predicted and measured G values over time from the subsequent testing step. Generally, a smaller vector size for the input features layer aggregates the quantitative contextual relationship among the different words, and vice versa, which usefulness depends on the specific application. This remains to be further explored for modelling the temporal G values on a daily basis via DNN. The values for the batch size were also varied accordingly for tuning DNN during its training phase as summarized in <xref rid="tbl3" ref-type="table">Table 3</xref>.</p>
                  <p id="d1e3475">For TfidfVectorizer, however, the vector size for the DNN’s input features layer is usually static and equates to the number of unique words/vocabularies which amounts to 94,237 from the present pool of combined corpus as extracted between 25 January 2020 and 11 May 2020. The contextual relationship among the different words is thus not encapsulated in the constructed input features layer using TfidfVectorizer since the size of the input layer is directly built upon the number of unique words in the corpus. The batch size hyperparameter was also tuned when training DNN, as built upon the input layer constructed using TfidfVectorizer features extraction method (see <xref rid="tbl3" ref-type="table">Table 3</xref>). Finally, we again note that we maintained a minimum lead-time of 1 day for the modelling step in all scenarios (different input layer sizes, etc.). As another example, to forecast the G value for 31 March 2020 via a trained DNN prediction model, we leveraged on the available Twitter data from 29 March 2020, hence maintaining the 1-day lead-time.</p>
                </sec>
                <sec id="sec3.3.2">
                  <label>3.3.2</label>
                  <title>ODANN</title>
                  <p id="d1e3486">Data assimilation into ODANN’s hidden layers, as illustrated in <xref rid="fig5" ref-type="fig">Fig. 5</xref>, is achieved by leveraging on a rolling historical time-window size, inclusive of the minimum lead-time of 1 day, which merges the selected hidden layer with additional neurons as representative of the actual G values from the previous days, based upon the defined time-window. For example, with a 3-day rolling time window (<inline-formula><mml:math id="d1e3494" display="inline" altimg="si9.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), where the lead-time of 1 day is inherent, 3 additional neurons representing the respective values for <inline-formula><mml:math id="d1e3535" display="inline" altimg="si9.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> will then be concatenated with the selected hidden layer from <xref rid="fig3" ref-type="fig">Fig. 3</xref> to forecast the G value on the current day itself (i.e., <inline-formula><mml:math id="d1e3581" display="inline" altimg="si11.svg"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) via ODANN. In the case of forecasting the G value on 25 January 2020, we leveraged on the historical time records for the G parameter from 21–23 January 2020 for the data assimilation step. The novelty of ODANN (<xref rid="fig5" ref-type="fig">Fig. 5</xref>) lies in its capability to concatenate the encoded input features from large volumes of COVID-19 related Twitter data and the historical time-series records for the G parameter into a single end-to-end model framework for near real-time predictions. Additional discussions on the novelty aspect of ODANN to model the G-parameter in the near real-time context will be provided in the later section.</p>
                  <p id="d1e3595">In this study, we explored the rolling historical time-window sizes of 3, 5, and 7 days for training the ODANN model with the proposed data assimilation component. As part of the grid-search process to optimize the initialized weightage values for the hidden layers of ODANN, we define an optimization criteria as: (i) estimated mean squared error (MSE) value must be lower than 0.0100 (RMSE <inline-formula><mml:math id="d1e3598" display="inline" altimg="si12.svg"><mml:mo>≈</mml:mo></mml:math></inline-formula> 0.1); and (ii) estimated mean squared error (MAE) value must be lower than 0.100, during ODANN’s validation step for the training scenarios with and without data assimilation component, i.e. simple DNN model from <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p>
                  <p id="d1e3606">
                    <table-wrap position="float" id="tbl3">
                      <label>Table 3</label>
                      <caption>
                        <p>Summary of hyperparameter values for training ODANN with and without data assimilation component.</p>
                      </caption>
                      <table frame="hsides" rules="groups">
                        <thead>
                          <tr>
                            <th align="left">Hyper-parameters</th>
                            <th align="left">ODANN</th>
                            <th align="left">DNN without <break/>data assimilation</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left">No. of neurons in hidden layer 1</td>
                            <td colspan="2" align="left">6</td>
                          </tr>
                          <tr>
                            <td align="left">No. of neurons in hidden layer 2</td>
                            <td colspan="2" align="left">1</td>
                          </tr>
                          <tr>
                            <td align="left">No. of neurons in hidden layer 3</td>
                            <td align="left">Size of rolling time-window <break/>(e.g., number of neurons <inline-formula><mml:math id="d1e658" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 3 for 3 days’ time-window)</td>
                            <td align="left">Nil</td>
                          </tr>
                          <tr>
                            <td align="left">No. of neurons in hidden layer 4</td>
                            <td align="left">Size of rolling time-window <break/>(e.g., number of neurons <inline-formula><mml:math id="d1e672" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 3 for 3 days’ time-window)</td>
                            <td align="left">Nil</td>
                          </tr>
                          <tr>
                            <td align="left">No. of neurons in output layer</td>
                            <td colspan="2" align="left">1</td>
                          </tr>
                          <tr>
                            <td align="left">Rolling time-window size</td>
                            <td colspan="2" align="left">3, 5 &amp; 7 days</td>
                          </tr>
                          <tr>
                            <td align="left">Batch Size</td>
                            <td colspan="2" align="left">2, 4, 6, 8, 12, 16</td>
                          </tr>
                          <tr>
                            <td align="left">Number of Epochs</td>
                            <td colspan="2" align="left">50</td>
                          </tr>
                          <tr>
                            <td align="left">Learning rate</td>
                            <td colspan="2" align="left">0.0001</td>
                          </tr>
                          <tr>
                            <td align="left">Activation function for all hidden layers</td>
                            <td colspan="2" align="left">Exponential Linear Unit (ELU)</td>
                          </tr>
                          <tr>
                            <td align="left">Optimization function</td>
                            <td colspan="2" align="left">Adam</td>
                          </tr>
                          <tr>
                            <td align="left">Key cost function</td>
                            <td colspan="2" align="left">Mean Squared Error (MSE) – set criteria to be below value of 0.0100 for model validation step</td>
                          </tr>
                        </tbody>
                      </table>
                    </table-wrap>
                    <fig id="fig5">
                      <label>Fig. 5</label>
                      <caption>
                        <p>Illustration of ODANN model, with data assimilation component, to model proposed G parameter with rolling time-window size.</p>
                      </caption>
                      <graphic xlink:href="gr5_lrg"/>
                    </fig>
                  </p>
                </sec>
              </sec>
              <sec id="sec3.4">
                <label>3.4</label>
                <title>Prediction performance evaluation</title>
                <p id="d1e3616">As discussed previously, to evaluate ODANN’s predictive capability (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, <xref rid="fig5" ref-type="fig">Fig. 5</xref>) during its training (with validation) and testing steps, the following error metrics were adopted, namely: (i) mean squared error (MSE) in Eq. <xref rid="fd2" ref-type="disp-formula">(2)</xref>; (ii) root mean squared error (RMSE) in Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref>; (iii) mean absolute error (MAE) in Eq. <xref rid="fd4" ref-type="disp-formula">(4)</xref>. We note that MSE was selected as the key cost function for the model training step (see <xref rid="tbl2" ref-type="table">Table 2</xref>) to minimize the error difference between the measured and simulated G values, while RMSE and MAE were also computed at the same time for a comprehensive analysis. <disp-formula-group id="d1e3648"><disp-formula id="fd2"><label>(2)</label><mml:math id="d1e3652" display="block" altimg="si15.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo id="mmlalignd1e3630" linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="fd3"><label>(3)</label><mml:math id="d1e3718" display="block" altimg="si16.svg"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo id="mmlalignd1e3698" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e3630">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula><disp-formula id="fd4"><label>(4)</label><mml:math id="d1e3788" display="block" altimg="si17.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo id="mmlalignd1e3766" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e3630">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></disp-formula-group> where <inline-formula><mml:math id="d1e3853" display="inline" altimg="si18.svg"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of data quantity being analysed,​ <inline-formula><mml:math id="d1e3867" display="inline" altimg="si19.svg"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the predicted G value on a specific day <inline-formula><mml:math id="d1e3881" display="inline" altimg="si20.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="d1e3895" display="inline" altimg="si21.svg"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the measured G value on the same day itself <inline-formula><mml:math id="d1e3909" display="inline" altimg="si20.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
              </sec>
            </sec>
            <sec id="sec4">
              <label>4</label>
              <title>Computational experiments and results</title>
              <sec id="sec4.1">
                <label>4.1</label>
                <title>ODANN’s model training configuration</title>
                <p id="d1e3932">The extracted and processed dataset, ranging between 25 January 2020 and 11 May 2020, had a total data quantity of 108 data instances on a daily basis. Training of ODANN is performed with 85% of the total quantity, while the remaining quantity is used for model testing. We note that within the 85% of data quantity for model training, 20% is used for validating ODANN during its training phase. In addition, we note that no random shuffling of the original dataset is performed prior to splitting it into components for model training, validation, and testing steps, for the following reasons:</p>
                <p id="d1e3934">
                  <list list-type="simple" id="d1e3936">
                    <list-item id="lst8">
                      <label>•</label>
                      <p id="d1e3940">Forecasting of the proposed G parameter for COVID-19 should be based upon extrapolation computations using the optimized weights of ODANN’s hidden layers where they have been trained, a priori, using continuous dataset for the G parameter since the pandemic’s inception, and not segmented discrete data points at separated timestamps. We believe that modelling the continuous evolution of the G parameter on a daily basis, with respect to the available Twitter data made available in near real-time, can more accurately forecast the growth rate in the confirmed number of COVID-19 cases on a global scale.</p>
                    </list-item>
                    <list-item id="lst9">
                      <label>•</label>
                      <p id="d1e3947">Building upon the near real-time requirement in ODANN’s predictive capability, the forecasting of the G parameter, as discussed in-detail previously, should be based upon Twitter data being collated on a daily basis with a lead-time of 1 day and a defined number of historical records for the same G metric with respect to the modeller’s rolling time-window size. Hence, the forecasting step by ODANN in the near real-time context on any given day requires continuous influx of big-data information, and not randomized sets of historical datasets.</p>
                    </list-item>
                  </list>
                </p>
                <p id="d1e3957">In summary, the computational protocol for training and validating ODANN (coupled with Algorithm 1) to forecast, with and without data assimilation, the global G parameter on any given day, i.e., <inline-formula><mml:math id="d1e3960" display="inline" altimg="si11.svg"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, with a lead-time of 1 day is summarized as follows:</p>
                <p id="d1e3969">
                  <list list-type="simple" id="d1e3971">
                    <list-item id="lst10">
                      <label>•</label>
                      <p id="d1e3975"><bold>DANN without data assimilation</bold>: The proposed workflow first constructs the required vectorized word representation for the collated Twitter data from <inline-formula><mml:math id="d1e3980" display="inline" altimg="si24.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> day using either: (i) TfidfVectorizer feature extraction method which resulting vector size will be 94 237; or (ii) CBOW or Skipgram feature extraction method which resulting vector size can be 100, 500 or 5000. Then, the vectorized word representations are fed as the input features layer for the simple DNN model, as depicted in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, to perform the model training and validation steps using the listed hyper-parameters from <xref rid="tbl3" ref-type="table">Table 3</xref>.</p>
                    </list-item>
                    <list-item id="lst11">
                      <label>•</label>
                      <p id="d1e4007"><bold>ODANN with data assimilation</bold>: Perform identical input features layer construction from that of ODANN without data assimilation. Next, previous days of G values, as represented in one-dimensional (1D) arrays, for the required data assimilation component are prepared based on the pre-defined rolling time-window size. For example, a rolling time-window size of 3 days will generate a 1D array size of 3 which compacts the G values from <inline-formula><mml:math id="d1e4012" display="inline" altimg="si24.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4026" display="inline" altimg="si26.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="d1e4040" display="inline" altimg="si27.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Then, the vectorized data are assimilated into ODANN’s hidden layer from <xref rid="fig5" ref-type="fig">Fig. 5</xref> for the same model training and validation steps. We note that selection of the specific hidden layer for the data assimilation step was determined after a series of trial and error, and additional discussions will be provided in the subsequent section. In the present neural network design for simple DNN and ODANN from <xref rid="tbl3" ref-type="table">Table 3</xref> and <xref rid="fig4" ref-type="fig">Fig. 4</xref>, <xref rid="fig5" ref-type="fig">Fig. 5</xref> respectively, each model training and validation run, pertaining to every combination of the defined hyperparameters, required an average of 1 min for completing their computations.</p>
                    </list-item>
                  </list>
                </p>
              </sec>
              <sec id="sec4.2">
                <label>4.2</label>
                <title>Analysis of prediction results for ODANN</title>
                <p id="d1e4071">This section presents a comprehensive comparison of the results derived from ODANN with and without data assimilation, i.e. simple DNN. To ensure clarity to our readers, the following details will be provided. Benchmarking was first performed for DNN without data assimilation using the different types of the NLP features extraction methods, as discussed earlier. The optimized model configuration for the vector size of the input features layer and the rolling time-window size, which can best minimize the computed error metrics in Eqs. <xref rid="fd2" ref-type="disp-formula">(2)</xref>–<xref rid="fd4" ref-type="disp-formula">(4)</xref> from the testing step of DNN without data assimilation, were subsequently adopted for training and validating ODANN with data assimilation component. The observed differences in the model’s resulting predictive capability from the testing step using different model configurations were also discussed. Finally, the best model configuration for ODANN, with data assimilation component, was then justified for its near real-time predictions of the proposed G parameter.</p>
                <sec id="sec4.2.1">
                  <label>4.2.1</label>
                  <title>Benchmark results from DNN without data assimilation</title>
                  <p id="d1e4086">Without any data assimilation, <xref rid="fig6" ref-type="fig">Fig. 6</xref>, <xref rid="fig7" ref-type="fig">Fig. 7</xref>, <xref rid="fig8" ref-type="fig">Fig. 8</xref> illustrate the best comparison between the measured and predicted G values obtained from the simple DNN design for its combined training, validation and testing steps, coupled with different types of NLP features extraction methods built upon the respective optimal batch size (from the grid-search step) adopted in this study. Readers are also referred to <xref rid="tbl4" ref-type="table">Table 4</xref> for the summary of the lowest MSE, RMSE, and MAE values computed from DNN’s combined training, validation, and testing steps using the optimal batch sizes coupled with the respective NLP features extraction methods. Generally, the benchmark results indicate the following key findings.</p>
                  <p id="d1e4096">
                    <list list-type="simple" id="d1e4098">
                      <list-item id="lst12">
                        <label>•</label>
                        <p id="d1e4102">The vector size of the input features layer for the deep learning model must be fine-tuned to best improve its level of agreement, i.e., goodness-of-fit, between the predicted and measured G values from the model’s testing step (see <xref rid="tbl4" ref-type="table">Table 4</xref>). For example, <xref rid="fig7" ref-type="fig">Fig. 7</xref>, <xref rid="fig8" ref-type="fig">Fig. 8</xref> show that increasing the vector size from 100 to 5000, using CBOW or Skipgram feature extraction method, resulted in better fitting between the measured and simulated G values from the model’s testing step. On the contrary, <xref rid="fig6" ref-type="fig">Fig. 6</xref> shows that a vector size of almost 20 times bigger than that of <xref rid="fig7" ref-type="fig">Figs. 7</xref>c and <xref rid="fig8" ref-type="fig">8</xref>c, resulting from the TfidfVectorizer features extraction method, will instead increase the overall error deviation between the measured and predicted G values from the same testing step. Therefore, the results indicate the need to determine an optimal vector size for the model’s input features layer, without data assimilation component, as depending on the type of features extraction method being adopted a priori.</p>
                      </list-item>
                      <list-item id="lst13">
                        <label>•</label>
                        <p id="d1e4128">Using Skip-gram as the features extraction method to build the input features layer for the simple DNN model is likely to overfit the measured G data as shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>, where a low error score was achieved with the training dataset, however, instead resulting in a high error score on the testing dataset subsequently. Hence, the model was likely to have overfitted the encoded input features from Skip-gram on the training data for the proposed G parameter. For comparison, it can be seen from <xref rid="fig7" ref-type="fig">Fig. 7</xref>, <xref rid="fig8" ref-type="fig">Fig. 8</xref> that CBOW and Skip-gram, as belonging to Word2Vec algorithms, can most likely mitigate the overfitting issue in the model’s combined training, validation, and testing steps.</p>
                      </list-item>
                      <list-item id="lst14">
                        <label>•</label>
                        <p id="d1e4141">Using Twitter data solely is unlikely to fully encapsulate the complex dynamics inherent to the global growth rate of the G parameter, hence indicating that the emotional responses of the global community towards COVID-19 are coupled with other unknown conditions controlling the spread of COVID-19 globally. Further discussions will be provided in the later section which can be useful to further underscore the usefulness (and novelty) of the proposed ODANN model with data assimilation component.</p>
                      </list-item>
                    </list>
                  </p>
                  <p id="d1e4143">
                    <fig id="fig6">
                      <label>Fig. 6</label>
                      <caption>
                        <p>Comparison between predicted and measured G values, using simple DNN model, for 25 Jan 2020 to 11 May 2020 using TfidfVectorizer pre-processing model with fixed vector size of 94,237.</p>
                      </caption>
                      <graphic xlink:href="gr6_lrg"/>
                    </fig>
                    <fig id="fig7">
                      <label>Fig. 7</label>
                      <caption>
                        <p>Comparison between predicted and measured G values, using simple DNN model, for 25 Jan 2020 to 11 May 2020 using CBOW NLP features extraction method with varying vector size for input layer: (a) vector size <inline-formula><mml:math id="d1e76" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e81" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e86" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p>
                      </caption>
                      <graphic xlink:href="gr7_lrg"/>
                    </fig>
                    <fig id="fig8">
                      <label>Fig. 8</label>
                      <caption>
                        <p>Comparison between predicted and measured G values, using simple DNN model, for 25 Jan 2020 to 11 May 2020 using Skip-gram NLP features extraction method with varying vector size for input layer: (a) vector size <inline-formula><mml:math id="d1e99" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e104" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e109" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p>
                      </caption>
                      <graphic xlink:href="gr8_lrg"/>
                    </fig>
                    <table-wrap position="float" id="tbl4">
                      <label>Table 4</label>
                      <caption>
                        <p>Summary of lowest possible MSE, RMSE, and MAE values derived for DNN model’s validation and testing steps, without data assimilation component, using different NLP features extraction methods.</p>
                      </caption>
                      <table frame="hsides" rules="groups">
                        <thead>
                          <tr>
                            <th align="left">NLP features extraction method</th>
                            <th align="left">Batch size</th>
                            <th align="left">Vector size</th>
                            <th colspan="3" align="left">Error scores on validation dataset<hr/></th>
                            <th colspan="3" align="left">Error scores on testing dataset<hr/></th>
                          </tr>
                          <tr>
                            <th align="left"/>
                            <th align="left"/>
                            <th align="left"/>
                            <th align="left">MSE</th>
                            <th align="left">RMSE</th>
                            <th align="left">MAE</th>
                            <th align="left">MSE</th>
                            <th align="left">RMSE</th>
                            <th align="left">MAE</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left">TfidfVectorizer</td>
                            <td align="left">2</td>
                            <td align="left">94 237</td>
                            <td align="left">0.000446</td>
                            <td align="left">0.0211</td>
                            <td align="left">0.0133</td>
                            <td align="left">0.00201</td>
                            <td align="left">0.0448</td>
                            <td align="left">0.0442</td>
                          </tr>
                          <tr>
                            <td align="left">CBOW</td>
                            <td align="left">12</td>
                            <td align="left">100</td>
                            <td align="left">0.00854</td>
                            <td align="left">0.0924</td>
                            <td align="left">0.0562</td>
                            <td align="left">0.000148</td>
                            <td align="left">0.0122</td>
                            <td align="left">0.0117</td>
                          </tr>
                          <tr>
                            <td align="left">CBOW</td>
                            <td align="left">4</td>
                            <td align="left">500</td>
                            <td align="left">0.00212</td>
                            <td align="left">0.0460</td>
                            <td align="left">0.0332</td>
                            <td align="left">0.000337</td>
                            <td align="left">0.0184</td>
                            <td align="left">0.0160</td>
                          </tr>
                          <tr>
                            <td align="left">CBOW</td>
                            <td align="left">16</td>
                            <td align="left">5000</td>
                            <td align="left">0.00256</td>
                            <td align="left">0.0506</td>
                            <td align="left">0.0337</td>
                            <td align="left">0.000221</td>
                            <td align="left">0.0149</td>
                            <td align="left">0.0116</td>
                          </tr>
                          <tr>
                            <td align="left">Skip-gram</td>
                            <td align="left">12</td>
                            <td align="left">100</td>
                            <td align="left">0.00858</td>
                            <td align="left">0.0926</td>
                            <td align="left">0.0564</td>
                            <td align="left">0.000137</td>
                            <td align="left">0.0117</td>
                            <td align="left">0.0112</td>
                          </tr>
                          <tr>
                            <td align="left">Skip-gram</td>
                            <td align="left">4</td>
                            <td align="left">500</td>
                            <td align="left">0.00292</td>
                            <td align="left">0.0540</td>
                            <td align="left">0.0390</td>
                            <td align="left">0.000538</td>
                            <td align="left">0.0232</td>
                            <td align="left">0.0222</td>
                          </tr>
                          <tr>
                            <td align="left">Skip-gram</td>
                            <td align="left">2</td>
                            <td align="left">5000</td>
                            <td align="left">0.00121</td>
                            <td align="left">0.0347</td>
                            <td align="left">0.0260</td>
                            <td align="left">0.000231</td>
                            <td align="left">0.0152</td>
                            <td align="left">0.0122</td>
                          </tr>
                        </tbody>
                      </table>
                    </table-wrap>
                  </p>
                </sec>
                <sec id="sec4.2.2">
                  <label>4.2.2</label>
                  <title>Improved results from ODANN</title>
                  <p id="d1e4157">Building upon the benchmark models, as described in the preceding sub-section, data assimilation with the proposed ODANN design from <xref rid="fig5" ref-type="fig">Fig. 5</xref> is then leveraged for the same modelling step. Likewise, <xref rid="fig9" ref-type="fig">Fig. 9</xref>, <xref rid="fig10" ref-type="fig">Fig. 10</xref>, <xref rid="fig11" ref-type="fig">Fig. 11</xref>, <xref rid="fig12" ref-type="fig">Fig. 12</xref>, <xref rid="fig13" ref-type="fig">Fig. 13</xref>, <xref rid="fig14" ref-type="fig">Fig. 14</xref>, <xref rid="fig15" ref-type="fig">Fig. 15</xref> illustrate the best comparison between the measured and predicted G values using the different types of NLP feature extraction methods with their respective optimal batch size for the data assimilation component. Readers are also referred to <xref rid="tbl5" ref-type="table">Table 5</xref> for the summary of the lowest possible MSE, RMSE, and MAE values computed from the model’s combined model training, validation, and testing steps using the optimal batch sizes which corresponds to each of the NLP features extraction method used, coupled with data assimilation. As compared to the previous benchmark results, the following key findings can be summarized:</p>
                  <p id="d1e4171">
                    <list list-type="simple" id="d1e4173">
                      <list-item id="lst15">
                        <label>•</label>
                        <p id="d1e4177">For all 3 NLP feature extraction methods used, the proposed data assimilation component, as opposed to no data assimilation, significantly improves the level of agreement between the predicted and measured G values for the validation and testing steps of ODANN under the different combinations of batch size and rolling time-window size. For example, comparing <xref rid="fig6" ref-type="fig">Fig. 6</xref>, <xref rid="fig9" ref-type="fig">Fig. 9</xref>a under the same batch size value of 2, there was a clear improvement in the level of agreement between the predicted and measured G values from the model’s testing step where the respective RMSE value reduced from 0.0448 to 0.00603 by assimilating a 3 days’ rolling time-window for the historical G values.</p>
                      </list-item>
                      <list-item id="lst16">
                        <label>•</label>
                        <p id="d1e4186">The current best agreement between the predicted and measured G values for the testing step of ODANN was achieved by using the CBOW feature extraction method together with the batch size of 2 and a rolling time-window size of 5 days (see <xref rid="fig11" ref-type="fig">Fig. 11</xref>a) with respect to the historical G values for the ODANN’s training phase. The best results indicate that CBOW is generally more effective to quantitatively encapsulate the inherent context of the Tweets (and re-Tweets) made by netizens towards COVID-19.</p>
                      </list-item>
                      <list-item id="lst17">
                        <label>•</label>
                        <p id="d1e4195">Building upon the preceding pointer, the optimal rolling time-window size of 5 days is likely to indicate a relatively fast transformation of the pandemic’s behaviour over time. The optimal batch sizes, as ranging between 2 and 6, for CBOW and Skip-gram (<xref rid="fig11" ref-type="fig">Figs. 11</xref>a, <xref rid="fig12" ref-type="fig">12</xref>a, and <xref rid="fig13" ref-type="fig">13</xref>a) respectively supports the observation that feeding smaller groups of input features into the ODANN with data assimilation component generally improved the model’s predictive accuracy during its testing phase.</p>
                      </list-item>
                      <list-item id="lst18">
                        <label>•</label>
                        <p id="d1e4213">Overall, a smaller vector size for ODANN’s input features layer, as derived from either CBOW or Skip-gram features extraction methods, can better capture the complex emotional responses of the general population towards COVID-19 and thus transforming them into more useful high-level input features to maximize the model’s predictive accuracy, i.e., lowering the MSE, RMSE, and MAE scores from the testing phase of ODANN (see <xref rid="tbl5" ref-type="table">Table 5</xref>), as compared to using larger vector size which can generally incur higher computational time during the model’s training and validation steps, especially if a larger model architecture is employed for ODANN in future studies.</p>
                      </list-item>
                      <list-item id="lst19">
                        <label>•</label>
                        <p id="d1e4222">If insufficient historical data records for the proposed G parameter (<inline-formula><mml:math id="d1e4226" display="inline" altimg="si34.svg"><mml:mo>≤</mml:mo></mml:math></inline-formula> 3 days) are present in a hypothetical scenario, the current results obtained indicate that using Skip-gram, as the features extraction method, may be more effective in generating more useful high-level input features (see <xref rid="fig13" ref-type="fig">Fig. 13</xref>a) for ODANN as compared to that of CBOW. The latter method (CBOW) appears to better complement ODANN’s predictive accuracy with additional historical data records, i.e., greater than 3 days, as shown in <xref rid="fig11" ref-type="fig">Figs. 11</xref>a and <xref rid="fig12" ref-type="fig">12</xref>a.</p>
                      </list-item>
                    </list>
                  </p>
                  <p id="d1e4243">At this stage, we have shown that ODANN, coupled with data assimilation using a historical time-window size of 5 days, which input features layers (vector size <inline-formula><mml:math id="d1e4263" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500) built upon CBOW features extraction method can best maximize model’s predictive accuracy, in terms of the resulting error scores, as summarized in <xref rid="tbl5" ref-type="table">Table 5</xref>. For extensiveness in our proposed analysis, we investigated varying sizes of the training and validation datasets as part of cross-validating ODANN’s predictive capability from its training step. Traditionally, cross-validation for ML/DL models requires randomizing the datasets for splitting into training, validation, and testing subsets. However, in our present time-series modelling for the proposed G parameter, we avoid shuffling the available data pool ranging between 25 Jan 2020 and 11 May 2020 for the same reasons as outlined earlier. Instead, our cross-validation analysis solely focuses on investigating the effects of varying the sizes of the training (and validation) and testing datasets into multiple combinations of: (i) 60% for training and validation, 40% for testing; (ii) 70% for training and validation, 30% for testing; (iii) 80% for training and validation, 20% for testing; and (iv) 85% for training and validation, 15% for testing, as listed in <xref rid="tbl6" ref-type="table">Table 6</xref> while retaining the 5-days data assimilation component, coupled with CBOW features extraction method, and batch-size of 2 for the ODANN model. In summary, varying the sizes of datasets assigned for training (and validating) and testing ODANN indicated the following:<fig id="fig9"><label>Fig. 9</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using TfidfVectorizer features extraction method with fixed vector size of 94 237 for ODANN, coupled varying rolling time-window sizes: (a) time-window <inline-formula><mml:math id="d1e124" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 3 days; (b) time-window <inline-formula><mml:math id="d1e129" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5 days; (c) time-window <inline-formula><mml:math id="d1e134" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 7 days.</p></caption><graphic xlink:href="gr9_lrg"/></fig><fig id="fig10"><label>Fig. 10</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using CBOW features extraction method for ODANN with 3 days rolling time-window, coupled with varying vector size for input features layer: (a) vector size <inline-formula><mml:math id="d1e147" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e152" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e157" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p></caption><graphic xlink:href="gr10_lrg"/></fig><fig id="fig11"><label>Fig. 11</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using CBOW features extraction method for ODANN with 5 days rolling time-window, coupled with varying vector size for input features layer: (a) vector size <inline-formula><mml:math id="d1e170" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e175" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e180" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p></caption><graphic xlink:href="gr11_lrg"/></fig><fig id="fig12"><label>Fig. 12</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using CBOW features extraction method for ODANN with 7 days rolling time-window, coupled with varying vector size for input features layer: (a) vector size <inline-formula><mml:math id="d1e194" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e199" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e204" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p></caption><graphic xlink:href="gr12_lrg"/></fig><fig id="fig13"><label>Fig. 13</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using Skip-Gram features extraction method for ODANN with 3 days rolling time-window, coupled with varying vector size for input features layer: (a) vector size <inline-formula><mml:math id="d1e217" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e222" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e227" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p></caption><graphic xlink:href="gr13_lrg"/></fig><fig id="fig14"><label>Fig. 14</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using Skip-Gram features extraction method for ODANN with 5 days rolling time-window, coupled with varying vector size for input features layer: (a) vector size <inline-formula><mml:math id="d1e240" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e245" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e250" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p></caption><graphic xlink:href="gr14_lrg"/></fig><fig id="fig15"><label>Fig. 15</label><caption><p>Comparison between predicted and measured G values for 25 Jan 2020 to 11 May 2020 using Skip-Gram features extraction method for ODANN with 7 days rolling time-window, coupled with varying vector size for input features layer: (a) vector size <inline-formula><mml:math id="d1e263" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 100; (b) vector size <inline-formula><mml:math id="d1e268" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500; (c) vector size <inline-formula><mml:math id="d1e273" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5000.</p></caption><graphic xlink:href="gr15_lrg"/></fig><table-wrap position="float" id="tbl5"><label>Table 5</label><caption><p>Summary of lowest MSE, RMSE, and MAE values derived for ODANN, with data assimilation component, using different NLP features extraction methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Rolling time-window</th><th align="left">NLP features extraction method</th><th align="left">Batch size</th><th align="left">Vector size</th><th colspan="3" align="left">Error scores on validation dataset<hr/></th><th align="left"/><th colspan="3" align="left">Error scores on testing dataset<hr/></th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left"/><th align="left">MSE</th><th align="left">RMSE</th><th align="left">MAE</th><th align="left"/><th align="left">MSE</th><th align="left">RMSE</th><th align="left">MAE</th></tr></thead><tbody><tr><td rowspan="6" align="left">3 days</td><td align="left">TfidfVectorizer</td><td align="left">2</td><td align="left">94 237</td><td align="left">0.000221</td><td align="left">0.0149</td><td align="left">0.00697</td><td align="left"/><td align="left">0.0000360</td><td align="left">0.00603</td><td align="left">0.00444</td></tr><tr><td align="left">CBOW</td><td align="left">2</td><td align="left">100</td><td align="left">0.00195</td><td align="left">0.0442</td><td align="left">0.0195</td><td align="left"/><td align="left">0.0000180</td><td align="left">0.00425</td><td align="left">0.00314</td></tr><tr><td align="left">CBOW</td><td align="left">6</td><td align="left">500</td><td align="left">0.00150</td><td align="left">0.0387</td><td align="left">0.0190</td><td align="left"/><td align="left">0.0000250</td><td align="left">0.00497</td><td align="left">0.00411</td></tr><tr><td align="left">CBOW</td><td align="left">4</td><td align="left">5000</td><td align="left">0.00117</td><td align="left">0.0343</td><td align="left">0.0176</td><td align="left"/><td align="left">0.0000210</td><td align="left">0.00454</td><td align="left">0.00358</td></tr><tr><td align="left">Skip-gram</td><td align="left"><bold>6</bold></td><td align="left"><bold>100</bold></td><td align="left"><bold>0.00124</bold></td><td align="left"><bold>0.0352</bold></td><td align="left"><bold>0.0177</bold></td><td align="left"/><td align="left"><bold>0.0000130</bold></td><td align="left"><bold>0.00361</bold></td><td align="left"><bold>0.002972</bold></td></tr><tr><td align="left">Skip-gram</td><td align="left">4</td><td align="left">500</td><td align="left">0.00127</td><td align="left">0.0356</td><td align="left">0.0181</td><td align="left"/><td align="left">0.0000190</td><td align="left">0.00434</td><td align="left">0.00369</td></tr><tr><td align="left"/><td align="left">Skip-gram</td><td align="left">6</td><td align="left">5000</td><td align="left">0.00112</td><td align="left">0.0334</td><td align="left">0.0166</td><td align="left"/><td align="left">0.0000250</td><td align="left">0.00500</td><td align="left">0.00395</td></tr><tr><td colspan="11"><hr/></td></tr><tr><td rowspan="6" align="left">5 days</td><td align="left">TfidfVectorizer</td><td align="left">8</td><td align="left">94 237</td><td align="left">0.000237</td><td align="left">0.0154</td><td align="left">0.00728</td><td align="left"/><td align="left">0.0000610</td><td align="left">0.00781</td><td align="left">0.00620</td></tr><tr><td align="left">CBOW</td><td align="left">2</td><td align="left">100</td><td align="left">0.000814</td><td align="left">0.0285</td><td align="left">0.0153</td><td align="left"/><td align="left">0.00000829</td><td align="left">0.00288</td><td align="left">0.00225</td></tr><tr><td align="left">CBOW</td><td align="left"><bold>2</bold></td><td align="left"><bold>500</bold></td><td align="left"><bold>0.000621</bold></td><td align="left"><bold>0.0249</bold></td><td align="left"><bold>0.0111</bold></td><td align="left"/><td align="left"><bold>0.0000080</bold></td><td align="left"><bold>0.00282</bold></td><td align="left"><bold>0.00214</bold></td></tr><tr><td align="left">CBOW</td><td align="left">8</td><td align="left">5000</td><td align="left">0.000588</td><td align="left">0.0242</td><td align="left">0.01118</td><td align="left"/><td align="left">0.0000130</td><td align="left">0.003552</td><td align="left">0.00295</td></tr><tr><td align="left">Skip-gram</td><td align="left">2</td><td align="left">100</td><td align="left">0.00104</td><td align="left">0.0322</td><td align="left">0.0150</td><td align="left"/><td align="left">0.0000150</td><td align="left">0.00387</td><td align="left">0.00287</td></tr><tr><td align="left">Skip-gram</td><td align="left">16</td><td align="left">500</td><td align="left">0.00155</td><td align="left">0.0394</td><td align="left">0.0218</td><td align="left"/><td align="left">0.0000210</td><td align="left">0.00460</td><td align="left">0.00325</td></tr><tr><td align="left"/><td align="left">Skip-gram</td><td align="left">4</td><td align="left">5000</td><td align="left">0.000613</td><td align="left">0.0248</td><td align="left">0.0127</td><td align="left"/><td align="left">0.0000110</td><td align="left">0.00327</td><td align="left">0.00273</td></tr><tr><td colspan="11"><hr/></td></tr><tr><td rowspan="6" align="left">7 days</td><td align="left">TfidfVectorizer</td><td align="left">16</td><td align="left">94 237</td><td align="left">0.000527</td><td align="left">0.0230</td><td align="left">0.00983</td><td align="left"/><td align="left">0.0000310</td><td align="left">0.00559</td><td align="left">0.00476</td></tr><tr><td align="left">CBOW</td><td align="left"><bold>4</bold></td><td align="left"><bold>100</bold></td><td align="left"><bold>0.000401</bold></td><td align="left"><bold>0.0200</bold></td><td align="left"><bold>0.0122</bold></td><td align="left"/><td align="left"><bold>0.00000918</bold></td><td align="left"><bold>0.00303</bold></td><td align="left"><bold>0.00247</bold></td></tr><tr><td align="left">CBOW</td><td align="left">8</td><td align="left">500</td><td align="left">0.00100</td><td align="left">0.0317</td><td align="left">0.0188</td><td align="left"/><td align="left">0.0000220</td><td align="left">0.00470</td><td align="left">0.00373</td></tr><tr><td align="left">CBOW</td><td align="left">6</td><td align="left">5000</td><td align="left">0.000781</td><td align="left">0.0279</td><td align="left">0.0145</td><td align="left"/><td align="left">0.0000210</td><td align="left">0.00458</td><td align="left">0.00386</td></tr><tr><td align="left">Skip-gram</td><td align="left">4</td><td align="left">100</td><td align="left">0.00117</td><td align="left">0.0342</td><td align="left">0.0170</td><td align="left"/><td align="left">0.0000200</td><td align="left">0.00447</td><td align="left">0.00363</td></tr><tr><td align="left">Skip-gram</td><td align="left">6</td><td align="left">500</td><td align="left">0.00102</td><td align="left">0.0320</td><td align="left">0.0147</td><td align="left"/><td align="left">0.0000230</td><td align="left">0.00483</td><td align="left">0.00412</td></tr><tr><td align="left"/><td align="left">Skip-gram</td><td align="left">12</td><td align="left">5000</td><td align="left">0.00130</td><td align="left">0.0361</td><td align="left">0.0189</td><td align="left"/><td align="left">0.0000210</td><td align="left">0.00453</td><td align="left">0.00374</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The values in bold indicate the best prediction model with the specific rolling time-window.</p></fn></table-wrap-foot></table-wrap></p>
                  <p id="d1e4276">
                    <list list-type="simple" id="d1e4278">
                      <list-item id="lst20">
                        <label>•</label>
                        <p id="d1e4282">As expected, reduction of the data availability for training and validating ODANN increases the resulting error scores from the model’s testing step as shown in <xref rid="tbl6" ref-type="table">Table 6</xref>. The likely reason is ascribed to the spike in the G value at around Day 60 (mid-March 2020) which causes the training model to over-predict the remaining period after Day 60, especially with smaller pools of training data as the model may learn that G value is increasing continuously.</p>
                      </list-item>
                      <list-item id="lst21">
                        <label>•</label>
                        <p id="d1e4291">On the contrary, the selection of 85% of the total available data quantity for training and validating ODANN is expected to optimize the model’s predictive capability (<xref rid="tbl5" ref-type="table">Table 5</xref>, <xref rid="tbl6" ref-type="table">Table 6</xref>) as the training data pool consists of 2 independent periods which demonstrated continuous rate of increase and decline in the G values as illustrated in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. Specifically, the rate of increase in the G values occurred in the approximate periods of Day 0 to Day 5 and Day 40 to Day 60, while the rate of decline occurred approximately during Day 10 to Day 30 and Day 60 to Day 85. Hence, the data distribution for the 2 contrasting dynamic behaviour in the proposed G parameter over time is balanced to a significant extent which benefits the model’s learning capability, especially when coupled with the data assimilation component.</p>
                      </list-item>
                    </list>
                  </p>
                  <p id="d1e4310">
                    <table-wrap position="anchor" id="tbl6">
                      <label>Table 6</label>
                      <caption>
                        <p>Summary of cross-validating ODANN with varying sizes of training, validation, and testing datasets from original data pool ranging between 25 Jan 2020 and 11 May 2020.</p>
                      </caption>
                      <table frame="hsides" rules="groups">
                        <thead>
                          <tr>
                            <th align="left">Training &amp; Validation data size</th>
                            <th align="left">Testing data size</th>
                            <th colspan="3" align="left">Error scores on validation dataset<hr/></th>
                            <th colspan="3" align="left">Error scores on testing dataset<hr/></th>
                          </tr>
                          <tr>
                            <th align="left"/>
                            <th align="left"/>
                            <th align="left">MSE</th>
                            <th align="left">RMSE</th>
                            <th align="left">MAE</th>
                            <th align="left">MSE</th>
                            <th align="left">RMSE</th>
                            <th align="left">MAE</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left">60%</td>
                            <td align="left">40%</td>
                            <td align="left">0.00122</td>
                            <td align="left">0.0349</td>
                            <td align="left">0.0218</td>
                            <td align="left">0.0000157</td>
                            <td align="left">0.00396</td>
                            <td align="left">0.000305</td>
                          </tr>
                          <tr>
                            <td align="left">70%</td>
                            <td align="left">30%</td>
                            <td align="left">0.00101</td>
                            <td align="left">0.0318</td>
                            <td align="left">0.0180</td>
                            <td align="left">0.0000130</td>
                            <td align="left">0.00361</td>
                            <td align="left">0.000279</td>
                          </tr>
                          <tr>
                            <td align="left">80%</td>
                            <td align="left">20%</td>
                            <td align="left">0.000856</td>
                            <td align="left">0.0293</td>
                            <td align="left">0.0153</td>
                            <td align="left">0.0000110</td>
                            <td align="left">0.00332</td>
                            <td align="left">0.000256</td>
                          </tr>
                          <tr>
                            <td align="left">85%</td>
                            <td align="left">15%</td>
                            <td align="left">0.000621</td>
                            <td align="left">0.0249</td>
                            <td align="left">0.0111</td>
                            <td align="left">0.0000080</td>
                            <td align="left">0.00282</td>
                            <td align="left">0.000214</td>
                          </tr>
                        </tbody>
                      </table>
                    </table-wrap>
                  </p>
                </sec>
              </sec>
              <sec id="sec4.3">
                <label>4.3</label>
                <title>Time-efficiency of proposed method</title>
                <p id="d1e4318">On any given day (e.g. 1 May 2020) in the near real-time context, with the defined lead-time of 1 day, our proposed method consists of the following sequential steps: <bold>(Data Hydration)</bold> extractions of COVID-19 Twitter text data from a pool of Tweets IDs collated from the relevant historical day coupled with the inherent 1 day lead-time (e.g. 29 April 2020); <bold>(Data Pre-Processing)</bold> encoding of the extracted text data from the relevant historical day into numerically useful input features layer having an optimal vector size (e.g. input layer of 500 neurons in size); <bold>(Trained Model Restoration)</bold> calling upon the pre-trained ODANN model which takes in the encoded input features layer while also assimilating with historical G values, based on the pre-defined rolling time-window size (e.g. 5 days), to forecast the G-value on the given day in near real-time. <xref rid="tbl7" ref-type="table">Table 7</xref> summarizes the average computational runtime for each of the key steps in the near real-time context. The listed runtimes in <xref rid="tbl7" ref-type="table">Table 7</xref> show that data hydration requires the longest time to be completed, where the remaining steps can be accomplished with a good efficiency. At this stage, we have not explored any forms of data parallelism to accelerate the computational runtime, especially for the data hydration. This remains to be explored in our continual experiments in future studies. Overall, the total runtime to perform all steps listed in <xref rid="tbl7" ref-type="table">Table 7</xref> takes an average of 6.4 h maximum on any given day in near real-time, hence the imposition of the lead-time of 1 day is more than sufficient to ensure that predictions of G-value on the given day can be performed with both efficiency and accuracy.</p>
                <p id="d1e4342">
                  <table-wrap position="anchor" id="tbl7">
                    <label>Table 7</label>
                    <caption>
                      <p>Summary of average computational runtime for each key step in proposed method in near real-time context.</p>
                    </caption>
                    <table frame="hsides" rules="groups">
                      <thead>
                        <tr>
                          <th align="left">Step</th>
                          <th align="left">Average runtime</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left">Data hydration (<inline-formula><mml:math id="d1e1560" display="inline" altimg="si57.svg"><mml:mo>∼</mml:mo></mml:math></inline-formula>1 million of COVID-19 Twitter text data daily)</td>
                          <td align="left">6 h</td>
                        </tr>
                        <tr>
                          <td align="left">Data pre-processing to derive input feature layer</td>
                          <td align="left">20 min</td>
                        </tr>
                        <tr>
                          <td align="left">Trained model restoration</td>
                          <td align="left">20 s</td>
                        </tr>
                        <tr>
                          <td align="left">Model predictions using input feature layer, coupled with data assimilation</td>
                          <td align="left">10 s</td>
                        </tr>
                      </tbody>
                    </table>
                  </table-wrap>
                </p>
              </sec>
            </sec>
            <sec id="sec5">
              <label>5</label>
              <title>Discussions</title>
              <sec id="sec5.1">
                <label>5.1</label>
                <title>Comparison of ODANN with alternative time-series models</title>
                <p id="d1e4355">To further validate the effectiveness of ODANN with its data assimilation component, several comparative experiments were performed with classical time-series prediction models or ML algorithms to forecast the same G parameter over time. In addition, the robustness of ODANN was further tested by setting different percentages of random missing quantities in the datasets for the combined model training, validation, and testing steps. In the following, we first discuss on the comparison analysis of ODANN with the other time-series models.</p>
                <p id="d1e4357">On a whole, the proposed ODANN model yielded the most satisfactory results in forecasting the G values over time, with a defined lead-time of 1 day, under the evaluation metrics of MSE, RMSE, and MAE. For an extensive comparison, several candidates, including ARIMA, AutoARIMA, Prophet, Random Forest (RF), Support Vector Machine (SVM), and Long Short-Term Memory (LSTM) algorithms <xref rid="b41" ref-type="bibr">[41]</xref> were also leveraged to perform the same combined training, validation, and testing phases, i.e. 85% of the total data quantity for model training and validation, and 15% for model testing, to directly model the dynamic behaviour of the proposed G parameter over time. However, without considering any Twitter data as representative of the general community’s emotional responses towards the current pandemic, they still maintaining the same lead-time of 1 day to forecast the G value on any given day between 25 January 2020 and 11 May 2020. The resulting prediction performances derived from the alternative time-series models on the testing dataset are summarized in <xref rid="fig16" ref-type="fig">Fig. 16</xref> and <xref rid="tbl8" ref-type="table">Table 8</xref>, while <xref rid="tbl9a" ref-type="table">Tables 9a</xref> and <xref rid="tbl9b" ref-type="table">9b</xref> summarize the statistical comparison results between ODANN and the alternative time-series models on the same testing dataset.</p>
                <p id="d1e4388">The statistical analyses are namely, <bold>Type 1</bold> paired T-test, Kruskal–Wallis test, and Wilcoxon Signed Rank test, which mainly involve the <inline-formula><mml:math id="d1e4394" display="inline" altimg="si58.svg"><mml:mi>p</mml:mi></mml:math></inline-formula>-value computations, as summarized in <xref rid="tbl9a" ref-type="table">Table 9a</xref>, to estimate the probability of obtaining the results that are at least as extreme as the results actually observed, under the assumption that the respective null hypothesis is correct while assuming a significance value of 0.05; <bold>Type 2:</bold> Pearson correlation coefficient, Spearman correlation coefficient, and Kendall’s tau, which focus on estimating different coefficient values, ranging between 0 and 1 as summarized in <xref rid="tbl9b" ref-type="table">Table 9b</xref>, which quantify the level of association between 2 sets of time-series predictions. In summary, the statistical analyses in both <xref rid="tbl9a" ref-type="table">Tables 9a</xref> and <xref rid="tbl9b" ref-type="table">9b</xref> indicate the following:</p>
                <p id="d1e4418">
                  <list list-type="simple" id="d1e4420">
                    <list-item id="lst22">
                      <label>•</label>
                      <p id="d1e4424"><bold>Paired T-test</bold>: The null hypothesis is that the predictions from ODANN, on the testing dataset, have identical average, i.e. expected, values with that of the other alternative time-series models. The summarized p-values in <xref rid="tbl9a" ref-type="table">Table 9a</xref> for the t-test indicate that the average values of the predictions from RF, SVM, ARIMA, and AutoARIMA model were most similar to that of ODANN’s since their p-values are greater than 0.05, while the average values of the predictions from LSTM and Prophet differed significantly in their average values as the null hypothesis can be rejected with p-values less than 0.05.</p>
                    </list-item>
                    <list-item id="lst23">
                      <label>•</label>
                      <p id="d1e4435"><bold>Kruskal–Wallis test</bold>: The null hypothesis is that the median value of the predictions from ODANN, on the testing dataset, was equal to that of the other alternative time-series models. The summarized p-values in <xref rid="tbl9a" ref-type="table">Table 9a</xref> for the Kruskal–Wallis test indicate that the median value of the predictions from ARIMA, AutoARIMA, RF, SVM and LSTM models were most similar to that of ODANN’s since their p-values are greater than 0.05, while the median value of the predictions from Prophet differed significantly as the null hypothesis can be rejected with p-values less than 0.05.</p>
                    </list-item>
                    <list-item id="lst24">
                      <label>•</label>
                      <p id="d1e4446"><bold>Wilcoxon Signed Rank test</bold>: The null hypothesis is that the predictions from ODANN, on the testing dataset, have the same data distribution from that of the other alternative time-series models. The summarized p-values in <xref rid="tbl9a" ref-type="table">Table 9a</xref> for the Wilcoxon Signed Rank test indicate that the predictions from RF, SVM, ARIMA and AutoARIMA had the most similar distribution to that of ODANN’s since their p-values are greater than 0.05, while the predictions from the remaining models differed significantly in their data distribution as the null hypothesis can be rejected with p-values less than 0.05. Specifically, the respective distributions for the time-series differences between the predictions from LSTM or Prophet, with that of ODANN were not symmetric about the zero-value point.</p>
                    </list-item>
                    <list-item id="lst25">
                      <label>•</label>
                      <p id="d1e4457"><bold>Pearson correlation coefficient</bold>: Measures the level of linear relationship between the predictions from ODANN, on the testing dataset, and that of the other alternative time-series models. The summarized coefficient values in <xref rid="tbl9b" ref-type="table">Table 9b</xref> for the Pearson correlation analysis indicate that the predictions values derived via the LSTM method had the closest linear relationship, however not necessarily identical, with that of ODANN due to its highest 0.839 coefficient value. The other coefficient values suggest that the predictions from the remaining models generally do not correlate linearly, to a significant extent, with that of ODANN’s.</p>
                    </list-item>
                    <list-item id="lst26">
                      <label>•</label>
                      <p id="d1e4468"><bold>Spearman correlation coefficient</bold>: Measures the monotonicity of the relationship between two datasets. Similar conclusion from the preceding Pearson coefficient can also be made when using the Spearman correlation coefficient (see <xref rid="tbl9b" ref-type="table">Table 9b</xref>), where the predictions from the LSTM model had the closest positive monotonic relationship with that of ODANN’s when using the testing dataset. Overall, the predictions from Prophet had the lowest monotonic correlation with that of ODANN’s on the same testing dataset. Additional discussion on Prophet’s result will be made subsequently.</p>
                    </list-item>
                    <list-item id="lst27">
                      <label>•</label>
                      <p id="d1e4479"><bold>Kendall’s tau</bold>: The Kendall’s tau value measures the correspondence between ODANN’s predictions, using the testing dataset, and the corresponding predictions from the other time-series models. Overall, the computed tau values indicate that no significant strong agreement exists between the predictions made by ODANN and the other time-series models on the same testing dataset. Relatively, only the predictions from LSTM had the closest agreement with that of ODANN.</p>
                    </list-item>
                    <list-item id="lst28">
                      <label>•</label>
                      <p id="d1e4486">Overall, the above-discussed statistical analyses indicate that there is no universal statistical analysis test capable of pre-determining which time-series model can generate test predictions to best match with that of ODANN. Each test serves a different objective, as depending on the context of the problem. Using the RMSE and MAE scores summarized in <xref rid="tbl8" ref-type="table">Table 8</xref>, the test predictions from SVM can best match with the predictive accuracy of ODANN, and the computed scores for Paired T-test, Kruskal–Wallis test, and Wilcoxon Signed Rank test (<xref rid="tbl9a" ref-type="table">Table 9a</xref>) indicate that SVM’s predictions were consistently similar with that of ODANN’s in terms of the average (expected), median, and data distribution parameters. However, we note that the latter model still outperformed SVM in the final prediction accuracy, i.e., minimizing the RMSE and MAE scores, when using the same testing dataset.</p>
                    </list-item>
                  </list>
                </p>
                <p id="d1e4497">As illustrated in <xref rid="fig16" ref-type="fig">Fig. 16</xref>, the trend of the prediction curves deriving from the alternative time-series models, with the exception of Prophet, was reasonably consistent with the measured G values (ground truth), which suggests that the alternative models can be reasonably useful in forecasting the growth rate in the number of confirmed COVID-19 cases on a global scale. The compared results listed in <xref rid="tbl8" ref-type="table">Table 8</xref> reveal the following ranked prediction performance of the 6 alternative time-series models: ODANN <inline-formula><mml:math id="d1e4508" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> SVM <inline-formula><mml:math id="d1e4514" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> ARIMA <inline-formula><mml:math id="d1e4519" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> LSTM <inline-formula><mml:math id="d1e4524" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> RF <inline-formula><mml:math id="d1e4529" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> AutoARIMA <inline-formula><mml:math id="d1e4535" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> Prophet. With the present dataset, ODANN outperformed the other traditional models with at least 0.00100 reductions in the average RMSE and MAE values, respectively, hence indicating ODANN’s capability to better encapsulate the growth trajectories of the analysed G parameter over time. The results also highlighted the likelihood that the community’s emotional responses towards the pandemic do affect, to an extent, the temporal variations of the G parameter since its inception, especially when coupled with the data assimilation component in ODANN. Therefore, there are valuable knowledge embedded in the extracted high-level features using the different NLP features extraction methods, which can potentially offer useful guidance to the different stakeholders to obtain more accurate predictions pertaining to the proposed G parameter.</p>
                <p id="d1e4547">On the other hand, we note that Prophet generally performed less satisfactorily as compared to the other models, inclusive of ODANN, due to the need for the training dataset to have some level of seasonality component, whether is it daily, weekly, monthly or yearly in nature. At this stage, it is obvious that the time-series profile for G parameter (see <xref rid="fig3" ref-type="fig">Fig. 3</xref>) do not quite follow any seasonal trends due to the relatively small dataset being analysed at this stage, hence it is difficult for Prophet algorithm to capture any observable seasonal trends during its training phase. As discussed, the rate of increase in the G values occurred in the approximate periods of Day 0 to Day 5 and Day 40 to Day 60, while the corresponding rate of decline occurred approximately during Day 10 to Day 30 and Day 60 to Day 85, hence there was no strong seasonality or trend components in the variations of the G parameter over time with the present dataset. We would, however, expect that the Prophet model can improve on its present predictive accuracy with a larger data pool for the combined model training, validation, and testing steps in any future works as more data is being made available. Finally, it is worth noting that the same results observations have also been reported by recent studies <xref rid="b42" ref-type="bibr">[42]</xref>, <xref rid="b43" ref-type="bibr">[43]</xref> where the authors’ Prophet model performed less ideally as compared to that of the other time-series models such as ARIMA, TBAT, etc., in modelling and forecasting the transmission rate of COVID-19.<fig id="fig16"><label>Fig. 16</label><caption><p>Prediction of G value on the testing dataset using ODANN, ARIMA, AutoARIMA, RF, SVM, LSTM, and Prophet time-series models.</p></caption><graphic xlink:href="gr16_lrg"/></fig><table-wrap position="float" id="tbl8"><label>Table 8</label><caption><p>Evaluation of predicted results from ODANN, ARIMA, AutoARIMA, RF, SVM, LSTM, and Prophet time-series models using the testing dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Parameter</th><th align="left">RMSE</th><th align="left">MAE</th></tr></thead><tbody><tr><td align="left">ARIMA</td><td align="left">Order (1, 1, 1)</td><td align="left">0.00412</td><td align="left">0.00336</td></tr><tr><td align="left">AutoARIMA</td><td align="left">Order (0,1,1)(3,1,1) <break/>Daily seasonality</td><td align="left">0.00639</td><td align="left">0.00468</td></tr><tr><td align="left">RF</td><td align="left">Number of trees <inline-formula><mml:math id="d1e1631" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 8 <break/>Maximum depth of the tree <inline-formula><mml:math id="d1e1638" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 4</td><td align="left">0.00447</td><td align="left">0.00358</td></tr><tr><td align="left">SVM</td><td align="left">Kernel <inline-formula><mml:math id="d1e1653" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> ‘rbf’ <break/>Regularization parameter <inline-formula><mml:math id="d1e1660" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 10 <break/>Epsilon <inline-formula><mml:math id="d1e1668" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 0.001</td><td align="left">0.00388</td><td align="left">0.00315</td></tr><tr><td align="left">LSTM</td><td align="left">Number of hidden layers <inline-formula><mml:math id="d1e1683" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 1 <break/>Number of neurons <inline-formula><mml:math id="d1e1690" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 10</td><td align="left">0.00430</td><td align="left">0.00356</td></tr><tr><td align="left">Prophet</td><td align="left">Width of the uncertainty intervals <inline-formula><mml:math id="d1e1705" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 0.95 <break/>No. of simulated draws for uncertainty intervals <inline-formula><mml:math id="d1e1712" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 35 <break/>Growth <inline-formula><mml:math id="d1e1720" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> linear <break/>Daily seasonality</td><td align="left">0.0300</td><td align="left">0.0294</td></tr><tr><td align="left">ODANN</td><td align="left">Input features layer (via CBOW) <inline-formula><mml:math id="d1e1737" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500 <break/>Epochs <inline-formula><mml:math id="d1e1744" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 50 <break/>Batch size <inline-formula><mml:math id="d1e1752" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 2 <break/>Rolling time-window size (data assimilation) <inline-formula><mml:math id="d1e1759" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5 days</td><td align="left">0.00282</td><td align="left">0.00214</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl9a"><label>Table 9a</label><caption><p>Summary of p-values to compare ODANN with other time-series models for predictions on testing dataset by assuming significance value of 0.05.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Statistical analysis method</th><th align="left">RF</th><th align="left">SVM</th><th align="left">ARIMA</th><th align="left">LSTM</th><th align="left">AutoARIMA</th><th align="left">Prophet</th></tr></thead><tbody><tr><td align="left">Paired T-test</td><td align="left">1.36E−01</td><td align="left">5.20E−02</td><td align="left">5.83E−02</td><td align="left">2.33E−02</td><td align="left">1.68E−01</td><td align="left">6.95E−17</td></tr><tr><td align="left">Kruskal–Wallis test</td><td align="left">4.97E−01</td><td align="left">1.51E−01</td><td align="left">1.41E−01</td><td align="left">5.80E−01</td><td align="left">3.33E−01</td><td align="left">2.89E−08</td></tr><tr><td align="left">Wilcoxon signed rank test</td><td align="left">1.79E−01</td><td align="left">9.58E−02</td><td align="left">8.88E−02</td><td align="left">3.19E−02</td><td align="left">3.38E−01</td><td align="left">9.54E−07</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl9b"><label>Table 9b</label><caption><p>Summary of correlation coefficient values to compare ODANN with other time-series models for predictions on testing dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Statistical analysis method</th><th align="left">RF</th><th align="left">SVM</th><th align="left">ARIMA</th><th align="left">LSTM</th><th align="left">AutoARIMA</th><th align="left">Prophet</th></tr></thead><tbody><tr><td align="left">Pearson correlation coefficient</td><td align="left">6.77E−01</td><td align="left">5.89E−01</td><td align="left">6.74E−01</td><td align="left">8.48E−01</td><td align="left">3.71E−01</td><td align="left">2.36E−01</td></tr><tr><td align="left">Spearman correlation coefficient</td><td align="left">5.76E−01</td><td align="left">5.26E−01</td><td align="left">6.01E−01</td><td align="left">7.22E−01</td><td align="left">4.04E−01</td><td align="left">1.29E−01</td></tr><tr><td align="left">Kendall’s tau</td><td align="left">4.20E−01</td><td align="left">3.61E−01</td><td align="left">4.23E−01</td><td align="left">5.54E−01</td><td align="left">3.04E−01</td><td align="left">1.06E−01</td></tr></tbody></table></table-wrap></p>
                <p id="d1e4574">ODANN consistently achieved the highest predictive accuracy from its testing step, even when random missing values were added into the dataset. Missing data commonly occurs during data collection/collation, which can generally reduce the overall representation of the samples and even cause biased estimations. Therefore, it is important to investigate how the proposed ODANN model, coupled with data assimilation component, and the other alternative time-series models respond to different percentages of missing data in their respective prediction task. To set the missing data condition, we randomly dropped 10%, 20%, and 30% of the G values data from the original dataset, and then simply filled out the corresponding missing values using its available previous day of observed G value. For the computational experiments, we randomly generate 30 combinations for each of the above-listed missing percentages of the data points for the G value. After the required data imputations for each of the combination, we maintained the same respective optimal model configurations, as listed in <xref rid="tbl8" ref-type="table">Table 8</xref>, for each of the time-series model, inclusive of ODANN, to forecast the same G parameter over time with the same lead-time of 1 day. Note that ODANN still leveraged on the optimal batch size of 4, input features layer size of 500 for CBOW features extraction method, and a rolling time-window size of 5 days for assimilating the historical time records for the G parameter to perform the prediction step.</p>
                <p id="d1e4580"><xref rid="tbl10" ref-type="table">Table 10</xref>, <xref rid="tbl11" ref-type="table">Table 11</xref> summarizes the average RMSE and MAE scores, coupled with their standard deviation values (also see <xref rid="fig17" ref-type="fig">Fig. 17</xref>, <xref rid="fig18" ref-type="fig">Fig. 18</xref>), computed for all time-series models from the 30 random combinations for each of the missing data percentages as shown. As expected, the accuracy gradually reduced with an increasing amount of missing data. Under the condition of 10% missing data, ODANN remained more accurate in its predictions on the testing dataset, with RMSE and MAE scores of 0.00315 and 0.00246 respectively, as compared to that of the other models. Both computed scores were generally lower than the 2nd lowest corresponding values attained from the SVM model (the next best prediction model). As the percentage of missing data increased, the comparative differences for the RMSE and MAE between our proposed model and SVM, however, reduced gradually as shown in <xref rid="tbl10" ref-type="table">Table 10</xref>, <xref rid="tbl11" ref-type="table">Table 11</xref>. Notwithstanding the missing data quantity for the random combinations, ODANN appeared to provide the highest accuracy capability with the lowest RMSE and MAE average values after the 30 experiments for each of the missing data percentages investigated. In all missing data scenarios experimented, there was no obvious change in the resulting accuracy of ARIMA and SVM, hence indicating the convergence in the predictive accuracy of the models. On the contrary, it can be observed from their corresponding length of the plot boxes (<xref rid="fig17" ref-type="fig">Fig. 17</xref>, <xref rid="fig18" ref-type="fig">Fig. 18</xref>) and computed standard deviation values that RF, LSTM, AutoARIMA, Prophet, and ODANN generally experienced more fluctuations, hence their prediction results may encompass greater level of uncertainty. This, however, can be appropriately addressed by running more random combinations of the missing data percentages in the future studies.</p>
                <p id="d1e4599">
                  <table-wrap position="float" id="tbl10">
                    <label>Table 10</label>
                    <caption>
                      <p>Comparison of RMSE under different percentage of missing data.</p>
                    </caption>
                    <table frame="hsides" rules="groups">
                      <thead>
                        <tr>
                          <th align="left">Method</th>
                          <th colspan="3" align="left">RMSE under different percentages of missing data<hr/></th>
                        </tr>
                        <tr>
                          <th align="left"/>
                          <th align="left">10%</th>
                          <th align="left">20%</th>
                          <th align="left">30%</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left">ARIMA</td>
                          <td align="left">4.19E−03 (<inline-formula><mml:math id="d1e1953" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.37E−05)</td>
                          <td align="left">4.21E−03 (<inline-formula><mml:math id="d1e1961" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.71E−05)</td>
                          <td align="left">4.23E−03 (<inline-formula><mml:math id="d1e1969" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>4.88E−05)</td>
                        </tr>
                        <tr>
                          <td align="left">AutoARIMA</td>
                          <td align="left">7.96E−03 (<inline-formula><mml:math id="d1e1980" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.67E−03)</td>
                          <td align="left">8.05E−03 (<inline-formula><mml:math id="d1e1988" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.62E−03)</td>
                          <td align="left">9.35E−03 (<inline-formula><mml:math id="d1e1996" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.89E−03)</td>
                        </tr>
                        <tr>
                          <td align="left">RF</td>
                          <td align="left">4.69E−03 (<inline-formula><mml:math id="d1e2007" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>5.88E−04)</td>
                          <td align="left">4.76E−03 (<inline-formula><mml:math id="d1e2015" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>5.51E−04)</td>
                          <td align="left">4.99E−03 (<inline-formula><mml:math id="d1e2023" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.15E−04)</td>
                        </tr>
                        <tr>
                          <td align="left">SVM</td>
                          <td align="left">4.11E−03 (<inline-formula><mml:math id="d1e2034" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>1.57E−04)</td>
                          <td align="left">4.13E−03 (<inline-formula><mml:math id="d1e2042" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>1.82E−04)</td>
                          <td align="left">4.21E−03 (<inline-formula><mml:math id="d1e2050" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.33E−04)</td>
                        </tr>
                        <tr>
                          <td align="left">LSTM</td>
                          <td align="left">4.22E−03 (<inline-formula><mml:math id="d1e2061" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.30E−04)</td>
                          <td align="left">4.46E−03 (<inline-formula><mml:math id="d1e2069" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>7.17E−04)</td>
                          <td align="left">4.48E−03 (<inline-formula><mml:math id="d1e2077" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>7.75E−04)</td>
                        </tr>
                        <tr>
                          <td align="left">Prophet</td>
                          <td align="left">3.16E−02 (<inline-formula><mml:math id="d1e2088" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.66E−03)</td>
                          <td align="left">3.21E−02 (<inline-formula><mml:math id="d1e2096" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.17E−03)</td>
                          <td align="left">3.32E−02 (<inline-formula><mml:math id="d1e2104" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>7.25E−03)</td>
                        </tr>
                        <tr>
                          <td align="left">ODANN</td>
                          <td align="left">3.15E−03 (<inline-formula><mml:math id="d1e2115" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>1.60E−03)</td>
                          <td align="left">4.10E−03 (<inline-formula><mml:math id="d1e2123" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.17E−03)</td>
                          <td align="left">4.29E−03 (<inline-formula><mml:math id="d1e2131" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.13E−04)</td>
                        </tr>
                      </tbody>
                    </table>
                  </table-wrap>
                  <table-wrap position="float" id="tbl11">
                    <label>Table 11</label>
                    <caption>
                      <p>Comparison of MAE under different percentage of missing data.</p>
                    </caption>
                    <table frame="hsides" rules="groups">
                      <thead>
                        <tr>
                          <th align="left">Method</th>
                          <th colspan="3" align="left">MAE under different percentages of missing data<hr/></th>
                        </tr>
                        <tr>
                          <th align="left"/>
                          <th align="left">10%</th>
                          <th align="left">20%</th>
                          <th align="left">30%</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left">ARIMA</td>
                          <td align="left">3.44E−03 (<inline-formula><mml:math id="d1e2168" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.98E−05)</td>
                          <td align="left">3.45E−03 (<inline-formula><mml:math id="d1e2176" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.19E−05)</td>
                          <td align="left">3.46E−03 (<inline-formula><mml:math id="d1e2184" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.23E−05)</td>
                        </tr>
                        <tr>
                          <td align="left">AutoARIMA</td>
                          <td align="left">6.66E−03 (<inline-formula><mml:math id="d1e2195" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.20E−03)</td>
                          <td align="left">6.93E−03 (<inline-formula><mml:math id="d1e2203" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.47E−03)</td>
                          <td align="left">8.09E−03 (<inline-formula><mml:math id="d1e2211" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.44E−03)</td>
                        </tr>
                        <tr>
                          <td align="left">RF</td>
                          <td align="left">3.53E−03 (<inline-formula><mml:math id="d1e2222" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.82E−04)</td>
                          <td align="left">3.58E−03 (<inline-formula><mml:math id="d1e2230" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.87E−04)</td>
                          <td align="left">3.64E−03 (<inline-formula><mml:math id="d1e2238" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>4.08E−04)</td>
                        </tr>
                        <tr>
                          <td align="left">SVM</td>
                          <td align="left">3.32E−03 (<inline-formula><mml:math id="d1e2249" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>1.32E−04)</td>
                          <td align="left">3.33E−03 (<inline-formula><mml:math id="d1e2257" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>1.71E−04)</td>
                          <td align="left">3.36E−03 (<inline-formula><mml:math id="d1e2265" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.06E−04)</td>
                        </tr>
                        <tr>
                          <td align="left">LSTM</td>
                          <td align="left">3.50E−03 (<inline-formula><mml:math id="d1e2276" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.34E−04)</td>
                          <td align="left">3.61E−03 (<inline-formula><mml:math id="d1e2284" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>7.76E−04)</td>
                          <td align="left">3.69E−03 (<inline-formula><mml:math id="d1e2292" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>7.73E−04)</td>
                        </tr>
                        <tr>
                          <td align="left">Prophet</td>
                          <td align="left">3.07E−02 (<inline-formula><mml:math id="d1e2303" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>3.76E−03)</td>
                          <td align="left">3.13E−02 (<inline-formula><mml:math id="d1e2311" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>6.40E−03)</td>
                          <td align="left">3.22E−02 (<inline-formula><mml:math id="d1e2319" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>7.46E−03)</td>
                        </tr>
                        <tr>
                          <td align="left">ODANN</td>
                          <td align="left">2.46E−03 (<inline-formula><mml:math id="d1e2330" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>1.32E−03)</td>
                          <td align="left">3.30E−03 (<inline-formula><mml:math id="d1e2338" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>2.08E−03)</td>
                          <td align="left">3.34E−03 (<inline-formula><mml:math id="d1e2346" display="inline" altimg="si79.svg"><mml:mo>±</mml:mo></mml:math></inline-formula>5.79E−04)</td>
                        </tr>
                      </tbody>
                    </table>
                  </table-wrap>
                  <fig id="fig17">
                    <label>Fig. 17</label>
                    <caption>
                      <p>Boxplot of RMSE under different percentages of missing data: (a) 10%; (b) 20%; (c) 30%.</p>
                    </caption>
                    <graphic xlink:href="gr17_lrg"/>
                  </fig>
                  <fig id="fig18">
                    <label>Fig. 18</label>
                    <caption>
                      <p>Boxplot of MAE under different percentages of missing data: (a) 10%; (b) 20%; (c) 30%.</p>
                    </caption>
                    <graphic xlink:href="gr18_lrg"/>
                  </fig>
                </p>
              </sec>
              <sec id="sec5.2">
                <label>5.2</label>
                <title>Novelty of ODANN model</title>
                <p id="d1e4613">We further demonstrate the novelty of ODANN model by considering the proposed model’s capability to assimilate additional data features of any pre-defined characteristics into the optimal locations of the available hidden layers within the DNN model. Previously, we have shown the effectiveness of assimilating 5 days of historical time-records, i.e. (<inline-formula><mml:math id="d1e4617" display="inline" altimg="si121.svg"><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula>), for the G parameter with the inherent lead-time of 1 day, where we can best minimize the RMSE and MAE scores as compared to the other time-series models as summarized in <xref rid="tbl8" ref-type="table">Table 8</xref>. To further improve on ODANN’s predictive accuracy to forecast the G parameter on a given day, we consider the scenario where we assimilate ODANN’s hidden layers with relevant socioeconomic factors and restrictive government policies pertaining to COVID-19 for the same modelling step. The additional factors to be considered are listed in <xref rid="tbl12" ref-type="table">Table 12</xref> for the same period between 25 January 2020 and 11 May 2020. For all factors, except for the government stringency index, they are typically represented as non-discrete values, i.e. not time-series continuous values, as shown in <xref rid="tbl12" ref-type="table">Table 12</xref>. Hence, we normalize the classes into discrete values via Equation <xref rid="fd5" ref-type="disp-formula">(5)</xref> which considers the data distribution among the different classes. <disp-formula id="fd5"><label>(5)</label><mml:math id="d1e4683" display="block" altimg="si122.svg"><mml:mrow><mml:mover accent="false" class="mml-overline"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e4706" display="inline" altimg="si123.svg"><mml:mover accent="false" class="mml-overline"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math></inline-formula> represents the normalized class discrete value, <inline-formula><mml:math id="d1e4716" display="inline" altimg="si124.svg"><mml:mi>X</mml:mi></mml:math></inline-formula> is the class score (e.g. Class 1, 2, 3, etc.), <inline-formula><mml:math id="d1e4721" display="inline" altimg="si125.svg"><mml:mi>μ</mml:mi></mml:math></inline-formula> the mean class value, and <inline-formula><mml:math id="d1e4726" display="inline" altimg="si126.svg"><mml:mi>σ</mml:mi></mml:math></inline-formula> the standard deviation of the class scores for the respective factor/policy. We note that the government stringency indexes are also normalized via Equation <xref rid="fd5" ref-type="disp-formula">(5)</xref> to ensure similar scaling values for all considered input data features.</p>
                <p id="d1e4734"><xref rid="fig19" ref-type="fig">Fig. 19</xref> illustrates the design of ODANN to concurrently assimilate the historical time-series records for the G parameter and the listed socioeconomic factors and restrictive government policies, based upon the defined rolling time-window size, to model and forecast the G value on any given day with a lead-time of 1 day. We again maintain the same optimal model configuration (batch size <inline-formula><mml:math id="d1e4740" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 2, input features layer size <inline-formula><mml:math id="d1e4745" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 500, CBOW features extraction method, rolling time-window size <inline-formula><mml:math id="d1e4750" display="inline" altimg="si28.svg"><mml:mo>=</mml:mo></mml:math></inline-formula> 5 days) to perform the new scenario of assimilating the additional data features from <xref rid="tbl12" ref-type="table">Table 12</xref> for the modelling step. For example, by adhering to the same rolling time-window size of 5 days, the total number of new neurons for assimilating the additional data features into the selected hidden layer of ODANN equates to 55 as shown in <xref rid="fig19" ref-type="fig">Fig. 19</xref>. The same model training (and validation), and testing datasets of 85% and 15% respectively, are also maintained for the analysis. By including the additional data features, we can further reduce the original RMSE and MAE scores (from <xref rid="tbl8" ref-type="table">Table 8</xref>) for ODANN to around 0.002000 and 0.00154 respectively, hence improving the model’s predictive accuracy for the near real-time predictions. Overall, the novelty of our proposed ODANN can be summarized as follows:</p>
                <p id="d1e4767">
                  <list list-type="simple" id="d1e4769">
                    <list-item id="lst29">
                      <label>•</label>
                      <p id="d1e4773">The current model design of ODANN is built to enable diverse data components to be fused systematically and effectively, as part our data assimilation/fusion step, hence preventing any data component to outweigh the others. For example, we avoid directly assimilating the encoded semantic word representations derived from the Twitter data, having the defined vector size, into the same hidden layer as that of the historical time-records for the G parameter due to the differing scales in their respective 1D array sizes. The vector size for the encoded Twitter data and assimilated historical time-records for the G parameter are in the ordering scales of <inline-formula><mml:math id="d1e4776" display="inline" altimg="si130.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4804" display="inline" altimg="si131.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, hence the differing scales are likely to exert more weights on the encoded Twitter data during the training phase of ODANN when both are fused directly together. Therefore, the derived word representation (1D array) is instead leveraged as the input features layer for ODANN which enables the systematic aggregation of the original word vectors into higher-level useful features of smaller vector sizes (as shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>, <xref rid="fig19" ref-type="fig">Fig. 19</xref>), along the depth of the ODANN model, having the same scaling as that of the assimilated data features at the selected hidden layer of ODANN in <xref rid="fig19" ref-type="fig">Fig. 19</xref>. By doing so, we have shown that we can maximize the predictive accuracy of ODANN on the testing dataset for forecasting the G parameter with a lead-time of 1 day.</p>
                    </list-item>
                    <list-item id="lst30">
                      <label>•</label>
                      <p id="d1e4832">In this latest scenario, we demonstrate the flexibility of ODANN to assimilate other important data features (from <xref rid="tbl12" ref-type="table">Table 12</xref>) which can be useful to enhance its predictive accuracy, while ensuring a well-balanced weights distribution among the different types of data features. While the number of assimilated neurons for the additional socioeconomic factors and restrictive government policies may exceed the number of assigned neurons for the aggregated word vectors and the historical time-series records for the G parameter, we highlight that each data feature in <xref rid="tbl12" ref-type="table">Table 12</xref> is actually a unique parameter by itself hence there is still no one singular parameter being placed more significance than the others. Going forward, as more data is being available to the community which include new data features (vaccination rates, environmental factors, etc.), the present design of ODANN can easily assimilate those new features to perform the same modelling analysis with equal efficiency.</p>
                    </list-item>
                    <list-item id="lst31">
                      <label>•</label>
                      <p id="d1e4845">While the design of ODANN may be relatively straightforward from neural network research, the better agreement achieved from ODANN as compared to other time-series models, as summarized in <xref rid="tbl8" ref-type="table">Table 8</xref>, <xref rid="tbl10" ref-type="table">Table 10</xref>, <xref rid="tbl11" ref-type="table">Table 11</xref>, clearly underline the effectiveness of our proposed workflow/approach by having a singular end-to-end network model to concomitantly process complex semantic word vectors, as representative of the society’s emotional responses towards the pandemic, and a multitude of socioeconomic and governmental factors in a single-shot learning and validation process. Accuracy and computational efficiency have both been achieved with ODANN, and we are hopeful that the same workflow and model design can be extended to other domain problems which models a target objective as function of multiple data features of varying characteristics.</p>
                    </list-item>
                  </list>
                </p>
                <p id="d1e4851">
                  <table-wrap position="float" id="tbl12">
                    <label>Table 12</label>
                    <caption>
                      <p>List of socioeconomic factors and restrictive government policies for additional data features assimilation into ODANN.</p>
                    </caption>
                    <table frame="hsides" rules="groups">
                      <thead>
                        <tr>
                          <th align="left">Type</th>
                          <th align="left">Data features</th>
                          <th align="left">Value range</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td rowspan="5" align="left">Socioeconomic</td>
                          <td align="left">Level of income support<hr/></td>
                          <td align="left"><bold>3 classes</bold> (no support, support <inline-formula><mml:math id="d1e2379" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula>50% loss salary, support <inline-formula><mml:math id="d1e2384" display="inline" altimg="si133.svg"><mml:mo>&lt;</mml:mo></mml:math></inline-formula>50% loss salary)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left">Level of debt relief<hr/></td>
                          <td align="left"><bold>3 classes</bold> (no relief, moderate relief, large relief)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left">Face covering<hr/></td>
                          <td align="left"><bold>5 classes</bold> (no policy, recommended, required in some public places, required in all public places, always required outside of homes)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left">Cancellation of public events and gatherings<hr/></td>
                          <td align="left"><bold>3 classes</bold> (no cancellations, recommended cancellations, compulsory cancellations)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left">Testing and contact tracing<hr/></td>
                          <td align="left"><bold>4 classes</bold> (no testing, testing for those with symptoms and belonging to higher-risk groups, testing for anyone with symptoms, open public testing)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left"/>
                          <td align="left">Public information campaigns</td>
                          <td align="left"><bold>3 classes</bold> (none, public officials urging caution, coordinated information campaigns)</td>
                        </tr>
                        <tr>
                          <td colspan="3">
                            <hr/>
                          </td>
                        </tr>
                        <tr>
                          <td rowspan="3" align="left">Restrictive government policies</td>
                          <td align="left">Government stringency index<hr/></td>
                          <td align="left"><bold>0–100</bold> (100 being the highest stringency score)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left">Schools and workplaces closures<hr/></td>
                          <td align="left"><bold>4 classes</bold> (no closures, recommended, recommended at some levels, required at all levels)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left">Stay-at-home restrictions<hr/></td>
                          <td align="left"><bold>4 classes</bold> (no measures, recommended, required except for running essentials, required with few exceptions)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left"/>
                          <td align="left">Controls for domestic travels<hr/></td>
                          <td align="left"><bold>3 classes</bold> (no measures, recommended movement restrictions, restricted movement)<hr/></td>
                        </tr>
                        <tr>
                          <td align="left"/>
                          <td align="left">Controls for international travels</td>
                          <td align="left"><bold>5 classes</bold> (no measures, screening, quarantine from high-risk countries, ban on high-risk countries, total border closure)</td>
                        </tr>
                      </tbody>
                    </table>
                  </table-wrap>
                  <fig id="fig19">
                    <label>Fig. 19</label>
                    <caption>
                      <p>Illustration of ODANN to assimilate historical time-series records for G parameter and other socioeconomic factors and restrictive governmental policies (from <xref rid="tbl11" ref-type="table">Table 11</xref>) for any given rolling time-window size.</p>
                    </caption>
                    <graphic xlink:href="gr19_lrg"/>
                  </fig>
                </p>
              </sec>
              <sec id="sec5.3">
                <label>5.3</label>
                <title>Comparison of ODANN with previous research studies</title>
                <p id="d1e4861">Besides comparing ODANN with other time-series prediction models as explained qualitatively and quantitatively in the preceding sub-section, we further validate the predictive accuracy of ODANN with recent similar notable studies <xref rid="b42" ref-type="bibr">[42]</xref>, <xref rid="b43" ref-type="bibr">[43]</xref>, <xref rid="b44" ref-type="bibr">[44]</xref>, <xref rid="b45" ref-type="bibr">[45]</xref>, <xref rid="b46" ref-type="bibr">[46]</xref>, which too focused on forecasting the transmission rate of COVID-19, in terms of the number of confirmed COVID-19 cases, since the virus’ inception. In the following, we outlined the key methodologies and reported results by the previous studies <xref rid="b42" ref-type="bibr">[42]</xref>, <xref rid="b43" ref-type="bibr">[43]</xref>, <xref rid="b44" ref-type="bibr">[44]</xref>, <xref rid="b45" ref-type="bibr">[45]</xref>, <xref rid="b46" ref-type="bibr">[46]</xref>, for comparison with our proposed ODANN model and its prediction results.</p>
                <p id="d1e4871">Kumar and Susan <xref rid="b42" ref-type="bibr">[42]</xref> too leveraged on ARIMA and Prophet time-series prediction models to model the temporal data of COVID-19 spread worldwide, and for several countries in the different continents, for the period between 22 January 2020 and 20 May 2020. Overall, the authors demonstrated that ARIMA model was generally more effective for forecasting the prevalence rate of COVID-19, which too aligned with our present study where we have shown that ARIMA/AutoARIMA had resulting error scores (RMSE and MAE) which were a magnitude smaller than that of Prophet as previous summarized in <xref rid="tbl8" ref-type="table">Table 8</xref>. A more similar study in using social media to forecast the virus outbreak with neural ordinary differential equations (ODEs) was performed by Núñez et al. <xref rid="b44" ref-type="bibr">[44]</xref>. The authors’ data comprised of a massive amount of online surveys, regarding COVID-19 symptoms, via Facebook to train and validate the authors’ personalized neural ODE, followed by using the trained neural ODE to forecast the virus’ outbreak rate in different US states for up to sixty days. No error metric scores were reported by the authors in their published paper, however, a visual inspection of their temporal plots for comparing their model’s predictions and the respective monitored data during the extrapolation phase, i.e. model testing, indicates some level of differences between both sets of data. In addition, the authors did not extend their prediction model for the forecasting on the global scale. A more aggregated analysis was carried out by Yousefinaghani et al. <xref rid="b45" ref-type="bibr">[45]</xref> to detect spikes/waves in the number of confirmed cases in the United States and Canada using social media data and Google searches online. Their adopted lead-time adopted ranged between 1 and 2 weeks which can be useful to decision-makers to early detect any possible spikes, however, the authors did not specifically investigated or reported on the number of false positive (FPs) forecasted hence the exact precision of their method is still not known at this stage. Overall, there are currently limited studies reported in the current literature which leverages on the big-data availability on social media (Facebook, Twitter, etc.) to model and forecast the actual number of reported/confirmed COVID-19 cases globally, as presented in our present study.</p>
                <p id="d1e4890">The remaining 2 previous studies <xref rid="b43" ref-type="bibr">[43]</xref>, <xref rid="b46" ref-type="bibr">[46]</xref> provided a more comprehensive comparison with our present results attained from ODANN. We first note that each of the studies, inclusive of ours, models the growth rate in the number of confirmed COVID-19 cases globally via different means. Papastefanopoulos et al. <xref rid="b43" ref-type="bibr">[43]</xref> modelled the number of active cases per unit population size in the top 10 countries having the largest number of reported COVID-19 cases as of 4 May 2020. The countries include the United States (US), Spain, Italy, United Kingdom (UK), France, Germany, Russia, Turkey, Brazil, and Iran. Their study is selected for comparison since the above-listed countries encompass the most significant portion (<inline-formula><mml:math id="d1e4902" display="inline" altimg="si132.svg"><mml:mo>&gt;</mml:mo></mml:math></inline-formula>70%) of the confirmed COVID-19 cases globally, which has also been underlined by the authors in their paper. They reported the respective predictive performance using different time-series prediction models (ARIMA, Prophet, HWAAS, NBEATS, Gluonts, and TBAT) for the 10 listed countries by computing the RMSE score (same of Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref>) based on the predictions made from their model testing step using data instances between 28 April 2020 and 4 May 2020. <xref rid="tbl13" ref-type="table">Table 13</xref> summarizes the RMSE scores from each of their method used, where the respective RMSE score, as shown in the table, represented the average of all 10 countries combined from the authors’ model testing step. Our updated RMSE scores in <xref rid="tbl13" ref-type="table">Table 13</xref> were based upon same the same working principles as that of <xref rid="b43" ref-type="bibr">[43]</xref> where we re-normalized the reported and forecasted confirmed number of COVID-19 cases, from our prediction step using ODANN, by the total world population (<inline-formula><mml:math id="d1e4927" display="inline" altimg="si12.svg"><mml:mo>≈</mml:mo></mml:math></inline-formula> 7.67 billion people as of 2020/2021) for the same period analysed by the authors. We note that prior de-normalization of the predicted and reported G values were performed by using the inverse version of Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>. Also, the de-normalization step was carried out after the predictions were made on the proposed G parameter using our ODANN model, as previously summarized in the preceding section(s). By performing the appropriate estimations, we can then better compare and evaluate our model performance from ODANN’s with that of <xref rid="b43" ref-type="bibr">[43]</xref> in the same ordering scale. The listed RMSE scores in <xref rid="tbl13" ref-type="table">Table 13</xref> show that our proposed ODANN, with the optimized model configuration (5 days rolling time-window size, 500 neurons for input layer, etc.) is more likely to perform better than the other time-series models from the previous study, and also in our current analysis from <xref rid="tbl8" ref-type="table">Table 8</xref> as demonstrated earlier, when modelling the contributory influences of large-scale social media data as the input features layer to ODANN, and also assimilating with the historical records for the G parameter and/or other important socioeconomic and governmental factors/data if necessary.</p>
                <p id="d1e4949">On the other hand, Petropoulos et al. <xref rid="b46" ref-type="bibr">[46]</xref> adopted a simpler time-series model, namely the non-seasonal multiplicative error and multiplicative trend exponential smoothing model (ETS-MMN), by modelling and forecasting the global numbers of confirmed COVID-19 cases and deaths with different number of horizons, i.e. lead-times. We focused on their results derived from the 1-day horizon to match with our present analysis due to the inherent lead-time of 1 day adopted to perform the required predictions. The authors leveraged on the mean absolute percentage error (MAPE) score, as defined in Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>, to evaluate their forecasting results for the period between 10 February 2020 and 30 May 2020 using different time horizons. For example, a 10-days horizon indicates that the predictions were made on 10 February 2020, 20 February 2020, etc., with a 10 days interval. By far, their MAPE score for the period of May 2020, which aligned with the dates for the testing phase of ODANN in our present study, was reported to be approximately 0.200% with the 1-day time horizon. To better compare and evaluate ODANN’s predictions with that of <xref rid="b46" ref-type="bibr">[46]</xref>, we again de-normalized the reported and predictions values made from ODANN by again using the inverse version of Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>, followed by computing the appropriate MAPE scores using Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>. Again, we note that the de-normalization step was carried out after the predictions were made on the proposed G parameter using ODANN. Overall, we can observe that the predictions derived from the simpler time-series model as proposed by Petropoulos et al. can match almost exactly with that of our ODANN model with the data assimilation component, where their computed MAPE scores were almost identical as shown in <xref rid="tbl13" ref-type="table">Table 13</xref>. However, by fusing with data features pertaining to the socioeconomic and governmental factors, the resulting MAPE score from ODANN can be reduced further by around 0.045%, hence improving the forecasting step. <disp-formula id="fd6"><label>(6)</label><mml:math id="d1e4987" display="block" altimg="si136.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>100</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></disp-formula>
</p>
                <p id="d1e5071">
                  <table-wrap position="anchor" id="tbl13">
                    <label>Table 13</label>
                    <caption>
                      <p>Comparative analysis of ODANN’s predictive accuracy with previous studies.</p>
                    </caption>
                    <table frame="hsides" rules="groups">
                      <thead>
                        <tr>
                          <th align="left">Method</th>
                          <th align="left">RMSE</th>
                          <th align="left">MAPE</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left">ARIMA <xref rid="b43" ref-type="bibr">[43]</xref></td>
                          <td align="left">1.75E−02</td>
                          <td rowspan="5" align="left">
                            <bold>-Nil-</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left">Prophet <xref rid="b43" ref-type="bibr">[43]</xref></td>
                          <td align="left">3.00E−02</td>
                        </tr>
                        <tr>
                          <td align="left">HWAAS <xref rid="b43" ref-type="bibr">[43]</xref></td>
                          <td align="left">2.40E−02</td>
                        </tr>
                        <tr>
                          <td align="left">BNEATS <xref rid="b43" ref-type="bibr">[43]</xref></td>
                          <td align="left">2.11E−02</td>
                        </tr>
                        <tr>
                          <td align="left">Gluonts <xref rid="b43" ref-type="bibr">[43]</xref></td>
                          <td align="left">4.45E−02</td>
                        </tr>
                        <tr>
                          <td align="left">TBAT <xref rid="b43" ref-type="bibr">[43]</xref></td>
                          <td align="left">6.99E−03</td>
                          <td align="left"/>
                        </tr>
                        <tr>
                          <td align="left">Simple-time series model <xref rid="b46" ref-type="bibr">[46]</xref></td>
                          <td align="left">
                            <bold>-Nil-</bold>
                          </td>
                          <td align="left">0.200%</td>
                        </tr>
                        <tr>
                          <td align="left">ODANN w/o SEG<xref rid="tblfn13a" ref-type="table-fn">a</xref> factors (optimized configuration)</td>
                          <td align="left">3.45E−03</td>
                          <td align="left">0.205%</td>
                        </tr>
                        <tr>
                          <td align="left">ODANN with SEG<xref rid="tblfn13a" ref-type="table-fn">a</xref> factors (optimized configuration)</td>
                          <td align="left">2.63E−03</td>
                          <td align="left">0.154%</td>
                        </tr>
                      </tbody>
                    </table>
                    <table-wrap-foot>
                      <fn id="tblfn13a">
                        <label>a</label>
                        <p id="d1e2580">SEG factors represent socioeconomic and governmental restrictive policies/factors.</p>
                      </fn>
                    </table-wrap-foot>
                  </table-wrap>
                </p>
              </sec>
            </sec>
            <sec id="sec6">
              <label>6</label>
              <title>Conclusions and future works</title>
              <p id="d1e5079">In this paper, we have developed a hybrid deep learning model, termed as ODANN, which effectively combines features extraction methods, via natural language processing (NLP), and data assimilation concept to accurately predict the daily growth rate in the number of confirmed COVID-19 cases globally, via a proposed G parameter, with a lead-time of 1 day. NLP features extraction methods were leveraged to pre-process large volumes of Twitter data (100 million in exceedance) to derive high-level semantic word vectors to quantify the general community’s emotional responses towards the current pandemic. Coupled with data assimilation, we demonstrated that ODANN can outperform traditional time-series models, including ARIMA, RF, SVM, LSTM, AutoARIMA, and Prophet, in forecasting the G parameter ranging between 25 January 2020 and 11 May 2020. Specifically, by learning from the historical time-series records for the G parameter using a rolling time-window size of 5 days for the data assimilation component, and fusing with the aggregated word vectors derived from large volumes of Twitter data at specific hidden layer(s) of the deep learning model which ensured a well-balanced weights distributions of the data features, ODANN can forecast the G parameter, with a lead-time of 1 day, having average RMSE and MAE scores of 0.00282 and 0.00214 respectively.</p>
              <p id="d1e5081">The novel contributions from the study mainly consist of the following aspects, namely: (a) combining NLP features extraction methods with neural network prediction model to forecast the global spread of COVID-19 over time. By considering the community’s daily aggregated emotional responses towards the pandemic, our proposed ODANN model shows superiority in both accuracy and robustness, even towards missing data conditions. Information from Twitter offers valuable insights into people’s emotional responses towards COVID-19, which has been proven to be useful to maximize the accuracy performance of our predictive model when coupled with data assimilation at the selected hidden layer(s) of the deep learning model. Moreover, ODANN can still maintain relatively low error scores, even when 30% of random missing values were artificially introduced into the original dataset; and (b) ability of ODANN to easily assimilate or fuse with other socioeconomic and governmental factors of varying characteristics, which can further enhance the model’s predictive accuracy by ensuring a well-balanced weightage distributions among the assigned neurons in the deep learning model. We note that this model design or framework for assimilating different types of data features, while balancing the resulting weightage distributions for the different types of data features within a single-shot learning process, has not been explored by far for COVID-19 related predictions.</p>
              <p id="d1e5083">In our future works, we intend to assimilate contemporary data after 11 May 2020 to perform near real-time predictions. Since the sentiments towards COVID-19 change rapidly, we can conduct semantic analysis on informative texts from other popular social media platforms (i.e., Twitter, Facebook, Weibo, etc.). The proposed approach has great potential to reveal the psychological responses of the public towards COVID-19, hence identifying possible social concerns to forecast the global emotional evolution towards the current pandemic. Besides, social network analysis (SNA) based upon graph theory is another method to monitor the spread of information on social media <xref rid="b47" ref-type="bibr">[47]</xref>. For example, we can perform SNA on COVID-19 related Twitter data to examine the dynamic and complex patterns of public health information to assist governments to control the volume of false or inaccurate information pertaining to COVID-19 on the internet.</p>
            </sec>
            <sec id="d1e5089">
              <title>CRediT authorship contribution statement</title>
              <p id="d1e5092"><bold>Alvin Wei Ze Chew:</bold> Conceptualization, Data curation, Resources, Methodology, Software, Formal Analysis, Validation, Investigation, Writing - Original Draft, Writing - review &amp; editing. <bold>Yue Pan:</bold> Methodology, Software, Formal analysis, Validation, Investigation, Writing – original draft. <bold>Ying Wang:</bold> Data curation, Resources, Software, Formal analysis, Validation. <bold>Limao Zhang:</bold> Conceptualization, Investigation, Resources, Writing – review &amp; editing.</p>
            </sec>
            <sec sec-type="COI-statement">
              <title>Declaration of Competing Interest</title>
              <p id="d1e5108">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
            </sec>
          </body>
          <back>
            <ref-list id="bib1">
              <title>References</title>
              <ref id="b1">
                <label>1</label>
                <element-citation publication-type="journal" id="sb1">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Huang</surname>
                      <given-names>C.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Clinical features of patients infected with 2019 novel coronavirus in wuhan, China</article-title>
                  <source>Lancet</source>
                  <volume>395</volume>
                  <issue>10223</issue>
                  <year>2020</year>
                  <fpage>497</fpage>
                  <lpage>506</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0140-6736(20)30183-5</pub-id>
                  <pub-id pub-id-type="pmid">31986264</pub-id>
                </element-citation>
              </ref>
              <ref id="b2">
                <label>2</label>
                <element-citation publication-type="journal" id="sb2">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ali</surname>
                      <given-names>I.</given-names>
                    </name>
                    <name>
                      <surname>Alharbi</surname>
                      <given-names>O.M.L.</given-names>
                    </name>
                  </person-group>
                  <article-title>COVID-19: Disease, management, treatment, and social impact</article-title>
                  <source>Sci. Total Environ.</source>
                  <volume>728</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">138861</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.138861</pub-id>
                </element-citation>
              </ref>
              <ref id="b3">
                <label>3</label>
                <element-citation publication-type="journal" id="sb3">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Aldaco</surname>
                      <given-names>R.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Food waste management during the COVID-19 outbreak: a holistic climate, economic and nutritional approach</article-title>
                  <source>Sci. Total Environ.</source>
                  <volume>742</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">140524</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.140524</pub-id>
                </element-citation>
              </ref>
              <ref id="b4">
                <label>4</label>
                <element-citation publication-type="journal" id="sb4">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chew</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Wang</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>l.</given-names>
                    </name>
                  </person-group>
                  <article-title>Correlating dynamic climate conditions and socioeconomic-governmental factors to spatiotemporal spread of COVID-19 via semantic segmentation deep learning analysis</article-title>
                  <source>Sustain. Cities Soc.</source>
                  <volume>75</volume>
                  <year>2021</year>
                  <object-id pub-id-type="publisher-id">103231</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.scs.2021.103231</pub-id>
                </element-citation>
              </ref>
              <ref id="b5">
                <label>5</label>
                <element-citation publication-type="journal" id="sb5">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Liu</surname>
                      <given-names>T.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Time-varying transmission dynamics of novel coronavirus pneumonia in China</article-title>
                  <source>BioRxiv</source>
                  <year>2020</year>
                  <pub-id pub-id-type="doi">10.1101/2020.01.25.919787</pub-id>
                  <comment>2020.01.25.919787</comment>
                </element-citation>
              </ref>
              <ref id="b6">
                <label>6</label>
                <element-citation publication-type="journal" id="sb6">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pan</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Yan</surname>
                      <given-names>Z.</given-names>
                    </name>
                    <name>
                      <surname>Lwin M.O.</surname>
                      <given-names>M.J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Discovering optimal strategies for mitigating COVID-19 spread using machine learning: Experience from Asia</article-title>
                  <source>Sustain. Cities Soc.</source>
                  <volume>75</volume>
                  <year>2021</year>
                  <fpage>103254</fpage>
                  <lpage>103306</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.scs.2021.103254</pub-id>
                  <pub-id pub-id-type="pmid">34414067</pub-id>
                </element-citation>
              </ref>
              <ref id="b7">
                <label>7</label>
                <element-citation publication-type="journal" id="sb7">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Unkel</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Farrington</surname>
                      <given-names>C.P.</given-names>
                    </name>
                    <name>
                      <surname>Garthwaite</surname>
                      <given-names>P.H.</given-names>
                    </name>
                    <name>
                      <surname>Robertson</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Andrews</surname>
                      <given-names>N.</given-names>
                    </name>
                  </person-group>
                  <article-title>Statistical methods for the prospective detection of infectious disease outbreaks: a review</article-title>
                  <source>J. R. Stat. Soc. Ser. A (Stat. Soc.)</source>
                  <volume>175</volume>
                  <issue>1</issue>
                  <year>2012</year>
                  <fpage>49</fpage>
                  <lpage>82</lpage>
                  <pub-id pub-id-type="doi">10.1111/j.1467-985X.2011.00714.x</pub-id>
                </element-citation>
              </ref>
              <ref id="b8">
                <label>8</label>
                <element-citation publication-type="journal" id="sb8">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Postnikov</surname>
                      <given-names>E.B.</given-names>
                    </name>
                  </person-group>
                  <article-title>Estimation of COVID-19 dynamics ‘on a back-of-envelope’: Does the simplest SIR model provide quantitative parameters and predictions?</article-title>
                  <source>Chaos Solitons Fractals</source>
                  <volume>135</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">109841</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.109841</pub-id>
                </element-citation>
              </ref>
              <ref id="b9">
                <label>9</label>
                <element-citation publication-type="journal" id="sb9">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hong</surname>
                      <given-names>H.G.</given-names>
                    </name>
                    <name>
                      <surname>Li</surname>
                      <given-names>Y.</given-names>
                    </name>
                  </person-group>
                  <article-title>Estimation of time-varying reproduction numbers underlying epidemiological processes: A new statistical tool for the COVID-19 pandemic</article-title>
                  <source>PLoS One</source>
                  <volume>15</volume>
                  <issue>7</issue>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">e0236464</object-id>
                  <comment>[Online]. Available: </comment>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0236464</pub-id>
                </element-citation>
              </ref>
              <ref id="b10">
                <label>10</label>
                <element-citation publication-type="journal" id="sb10">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Browning</surname>
                      <given-names>L.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Digital pathology and artificial intelligence will be key to supporting clinical and academic cellular pathology through COVID-19 and future crises: the pathlake consortium perspective.</article-title>
                  <source>J. Clin. Pathol.</source>
                  <volume>74</volume>
                  <issue>7</issue>
                  <year>2021</year>
                  <fpage>443</fpage>
                  <lpage>447</lpage>
                  <pub-id pub-id-type="doi">10.1136/jclinpath-2020-206854</pub-id>
                  <pub-id pub-id-type="pmid">32620678</pub-id>
                </element-citation>
              </ref>
              <ref id="b11">
                <label>11</label>
                <element-citation publication-type="journal" id="sb11">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hou</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Hou</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Cai</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Public attention about COVID-19 on social media: An investigation based on data mining and text analysis.</article-title>
                  <source>Pers. Individ. Differ.</source>
                  <volume>175</volume>
                  <year>2021</year>
                  <object-id pub-id-type="publisher-id">110701</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.paid.2021.110701</pub-id>
                </element-citation>
              </ref>
              <ref id="b12">
                <label>12</label>
                <element-citation publication-type="journal" id="sb12">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tsao</surname>
                      <given-names>S.-F.</given-names>
                    </name>
                    <name>
                      <surname>Chen</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Tisseverasinghe</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Yang</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Li</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Butt</surname>
                      <given-names>Z.A.</given-names>
                    </name>
                  </person-group>
                  <article-title>What social media told us in the time of COVID-19: a scoping review</article-title>
                  <source>Lancet Digit. Health</source>
                  <volume>3</volume>
                  <issue>3</issue>
                  <year>2021</year>
                  <fpage>e175</fpage>
                  <lpage>e194</lpage>
                  <pub-id pub-id-type="doi">10.1016/S2589-7500(20)30315-0</pub-id>
                  <pub-id pub-id-type="pmid">33518503</pub-id>
                </element-citation>
              </ref>
              <ref id="b13">
                <label>13</label>
                <element-citation publication-type="journal" id="sb13">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Haman</surname>
                      <given-names>M.</given-names>
                    </name>
                  </person-group>
                  <article-title>The use of Twitter by state leaders and its impact on the public during the COVID-19 pandemic</article-title>
                  <source>Heliyon</source>
                  <volume>6</volume>
                  <issue>11</issue>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">e05540</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.heliyon.2020.e05540</pub-id>
                </element-citation>
              </ref>
              <ref id="b14">
                <label>14</label>
                <element-citation publication-type="journal" id="sb14">
                  <person-group person-group-type="author">
                    <name>
                      <surname>O’Leary</surname>
                      <given-names>D.E.</given-names>
                    </name>
                  </person-group>
                  <article-title>Twitter Mining for discovery, prediction and causality: applications and methodologies</article-title>
                  <source>Intell. Syst. Account. Financ. Manag.</source>
                  <volume>22</volume>
                  <issue>3</issue>
                  <year>2015</year>
                  <fpage>227</fpage>
                  <lpage>247</lpage>
                  <pub-id pub-id-type="doi">10.1002/isaf.1376</pub-id>
                </element-citation>
              </ref>
              <ref id="b15">
                <label>15</label>
                <element-citation publication-type="journal" id="sb15">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ayo</surname>
                      <given-names>F.E.</given-names>
                    </name>
                    <name>
                      <surname>Folorunso</surname>
                      <given-names>O.</given-names>
                    </name>
                    <name>
                      <surname>Ibharalu</surname>
                      <given-names>F.T.</given-names>
                    </name>
                    <name>
                      <surname>Osinuga</surname>
                      <given-names>I.A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Machine learning techniques for hate speech classification of twitter data: State-of-the-art, future challenges and research directions</article-title>
                  <source>Comput. Sci. Rev.</source>
                  <volume>38</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">100311</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.cosrev.2020.100311</pub-id>
                </element-citation>
              </ref>
              <ref id="b16">
                <label>16</label>
                <element-citation publication-type="journal" id="sb16">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Signorini</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Segre</surname>
                      <given-names>A.M.</given-names>
                    </name>
                    <name>
                      <surname>Polgreen</surname>
                      <given-names>P.M.</given-names>
                    </name>
                  </person-group>
                  <article-title>The use of Twitter to track levels of disease activity and public concern in the U.S. during the influenza a H1n1 pandemic</article-title>
                  <source>PLoS One</source>
                  <volume>6</volume>
                  <issue>5</issue>
                  <year>2011</year>
                  <object-id pub-id-type="publisher-id">e19467</object-id>
                  <comment>[Online]. Available: </comment>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0019467</pub-id>
                </element-citation>
              </ref>
              <ref id="b17">
                <label>17</label>
                <element-citation publication-type="book" id="sb17">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hirose</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Wang</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <part-title>Prediction of infectious disease spread using Twitter: A case of influenza</part-title>
                  <source>2012 Fifth International Symposium on Parallel Architectures, Algorithms and Programming</source>
                  <year>2012</year>
                  <fpage>100</fpage>
                  <lpage>105</lpage>
                  <pub-id pub-id-type="doi">10.1109/PAAP.2012.23</pub-id>
                </element-citation>
              </ref>
              <ref id="b18">
                <label>18</label>
                <element-citation publication-type="journal" id="sb18">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Santos</surname>
                      <given-names>J.C.</given-names>
                    </name>
                    <name>
                      <surname>Matos</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <article-title>Analysing Twitter and web queries for flu trend prediction</article-title>
                  <source>Theor. Biol. Med. Model.</source>
                  <volume>11 Suppl 1</volume>
                  <issue>Suppl 1</issue>
                  <year>2014</year>
                  <fpage>S6</fpage>
                  <pub-id pub-id-type="doi">10.1186/1742-4682-11-S1-S6</pub-id>
                  <pub-id pub-id-type="pmid">25077431</pub-id>
                </element-citation>
              </ref>
              <ref id="b19">
                <label>19</label>
                <element-citation publication-type="journal" id="sb19">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Sinnenberg</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Buttenheim</surname>
                      <given-names>A.M.</given-names>
                    </name>
                    <name>
                      <surname>Padrez</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Mancheno</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Ungar</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Merchant</surname>
                      <given-names>R.M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Twitter As a tool for health research: A systematic review</article-title>
                  <source>Am. J. Public Health</source>
                  <volume>107</volume>
                  <issue>1</issue>
                  <year>2017</year>
                  <fpage>e1</fpage>
                  <lpage>e8</lpage>
                  <pub-id pub-id-type="doi">10.2105/AJPH.2016.303512</pub-id>
                </element-citation>
              </ref>
              <ref id="b20">
                <label>20</label>
                <element-citation publication-type="journal" id="sb20">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Agerri</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Artola</surname>
                      <given-names>X.</given-names>
                    </name>
                    <name>
                      <surname>Beloki</surname>
                      <given-names>Z.</given-names>
                    </name>
                    <name>
                      <surname>Rigau</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Soroa</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Big data for natural language processing: A streaming approach</article-title>
                  <source>Knowl.-Based Syst.</source>
                  <volume>79</volume>
                  <year>2015</year>
                  <fpage>36</fpage>
                  <lpage>42</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.knosys.2014.11.007</pub-id>
                </element-citation>
              </ref>
              <ref id="b21">
                <label>21</label>
                <element-citation publication-type="journal" id="sb21">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Davenport</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Kalakota</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <article-title>The potential for artificial intelligence in healthcare</article-title>
                  <source>Future Healthc. J.</source>
                  <volume>6</volume>
                  <issue>2</issue>
                  <year>2019</year>
                  <fpage>94</fpage>
                  <lpage>98</lpage>
                  <pub-id pub-id-type="doi">10.7861/futurehosp.6-2-94</pub-id>
                </element-citation>
              </ref>
              <ref id="b22">
                <label>22</label>
                <element-citation publication-type="journal" id="sb22">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Beck</surname>
                      <given-names>B.R.</given-names>
                    </name>
                    <name>
                      <surname>Shin</surname>
                      <given-names>B.</given-names>
                    </name>
                    <name>
                      <surname>Choi</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Park</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Kang</surname>
                      <given-names>K.</given-names>
                    </name>
                  </person-group>
                  <article-title>Predicting commercially available antiviral drugs that may act on the novel coronavirus (SARS-CoV-2) through a drug-target interaction deep learning model.</article-title>
                  <source>Comput. Struct. Biotechnol. J.</source>
                  <volume>18</volume>
                  <year>2020</year>
                  <fpage>784</fpage>
                  <lpage>790</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.csbj.2020.03.025</pub-id>
                  <pub-id pub-id-type="pmid">32280433</pub-id>
                </element-citation>
              </ref>
              <ref id="b23">
                <label>23</label>
                <element-citation publication-type="journal" id="sb23">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Li</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Yang</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Liang</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Wu</surname>
                      <given-names>B.</given-names>
                    </name>
                  </person-group>
                  <article-title>Transfer learning for establishment of recognition of COVID-19 on CT imaging using small-sized training datasets</article-title>
                  <source>Knowl.-Based Syst.</source>
                  <volume>218</volume>
                  <year>2021</year>
                  <object-id pub-id-type="publisher-id">106849</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.knosys.2021.106849</pub-id>
                </element-citation>
              </ref>
              <ref id="b24">
                <label>24</label>
                <element-citation publication-type="journal" id="sb24">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shi</surname>
                      <given-names>F.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Review of artificial intelligence techniques in imaging data acquisition, segmentation, and diagnosis for COVID-19</article-title>
                  <source>IEEE Rev. Biomed. Eng.</source>
                  <volume>14</volume>
                  <year>2021</year>
                  <fpage>4</fpage>
                  <lpage>15</lpage>
                  <pub-id pub-id-type="doi">10.1109/RBME.2020.2987975</pub-id>
                  <pub-id pub-id-type="pmid">32305937</pub-id>
                </element-citation>
              </ref>
              <ref id="b25">
                <label>25</label>
                <element-citation publication-type="book" id="sb25">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chen</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Li</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>Z.</given-names>
                    </name>
                    <name>
                      <surname>Li</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Yu</surname>
                      <given-names>P.S.</given-names>
                    </name>
                  </person-group>
                  <part-title>A survey on applications of artificial intelligence in fighting against COVID-19</part-title>
                  <year>2020</year>
                  <fpage>1</fpage>
                  <lpage>37</lpage>
                  <comment>[Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.02202" id="interref6">http://arxiv.org/abs/2007.02202</ext-link></comment>
                </element-citation>
              </ref>
              <ref id="b26">
                <label>26</label>
                <element-citation publication-type="journal" id="sb26">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rustam</surname>
                      <given-names>F.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>COVID-19 future forecasting using supervised machine learning models</article-title>
                  <source>IEEE Access</source>
                  <volume>8</volume>
                  <year>2020</year>
                  <fpage>101489</fpage>
                  <lpage>101499</lpage>
                  <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2997311</pub-id>
                </element-citation>
              </ref>
              <ref id="b27">
                <label>27</label>
                <element-citation publication-type="journal" id="sb27">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Yeşilkanat</surname>
                      <given-names>C.M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Spatio-temporal estimation of the daily cases of COVID-19 in worldwide using random forest machine learning algorithm</article-title>
                  <source>Chaos Solitons Fractals</source>
                  <volume>140</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">110210</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110210</pub-id>
                </element-citation>
              </ref>
              <ref id="b28">
                <label>28</label>
                <element-citation publication-type="journal" id="sb28">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Arora</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Kumar</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Panigrahi</surname>
                      <given-names>B.K.</given-names>
                    </name>
                  </person-group>
                  <article-title>Prediction and analysis of COVID-19 positive cases using deep learning models: A descriptive case study of India</article-title>
                  <source>Chaos Solitons Fractals</source>
                  <volume>139</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">110017</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110017</pub-id>
                </element-citation>
              </ref>
              <ref id="b29">
                <label>29</label>
                <element-citation publication-type="journal" id="sb29">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chimmula</surname>
                      <given-names>V.K.R.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Time series forecasting of COVID-19 transmission in Canada using LSTM networks</article-title>
                  <source>Chaos Solitons Fractals</source>
                  <volume>135</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">109864</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.109864</pub-id>
                </element-citation>
              </ref>
              <ref id="b30">
                <label>30</label>
                <element-citation publication-type="journal" id="sb30">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tian</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Jiang</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Li</surname>
                      <given-names>Z.</given-names>
                    </name>
                    <name>
                      <surname>Wang</surname>
                      <given-names>X.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>H.</given-names>
                    </name>
                  </person-group>
                  <article-title>COVID-Net: A deep learning based and interpretable predication model for the county-wise trajectories of COVID-19 in the United States</article-title>
                  <source>MedRxiv</source>
                  <year>2020</year>
                  <pub-id pub-id-type="doi">10.1101/2020.05.26.20113787</pub-id>
                  <comment>2020.05.26.20113787</comment>
                </element-citation>
              </ref>
              <ref id="b31">
                <label>31</label>
                <element-citation publication-type="journal" id="sb31">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chen</surname>
                      <given-names>E.</given-names>
                    </name>
                    <name>
                      <surname>Lerman</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Ferrara</surname>
                      <given-names>E.</given-names>
                    </name>
                  </person-group>
                  <article-title>Tracking social media discourse about the COVID-19 pandemic: Development of a public coronavirus Twitter data set</article-title>
                  <source>JMIR Public Health Surveill.</source>
                  <volume>6</volume>
                  <issue>2</issue>
                  <year>2020</year>
                  <pub-id pub-id-type="doi">10.2196/19273</pub-id>
                </element-citation>
              </ref>
              <ref id="b32">
                <label>32</label>
                <element-citation publication-type="journal" id="sb32">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zengin Alp</surname>
                      <given-names>Z.</given-names>
                    </name>
                    <name>
                      <surname>Gündüz Öğüdücü</surname>
                      <given-names>Ş.</given-names>
                    </name>
                  </person-group>
                  <article-title>Identifying topical influencers on twitter based on user behavior and network topology</article-title>
                  <source>Knowl.-Based Syst.</source>
                  <volume>141</volume>
                  <year>2018</year>
                  <fpage>211</fpage>
                  <lpage>221</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.knosys.2017.11.021</pub-id>
                </element-citation>
              </ref>
              <ref id="b33">
                <label>33</label>
                <element-citation publication-type="journal" id="sb33">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lwin</surname>
                      <given-names>M.O.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Global sentiments surrounding the COVID-19 pandemic on Twitter: Analysis of Twitter trends</article-title>
                  <source>JMIR Public Health Surveill.</source>
                  <volume>6</volume>
                  <issue>2</issue>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">e19447</object-id>
                  <pub-id pub-id-type="doi">10.2196/19447</pub-id>
                </element-citation>
              </ref>
              <ref id="b34">
                <label>34</label>
                <element-citation publication-type="journal" id="sb34">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Park</surname>
                      <given-names>H.W.</given-names>
                    </name>
                    <name>
                      <surname>Park</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Chong</surname>
                      <given-names>M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Conversations and medical news frames on Twitter: Infodemiological study on COVID-19 in South Korea</article-title>
                  <source>J. Med. Internet Res.</source>
                  <volume>22</volume>
                  <issue>5</issue>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">e18897</object-id>
                  <pub-id pub-id-type="doi">10.2196/18897</pub-id>
                </element-citation>
              </ref>
              <ref id="b35">
                <label>35</label>
                <element-citation publication-type="book" id="sb35">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Venigalla</surname>
                      <given-names>A.S.M.</given-names>
                    </name>
                    <name>
                      <surname>Chimalakonda</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Vagavolu</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <part-title>Mood of India during Covid-19 - an interactive web portal based on emotion analysis of Twitter data</part-title>
                  <source>Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing</source>
                  <year>2020</year>
                  <fpage>65</fpage>
                  <lpage>68</lpage>
                  <pub-id pub-id-type="doi">10.1145/3406865.3418567</pub-id>
                </element-citation>
              </ref>
              <ref id="b36">
                <label>36</label>
                <element-citation publication-type="journal" id="sb36">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zheng</surname>
                      <given-names>N.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Predicting COVID-19 in China using hybrid AI model</article-title>
                  <source>IEEE Trans. Cybern.</source>
                  <volume>50</volume>
                  <issue>7</issue>
                  <year>2020</year>
                  <fpage>2891</fpage>
                  <lpage>2904</lpage>
                  <pub-id pub-id-type="doi">10.1109/TCYB.2020.2990162</pub-id>
                  <pub-id pub-id-type="pmid">32396126</pub-id>
                </element-citation>
              </ref>
              <ref id="b37">
                <label>37</label>
                <element-citation publication-type="journal" id="sb37">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hazarika</surname>
                      <given-names>B.B.</given-names>
                    </name>
                    <name>
                      <surname>Gupta</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Modelling and forecasting of COVID-19 spread using wavelet-coupled random vector functional link networks</article-title>
                  <source>Appl. Soft Comput.</source>
                  <volume>96</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">106626</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.asoc.2020.106626</pub-id>
                </element-citation>
              </ref>
              <ref id="b38">
                <label>38</label>
                <element-citation publication-type="journal" id="sb38">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pedregosa</surname>
                      <given-names>F.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Scikit-learn: Machine learning in python</article-title>
                  <source>J. Mach. Learn. Res.</source>
                  <volume>12</volume>
                  <issue>Null</issue>
                  <year>2011</year>
                  <fpage>2825</fpage>
                  <lpage>2830</lpage>
                </element-citation>
              </ref>
              <ref id="b39">
                <label>39</label>
                <element-citation publication-type="journal" id="sb39">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mikolov</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Sutskever</surname>
                      <given-names>I.</given-names>
                    </name>
                    <name>
                      <surname>Chen</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Corrado</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Dean</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Distributed representations ofwords and phrases and their compositionality</article-title>
                  <source>Adv. Neural Inf. Process. Syst.</source>
                  <year>2013</year>
                  <fpage>1</fpage>
                  <lpage>9</lpage>
                </element-citation>
              </ref>
              <ref id="b40">
                <label>40</label>
                <element-citation publication-type="book" id="sb40">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mikolov</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Chen</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Corrado</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Dean</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <part-title>Efficient estimation of word representations in vector space</part-title>
                  <source>1st Int. Conf. Learn. Represent. ICLR 2013 - Work. Track Proc.</source>
                  <year>2013</year>
                  <fpage>1</fpage>
                  <lpage>12</lpage>
                </element-citation>
              </ref>
              <ref id="b41">
                <label>41</label>
                <element-citation publication-type="book" id="sb41">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zhang</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Pan</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Wu</surname>
                      <given-names>X.</given-names>
                    </name>
                    <name>
                      <surname>Skibniewski</surname>
                      <given-names>M.J.</given-names>
                    </name>
                  </person-group>
                  <part-title>Artificial intelligence in construction engineering and management</part-title>
                  <year>2021</year>
                  <publisher-name>Springer</publisher-name>
                  <isbn>978-981-16-2842-9</isbn>
                  <fpage>1</fpage>
                  <lpage>256</lpage>
                </element-citation>
              </ref>
              <ref id="b42">
                <label>42</label>
                <element-citation publication-type="book" id="sb42">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kumar</surname>
                      <given-names>N.</given-names>
                    </name>
                    <name>
                      <surname>Susan</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <part-title>COVID-19 pandemic prediction using time series forecasting models</part-title>
                  <source>2020 11th Int. Conf. Comput. Commun. Netw. Technol. ICCCNT 2020</source>
                  <year>2020</year>
                  <pub-id pub-id-type="doi">10.1109/ICCCNT49239.2020.9225319</pub-id>
                </element-citation>
              </ref>
              <ref id="b43">
                <label>43</label>
                <element-citation publication-type="journal" id="sb43">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Papastefanopoulos</surname>
                      <given-names>V.</given-names>
                    </name>
                    <name>
                      <surname>Linardatos</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Kotsiantis</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <article-title>COVID-19: A comparison of time series methods to forecast percentage of active cases per population</article-title>
                  <source>Appl. Sci.</source>
                  <volume>10</volume>
                  <issue>11</issue>
                  <year>2020</year>
                  <fpage>1</fpage>
                  <lpage>15</lpage>
                  <pub-id pub-id-type="doi">10.3390/app10113880</pub-id>
                </element-citation>
              </ref>
              <ref id="b44">
                <label>44</label>
                <element-citation publication-type="journal" id="sb44">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Núñez</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Barreiro</surname>
                      <given-names>N.L.</given-names>
                    </name>
                    <name>
                      <surname>Barrio</surname>
                      <given-names>R.A.</given-names>
                    </name>
                    <name>
                      <surname>Rackauckas</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Forecasting virus outbreaks with social media data via neural ordinary differential equations</article-title>
                  <source>MedRxiv</source>
                  <year>2021</year>
                  <comment>[Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://medrxiv.org/content/early/2021/01/31/2021.01.27.21250642.abstract" id="interref7">http://medrxiv.org/content/early/2021/01/31/2021.01.27.21250642.abstract</ext-link>, 2021.01.27.21250642</comment>
                </element-citation>
              </ref>
              <ref id="b45">
                <label>45</label>
                <element-citation publication-type="journal" id="sb45">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Yousefinaghani</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Dara</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Mubareka</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Sharif</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <article-title>Prediction of COVID-19 waves using social media and google search: A case study of the US and Canada</article-title>
                  <source>Front. Public Health</source>
                  <volume>9</volume>
                  <issue>April</issue>
                  <year>2021</year>
                  <fpage>1</fpage>
                  <lpage>11</lpage>
                  <pub-id pub-id-type="doi">10.3389/fpubh.2021.656635</pub-id>
                </element-citation>
              </ref>
              <ref id="b46">
                <label>46</label>
                <element-citation publication-type="journal" id="sb46">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Petropoulos</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Makridakis</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Stylianou</surname>
                      <given-names>N.</given-names>
                    </name>
                  </person-group>
                  <article-title>COVID-19: Forecasting confirmed cases and deaths with a simple time-series model</article-title>
                  <source>Int. J. Forecast.</source>
                  <year>2020</year>
                  <pub-id pub-id-type="doi">10.1016/j.ijforecast.2020.11.010</pub-id>
                  <comment>doi: 10.1016/j.ijforecast.2020.11.010</comment>
                </element-citation>
              </ref>
              <ref id="b47">
                <label>47</label>
                <element-citation publication-type="journal" id="sb47">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pan</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>Li</given-names>
                    </name>
                  </person-group>
                  <article-title>A BIM-data mining integrated digital twin framework for advanced project management</article-title>
                  <source>Autom. Constr.</source>
                  <volume>124</volume>
                  <year>2021</year>
                  <object-id pub-id-type="publisher-id">103564</object-id>
                  <pub-id pub-id-type="doi">10.1016/j.autcon.2021.103564</pub-id>
                </element-citation>
              </ref>
            </ref-list>
            <ack id="d1e5110">
              <title>Acknowledgements</title>
              <p id="d1e5113">This study was supported in part by <funding-source id="GS1">Microsoft Corporation for the AI for Health COVID-19 Azure Compute</funding-source> Grant of ID:00011000272 and the <funding-source id="GS2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001475</institution-id><institution>Start-Up Grant at Nanyang Technological University, Singapore</institution></institution-wrap></funding-source>
(No. 04INS000423C120).</p>
            </ack>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
