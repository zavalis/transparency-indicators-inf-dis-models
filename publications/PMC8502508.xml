<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2022-03-11T00:48:27Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:8502508" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:8502508</identifier>
        <datestamp>2021-10-12</datestamp>
        <setSpec>phenaturepg</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article" dtd-version="1.3">
          <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
            <restricted-by>pmc</restricted-by>
          </processing-meta>
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Neural Comput Appl</journal-id>
              <journal-id journal-id-type="iso-abbrev">Neural Comput Appl</journal-id>
              <journal-title-group>
                <journal-title>Neural Computing &amp; Applications</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0941-0643</issn>
              <issn pub-type="epub">1433-3058</issn>
              <publisher>
                <publisher-name>Springer London</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC8502508</article-id>
              <article-id pub-id-type="pmcid">PMC8502508</article-id>
              <article-id pub-id-type="pmc-uid">8502508</article-id>
              <article-id pub-id-type="pmid">34658536</article-id>
              <article-id pub-id-type="publisher-id">6548</article-id>
              <article-id pub-id-type="doi">10.1007/s00521-021-06548-9</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Original Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>A novel approach based on combining deep learning models with statistical methods for COVID-19 time series forecasting</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" corresp="yes">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8615-5553</contrib-id>
                  <name>
                    <surname>Abbasimehr</surname>
                    <given-names>Hossein</given-names>
                  </name>
                  <address>
                    <email>abbasimehr@azaruniv.ac.ir</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Paki</surname>
                    <given-names>Reza</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Bahrini</surname>
                    <given-names>Aram</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411468.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 0417 5692</institution-id><institution>Faculty of Information Technology and Computer Engineering, </institution><institution>Azarbaijan Shahid Madani University, </institution></institution-wrap>Tabriz, Iran </aff>
                <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.27755.32</institution-id><institution-id institution-id-type="ISNI">0000 0000 9136 933X</institution-id><institution>Department of Engineering Systems and Environment, </institution><institution>University of Virginia, </institution></institution-wrap>Charlottesville, Virginia USA </aff>
                <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.4643.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0327</institution-id><institution>Present Address: School of Industrial and Information Engineering, </institution><institution>Politecnico di Milano University, </institution></institution-wrap>Milano, Italy </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>10</day>
                <month>10</month>
                <year>2021</year>
              </pub-date>
              <fpage>1</fpage>
              <lpage>15</lpage>
              <history>
                <date date-type="received">
                  <day>1</day>
                  <month>12</month>
                  <year>2020</year>
                </date>
                <date date-type="accepted">
                  <day>14</day>
                  <month>9</month>
                  <year>2021</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2021</copyright-statement>
                <license>
                  <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <p id="Par1">The COVID-19 pandemic has disrupted the economy and businesses and impacted all facets of people’s lives. It is critical to forecast the number of infected cases to make accurate decisions on the necessary measures to control the outbreak. While deep learning models have proved to be effective in this context, time series augmentation can improve their performance. In this paper, we use time series augmentation techniques to create new time series that take into account the characteristics of the original series, which we then use to generate enough samples to fit deep learning models properly. The proposed method is applied in the context of COVID-19 time series forecasting using three deep learning techniques, (1) the long short-term memory, (2) gated recurrent units, and (3) convolutional neural network. In terms of symmetric mean absolute percentage error and root mean square error measures, the proposed method significantly improves the performance of long short-term memory and convolutional neural networks. Also, the improvement is average for the gated recurrent units. Finally, we present a summary of the top augmentation model as well as a visual representation of the actual and forecasted data for each country.</p>
              </abstract>
              <kwd-group xml:lang="en">
                <title>Keywords</title>
                <kwd>Deep learning</kwd>
                <kwd>Time series forecasting</kwd>
                <kwd>Augmentation methods</kwd>
                <kwd>COVID-19 pandemic</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1">
              <title>Introduction</title>
              <p id="Par2">Temporary interventions such as social distancing, self-isolating, quarantining, and shutting down nonessential activities have been strategies for the governments to prevent the virus from spreading. It is essential to forecast the number of infected cases using different data types to notify public health decision-makers by estimating the likely impact of the COVID-19 pandemic and plan accordingly [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref>].</p>
              <p id="Par3">Deep learning models have demonstrated successful performance in language and image processing tasks [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR8">8</xref>]. Also, they exhibited state-of-the-art performance in forecasting complex time series data [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR12">12</xref>]. The main advantage of deep learning models is their ability to learn representations from raw input data. Among the most popular deep learning algorithms, long short-term memory (LSTM) and bidirectional LSTM (Bi-LSTM) [<xref ref-type="bibr" rid="CR13">13</xref>] have been used in [<xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR17">17</xref>], with significant results in COVID-19 forecasting. LSTM is a special type of recurrent neural networks (RNNs), which is developed to learn temporal information from sequential data [<xref ref-type="bibr" rid="CR18">18</xref>].</p>
              <p id="Par4">Despite the fact that deep learning algorithms can reach acceptable performance in time series forecasting, particularly in COVID-19 forecasting applications, their forecasting capability is primarily dependent on the amount of data available to fit their parameters appropriately [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. Another challenge with deep learning for time series forecasting is that, even though adequate data samples are available, data from the distant past are typically less useful for forecasting [<xref ref-type="bibr" rid="CR12">12</xref>]. In other words, in predicting, recent observations of an individual series are more valuable. This may be due to shifts in patterns that formerly occurred in a series.</p>
              <p id="Par5">To overcome the aforementioned issue and increase the performance of deep learning models in time series forecasting, we propose exploiting time series augmentation techniques [<xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref>] to generate new series with similar temporal dependencies as the original series. We then extract new samples from the augmented time series to enhance model training. Three deep learning models based on the LSTM, gated recurrent units (GRU) [<xref ref-type="bibr" rid="CR22">22</xref>], and convolutional neural network (CNN) [<xref ref-type="bibr" rid="CR23">23</xref>] are used to see whether the proposed approach is useful. A multi-step-ahead forecasting strategy [<xref ref-type="bibr" rid="CR24">24</xref>] is used to develop the models, allowing them to predict the number of cases for the next few days. It is a preferable alternative to single-step-ahead forecasting for long-horizon forecasting [<xref ref-type="bibr" rid="CR25">25</xref>].</p>
              <p id="Par6">The proposed models are applied to COVID-19 data from the top 10 countries with the most reported confirmed cases from January 20, 2020, until March 28, 2021. We show that the proposed method significantly improves the performance of the LSTM-based and CNN-based models but has an average improvement on the GRU performance. To evaluate the effectiveness of the proposed model, we visualize the forecasting results and provide statistical characteristics of the data to enable governments to make long-term decisions on how to deal with the pandemic.</p>
              <p id="Par7">The remainder of this paper is organized as follows. Section <xref rid="Sec2" ref-type="sec">2</xref> provides a brief review on COVID-19 time series forecasting and the description of the employed deep learning methods. In Sect. <xref rid="Sec8" ref-type="sec">3</xref>, we present the proposed approach and the architectures of the designed models. Section <xref rid="Sec11" ref-type="sec">4</xref> assesses the usefulness of the proposed method via the experimental study. Discussions are provided in Sect. <xref rid="Sec23" ref-type="sec">5</xref>, and finally, the paper concludes in Sect. <xref rid="Sec28" ref-type="sec">6</xref> with some suggestions for future work in this area.</p>
            </sec>
            <sec id="Sec2">
              <title>Related work</title>
              <p id="Par8">This section first presents a review of the COVID-19 time series forecasting methods and then describes the utilized models throughout the study.</p>
              <sec id="Sec3">
                <title>COVID-19 Forecasting</title>
                <p id="Par9">Various approaches, mostly mathematical, statistical, machine learning, and deep learning models have been utilized in previous studies [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR26">26</xref>]. Rahimi et al. [<xref ref-type="bibr" rid="CR27">27</xref>] provided a review of widely used forecasting models on COVID-19 data. Here, we concentrate mainly on COVID-19 time series forecasting studies and present a brief review in this context.</p>
                <p id="Par10">Al-Qaness et al. [<xref ref-type="bibr" rid="CR28">28</xref>] presented an improved adaptive neuro-fuzzy inference method (ANFIS) that uses an enhanced flower pollination algorithm (FPA) by the salp swarm algorithm (SSA) to forecast the COVID-19 cases in China. Their model is more potent in terms of mean absolute percentage error (MAPE), root mean squared relative error (RMSRE), coefficient of determination, and computing time. Torrealba-Rodriguez et al. [<xref ref-type="bibr" rid="CR3">3</xref>] used Gompertz, logistic, and artificial neural network (ANN) models. Their results from the infected cases in Mexico showed a high coefficient of determination between the studied data and those obtained by the proposed models. Similar studies which considered Gompertz and logistic models can be found in [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR32">32</xref>]. Castillo and Melin [<xref ref-type="bibr" rid="CR26">26</xref>] studied an approach based on fuzzy fractal for data from 10 countries by combining (1) fractal dimension to evaluate the complexity of the dynamics in the time series and (2) fuzzy logic to reflect the uncertainty forecasting. Melin et al. [<xref ref-type="bibr" rid="CR33">33</xref>] introduced a multiple ensemble neural network model with fuzzy logic response aggregation. Their experiments on the data of Mexico infected cases show the superiority of their proposed model over the single ANN.</p>
                <p id="Par11">Kırbaş et al. [<xref ref-type="bibr" rid="CR15">15</xref>] used autoregressive integrated moving average (ARIMA), nonlinear autoregression neural network (NARNN), and LSTM approaches to study the data of 8 European countries. Shahid et al. [<xref ref-type="bibr" rid="CR16">16</xref>] proposed forecast models with ARIMA, support vector regression (SVR), LSTM, Bi-LSTM in 10 significantly affected countries. Leila et al. [<xref ref-type="bibr" rid="CR34">34</xref>] applied ANN and ARIMA models, and Petropoulos and Makridakis [<xref ref-type="bibr" rid="CR35">35</xref>] implemented exponential smoothing forecasting to predict the infected cases.</p>
                <p id="Par12">Arura et al. [<xref ref-type="bibr" rid="CR14">14</xref>] utilized recurrent neural network (RNN)-based variants such as deep LSTM, convolutional LSTM, and Bi-LSTM for the cases in India. For Russia, Peru, and Iran, Wang et al. [16] used LSTM networks and rolling updating mechanisms to feed new forecasting outcomes into model training for the next iteration. The study of Hasan [<xref ref-type="bibr" rid="CR36">36</xref>] suggested a hybrid model consisting of ensemble empirical mode decomposition (EEMD) and artificial neural network (ANN), which outperformed conventional statistical analysis.</p>
                <p id="Par13">Machine learning algorithms were used by Li et al. [<xref ref-type="bibr" rid="CR37">37</xref>] in predicting mortality in confirmed cases of COVID-19. Their results indicated that the gradient boosting decision tree (GBDT) outperforms logistic regression (LR) models, the performance comparison appeared to be independent of disease severity, and the 5-index LR or LR-5 model is powerful in death prediction with a high area under the curve (AUC).</p>
                <p id="Par14">Reviewing the previous studies indicate that computational intelligence methods and especially deep learning methods have attracted growing attention in COVID-19 time series forecasting. Even though deep neural networks have performed reasonably well when applied on COVID-19 time series data, in this study, we aim to enhance their predictive power by feeding them with more data. In general, the performance of the generated model in a deep learning task is largely determined by the amount of samples used in the model training phase. The inherent problem in time series forecasting is that time series are often short, and accordingly, the number of extracted samples becomes small. To address this problem, we propose to generate a new time series with similar characteristics to the original time series using statistical data augmentation methods. The obtained series via the augmentation approach is used to create new samples. In this way, a sufficient number of instances are provided for model learning.</p>
              </sec>
              <sec id="Sec4">
                <title>Description of the employed models</title>
                <p id="Par15">We use RNN for the sequence processing task, which can catch the temporal dependencies in a time sequence, unlike ANN. However, the key issue with RNN is the gradient vanishing/exploding problem, which makes them difficult to train. Two new architectures with gating mechanisms, the LSTM [<xref ref-type="bibr" rid="CR38">38</xref>] and GRU [<xref ref-type="bibr" rid="CR39">39</xref>], have been proposed to solve this problem. In addition, we will use CNN, which is briefly discussed here, as another deep learning unit in our experiments.</p>
                <sec id="Sec5">
                  <title>LSTM</title>
                  <p id="Par16">In this section, we explain the structure and mechanism of the LSTM unit. As illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, each LSTM unit is comprised of a memory cell <italic>C</italic>, an input gate <italic>i</italic>, an output gate <italic>o</italic> and a forget gate <italic>f</italic>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Structure of an LSTM unit</p></caption><graphic xlink:href="521_2021_6548_Fig1_HTML" id="MO1"/></fig></p>
                  <p id="Par17">Considering the following parameters, the learning procedure of LSTM is described below:<list list-type="bullet"><list-item><p id="Par18"><inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_t:$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>:</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq1.gif"/></alternatives></inline-formula></p><p> the input vector at time step <italic>t</italic></p></list-item><list-item><p id="Par19"><inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b=\{b_i,b_o,b_f,b_c\}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq2.gif"/></alternatives></inline-formula></p><p>are bias vectors of input, output, forget, and memory cell.</p></list-item><list-item><p id="Par20"><inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W=\{W_i,W_o,W_f,W_c\}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq3.gif"/></alternatives></inline-formula></p><p> are weight matrix of input, output, forget, and memory cell.</p></list-item><list-item><p id="Par21"><inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$U=\{U_i,U_o,U_f,U_c\}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq4.gif"/></alternatives></inline-formula></p><p>are the recurrent weights of input, output, forget, and memory cell.</p></list-item></list>The output <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_t$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq5.gif"/></alternatives></inline-formula> of the LSTM unit is computed as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} h_t=o_t \tan h(c_t), \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>tan</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_t$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq6.gif"/></alternatives></inline-formula> is the output gate that regulates the outgoing information of the LSTM unit and <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c_t$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq7.gif"/></alternatives></inline-formula> is the memory. <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_t$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq8.gif"/></alternatives></inline-formula> is computed by<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} o_t=\sigma (W_o x_t+U_oh_{t-1}+b_o), \end{aligned}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M22"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq9.gif"/></alternatives></inline-formula> is the logistic sigmoid and <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t-1}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq10.gif"/></alternatives></inline-formula> is the output vector (hidden state) of the time <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t-1$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq11.gif"/></alternatives></inline-formula>. The memory cell is updated as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {c_t}= f_t c_{t-1}+i_t \tilde{c_t}, \end{aligned}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tilde{c_t}$$\end{document}</tex-math><mml:math id="M30"><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq12.gif"/></alternatives></inline-formula>, the newly computed memory is obtained as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \tilde{c_t}=\tan h(W_c x_t+U_ch_{t-1}+b_c). \end{aligned}$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>tan</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>In fact, the memory cell <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c_t$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq13.gif"/></alternatives></inline-formula> is a combination of the previous memory multiplied by the forget gate, <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_t$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq14.gif"/></alternatives></inline-formula> and the new memory <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{c}}$$\end{document}</tex-math><mml:math id="M38"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq15.gif"/></alternatives></inline-formula> regulated by the input gate, <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i_t$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq16.gif"/></alternatives></inline-formula>. <inline-formula id="IEq17"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_t$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq17.gif"/></alternatives></inline-formula> and are computed as follows:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {f_t}=\sigma (W_f x_t+U_f h_{t-1}+b_f), \end{aligned}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {i_t}=\sigma (W_i x_t+U_i h_{t-1}+b_i). \end{aligned}$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
                </sec>
                <sec id="Sec6">
                  <title>GRU</title>
                  <p id="Par22">GRU is another variant of RNN that uses gating mechanism to regulate the flow of information inside the unit. Unlike LSTM, GRU does not contain a memory cell. As Figure <xref rid="Fig2" ref-type="fig">2</xref> portrays, the GRU has two gates, a reset gate <inline-formula id="IEq18"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_t$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq18.gif"/></alternatives></inline-formula> and an update gate <inline-formula id="IEq19"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_t$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq19.gif"/></alternatives></inline-formula>. The rest gate decides how to combine the new input <inline-formula id="IEq20"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_t$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq20.gif"/></alternatives></inline-formula> with the previous hidden state, <inline-formula id="IEq21"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t-1}$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq21.gif"/></alternatives></inline-formula>. Also, the update gate determines how much unit updates its hidden state.<fig id="Fig2"><label>Fig. 2</label><caption><p>Structure of a GRU unit [<xref ref-type="bibr" rid="CR39">39</xref>]</p></caption><graphic xlink:href="521_2021_6548_Fig2_HTML" id="MO8"/></fig></p>
                </sec>
                <sec id="Sec7">
                  <title>CNN</title>
                  <p id="Par23">CNN has shown promise in a variety of fields, including machine vision [<xref ref-type="bibr" rid="CR23">23</xref>]. CNN’s convolutional layers take input data and extract new features by performing convolution operations on it with convolution kernels. Each CNN contains a convolution kernel (i.e., a small window) that slides over the input data and performs convolutional operations to generate new features, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> [<xref ref-type="bibr" rid="CR40">40</xref>]. The generated features obtained by the convolution technique are typically more discriminative than the raw input data, resulting in better forecasting.<fig id="Fig3"><label>Fig. 3</label><caption><p>Structure of CNN for time series</p></caption><graphic xlink:href="521_2021_6548_Fig3_HTML" id="MO9"/></fig></p>
                </sec>
              </sec>
            </sec>
            <sec id="Sec8">
              <title>Proposed method</title>
              <p id="Par24">Deep learning methods such as LSTM, CNN, and GRU have been applied successfully in the time series forecasting context. These techniques’ performance mainly depends on having enough data to fit their parameters suitably [<xref ref-type="bibr" rid="CR12">12</xref>]. The number of samples extracted from a short time series may be insufficient to achieve an optimal model [<xref ref-type="bibr" rid="CR19">19</xref>]. These methods should be appropriately regularized to prevent them from overfitting. Another difficulty with time series forecasting is that, even if the series is long and adequate data are available, the observations from the far past usually provide fewer determinants for predicting. In other words, recent observations of an individual series are more useful in forecasting. This may be because of the changes that happen in patterns that existed in a series.</p>
              <p id="Par25">The commonly used procedure of data preparation for a time series forecasting task is illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. As shown, a given time series is divided into in-samples and out-samples considering a certain ratio, for example, 80/20. The out-sample part (test data) <inline-formula id="IEq22"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{t_{m+1},t_{m+2}, ..., t_n\}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq22.gif"/></alternatives></inline-formula> is used to evaluate the obtained model. Also, the in-sample part is divided into the train data <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{t_{1},t_{2}, ..., t_k\}$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq23.gif"/></alternatives></inline-formula> and the validation data <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{t_{k+1},t_{k+2}, ..., t_m\}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq24.gif"/></alternatives></inline-formula>. The validation data are utilized to tune the model’s hyperparameters and to evaluate a model fit on the train data. Selecting separate validation data leads to excluding the recent observations from the train data, so the recent patterns that exist in the data will not be captured. One simple solution to tackle this problem is to include the validation data in model training. However, in this way, overfitting may occur, which usually leads to loss of accuracy on test data. In this study, we propose to use time series augmentation methods to avoid model overfitting and improve the accuracy.<fig id="Fig4"><label>Fig. 4</label><caption><p>An example of time series</p></caption><graphic xlink:href="521_2021_6548_Fig4_HTML" id="MO10"/></fig></p>
              <p id="Par26">Specifically, we utilize a time series augmentation technique to create new series with the same temporal dependencies that exist in the original series. The augmented time series is used to create a new validation set. The overall procedure of the proposed idea is illustrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The proposed model contains preprocessing and model training phases. Firstly, in the preprocessing phase, a time series augmentation technique is applied, and then, the sample generation procedure is accomplished. In the modeling phase, the deep learning models are employed on the generated samples, and the best model is achieved. In the model training process, we adopt the Bayesian optimization algorithm to fine-tune the hyperparameters of each model.<fig id="Fig5"><label>Fig. 5</label><caption><p>Proposed schema</p></caption><graphic xlink:href="521_2021_6548_Fig5_HTML" id="MO11"/></fig></p>
              <p id="Par27">To explain our proposal, we describe its procedure using Algorithm 1. To augment a time series, we apply the method proposed in [<xref ref-type="bibr" rid="CR20">20</xref>]. This algorithm firstly applies the Box–Cox transformation to the series and then decomposes the series into trend, seasonal, and reminder adopting STL or Loess [<xref ref-type="bibr" rid="CR41">41</xref>]. Then it bootstraps the reminder using the moving block bootstrap (MBB) [<xref ref-type="bibr" rid="CR42">42</xref>], and the trend and seasonal components are added together, and finally, inverse Box–Cox transformation is applied. As illustrated in Algorithm 1, lines 1-7 show the procedure of computing bootstrapped series. In lines 8-10, the bootstrapped series are aggregated, and then, for the original series and the augmented series, the instances with input–output format are created considering a Lag, and an output window (Output_Window). Line 11 concatenates the two validation sets. In lines 12-18, the benchmarking models are trained and evaluated, and the best model in terms of RMSE is returned.</p>
              <graphic position="anchor" xlink:href="521_2021_6548_Figa_HTML" id="MO12"/>
              <sec id="Sec9">
                <title>Architecture of the utilized deep learning models</title>
                <p id="Par28">Three state-of-the-art deep learning models are employed to explore whether the forecasting performance of the proposed scheme is better than the performance of the regular approach. The list of benchmarking models along with their architectures is provided in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Architecture of three deep learning models used to evaluate the proposal</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Benchmarking models</th><th align="left">Architecture</th></tr></thead><tbody><tr><td align="left" rowspan="3">LSTM</td><td align="left">LSTM layer</td></tr><tr><td align="left">Dense</td></tr><tr><td align="left">Output</td></tr><tr><td align="left" rowspan="3">GRU</td><td align="left">GRU layer</td></tr><tr><td align="left">Dense</td></tr><tr><td align="left">Output</td></tr><tr><td align="left" rowspan="3">CNN</td><td align="left">1D convolution layer</td></tr><tr><td align="left">Dense</td></tr><tr><td align="left">Output</td></tr></tbody></table></table-wrap></p>
                <p id="Par29">Also, Fig. <xref rid="Fig6" ref-type="fig">6</xref> illustrates the full architectures of the proposed methods. As can be observed from the figure, the dense and output layers are the same for LSTM, CNN, and GRU. Every model learns a representation (a feature vector) of the input data and feeds it into the fully connected (dense) layer; afterward, the predictions are computed using the output layer.<fig id="Fig6"><label>Fig. 6</label><caption><p>Architecture of the utilized models</p></caption><graphic xlink:href="521_2021_6548_Fig6_HTML" id="MO13"/></fig></p>
              </sec>
              <sec id="Sec10">
                <title>Hyperparameter selection procedure using bayesian optimization</title>
                <p id="Par30">The choice of optimal hyperparameters is essential in obtaining a forecasting model with high accuracy [<xref ref-type="bibr" rid="CR43">43</xref>]. Deep learning-based models usually contain several hyperparameters. Although grid search is a popular strategy for finding the optimal hyperparameters, it requires more computational time and resources to fine-tune deep learning methods. This is due to the fact that the grid search method exhaustively considers all parameter combinations, so it needs more computational resources, especially in the case of deep learning. The main reason behind using the Bayesian hyperparameter optimization is that it does not consider all hyperparameter combinations, and so less training time and resources are needed. The Bayesian hyperparameter optimization uses Bayesian models based on Gaussian processes to predict good tuning parameters [<xref ref-type="bibr" rid="CR43">43</xref>]. The study of Wu et al. [<xref ref-type="bibr" rid="CR43">43</xref>] indicated that the Bayesian optimization-based method could find the optimal hyperparameters for the popular machine learning algorithms. In line with [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR43">43</xref>], the Bayesian optimization technique [<xref ref-type="bibr" rid="CR44">44</xref>, <xref ref-type="bibr" rid="CR45">45</xref>] is used to tune the hyperparameters in all of the experiments in this study. The Bayesian optimization algorithm uses the error on the validation data to determine the appropriateness of each model.</p>
              </sec>
            </sec>
            <sec id="Sec11">
              <title>Experimental study</title>
              <p id="Par31">In this study, we use R forecast package<xref ref-type="fn" rid="Fn1">1</xref> version 8.14 to generate the augmentation of each time series. Also, the deep learning models are implemented with Keras [<xref ref-type="bibr" rid="CR46">46</xref>], the Python deep learning library.</p>
              <sec id="Sec12">
                <title>Dataset</title>
                <p id="Par33">The Humanitarian Data Exchange (HDX) [<xref ref-type="bibr" rid="CR47">47</xref>] is the source of the data utilized in this study. The Corona Virus Resource Center at Johns Hopkins University has compiled and released a credible source of COVID-19 reported cases on HDX so that scientists can model the disease’s spread and conduct data analysis [<xref ref-type="bibr" rid="CR48">48</xref>]. The dataset contains the daily record of confirmed cases in the time series format, including temporal patterns. In this study, the experiments were conducted using the time series data for ten countries with the highest number of confirmed cases from January 20, 2020, to March 28, 2021. These countries are the USA, Brazil, India, France, Russia, the UK, Italy, Spain, Turkey, and Germany. The last 28 days of each series are used as the test set, and the remaining days are used as the training data. We also made the validation set the same size as the test set (28 days).</p>
              </sec>
              <sec id="Sec13">
                <title>Statistical properties of the data</title>
                <p id="Par34">In Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>, we apply statistical properties to the aforementioned ten countries with the highest COVID-19 cases to better interpret the dataset.</p>
                <p id="Par35">The sample size refers to the number of observations included in the experiment for each country which is not necessarily equal because it is counted from the day the first COVID-19 cases were reported.</p>
                <p id="Par36">The mean or average of the data is the most popular and well-known measure of central tendency and is equal to the sum of all the values in the dataset divided by the number of observations. It is worth noting that the total cases can be obtained by multiplying the sample size by the sample mean during the study period. Other than mean, two other measures of central tendency are median and mode. Median is the middle value for the dataset that has been arranged in order of magnitude. An essential property about the median is that it is less affected or “Robust” by outliers and skewed data. Mode, on the other hand, is the most frequent number of daily cases in our dataset. It does not give a fair measure of central tendency when compared to median and mean [<xref ref-type="bibr" rid="CR50">50</xref>]. The obtained mode for most of the countries is zero. The reason for that could be having days without any new cases or failing to report instances due to holidays or weekends.</p>
                <p id="Par37">The sample’s square root of variance often known as standard deviation is a measure of the amount of variation or dispersion of the dataset, using the same unit as the mean. A small standard deviation implies that the values tend to be close to the mean of the dataset, whereas a high standard deviation suggests that the values are spread out over a wider range [<xref ref-type="bibr" rid="CR51">51</xref>]. Besides standard deviation, two other dispersion measures are (1) skewness, where it measures the amount of asymmetricity, and (2) kurtosis, where it determines the heaviness of the distribution tails, also known as is the “tailedness” or the “peakedness.” For a country dataset with one mode (uni-modal), a positive skewness shows that the data are asymmetric and skewed to the right, a negative skewness explains that the data are asymmetric and skewed to the left, and finally, a symmetric dataset always has a zero skewness. To provide a comparison to the standard normal distribution, it is common to use an adjusted version known as the excess kurtosis, which is the kurtosis minus 3. A dataset with zero excess kurtosis is called “Mesokurtic,” with a positive excess kurtosis is named “Leptokurtic” indicating heavy tails with large outliers and less variable, and with a negative excess kurtosis is known as “Platykurtic” which have the flattest peak and highly dispersed [<xref ref-type="bibr" rid="CR52">52</xref>].</p>
                <p id="Par38">A <italic>Z</italic>-score for skewness and kurtosis can be obtained by dividing the skew values or excess kurtosis by their standard errors, respectively, which are shown as <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z_{Skewness}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi mathvariant="italic">Skewness</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq25.gif"/></alternatives></inline-formula> and <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z_{Kurtosis}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi mathvariant="italic">Kurtosis</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq26.gif"/></alternatives></inline-formula> in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>. As the studied sample size of the countries is large, either an absolute skew value larger than 2 or an absolute kurtosis larger than 7 can be used as reference values for determining the significance of non-normality [<xref ref-type="bibr" rid="CR53">53</xref>]. It is worth mentioning that the utilized models in this study are based on neural networks and deep learning. These methods are nonparametric that model the data without prior assumptions of their distribution [<xref ref-type="bibr" rid="CR54">54</xref>].</p>
                <p id="Par39">Finally, the range for each country is the difference between the dataset’s largest and smallest observations, which expresses a country dataset’s dispersion.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Statistical properties of the daily data of the COVID-19 cases for the USA, Brazil, India, France, and the UK</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Country</th><th align="left">USA</th><th align="left">Brazil</th><th align="left">India</th><th align="left">France</th><th align="left">UK</th></tr></thead><tbody><tr><td align="left">Sample size</td><td align="left">432</td><td align="left">397</td><td align="left">424</td><td align="left">430</td><td align="left">423</td></tr><tr><td align="left">Mean</td><td align="left">70,052</td><td align="left">31,574</td><td align="left">28,395</td><td align="left">11,088</td><td align="left">10,277</td></tr><tr><td align="left">Median</td><td align="left">47,043</td><td align="left">28,629</td><td align="left">18,537</td><td align="left">4321.5</td><td align="left">4,329</td></tr><tr><td align="left">Mode</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">Standard deviation</td><td align="left">68,074.5</td><td align="left">23,178.8</td><td align="left">27,378.6</td><td align="left">14,657.6</td><td align="left">13,657.4</td></tr><tr><td align="left">Skewness</td><td align="left">1.31</td><td align="left">0.45</td><td align="left">0.83</td><td align="left">2.24</td><td align="left">1.88</td></tr><tr><td align="left">Standard error of skewness</td><td align="left">0.12</td><td align="left">0.12</td><td align="left">0.12</td><td align="left">0.12</td><td align="left">0.12</td></tr><tr><td align="left"><inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z_{Skewness}}$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi mathvariant="italic">Skewness</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq27.gif"/></alternatives></inline-formula></td><td align="left">11.15</td><td align="left">3.68</td><td align="left">6.98</td><td align="left">19.01</td><td align="left">15.82</td></tr><tr><td align="left">Kurtosis</td><td align="left">0.76</td><td align="left"><inline-formula id="IEq28"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M68"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq28.gif"/></alternatives></inline-formula>0.53</td><td align="left"><inline-formula id="IEq29"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M70"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq29.gif"/></alternatives></inline-formula>0.42</td><td align="left">7.43</td><td align="left">3.37</td></tr><tr><td align="left">Standard error of kurtosis</td><td align="left">0.23</td><td align="left">0.24</td><td align="left">0.24</td><td align="left">0.24</td><td align="left">0.24</td></tr><tr><td align="left"><inline-formula id="IEq30"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z_{Kurtosis}}$$\end{document}</tex-math><mml:math id="M72"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi mathvariant="italic">Kurtosis</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq30.gif"/></alternatives></inline-formula></td><td align="left">3.26</td><td align="left"><inline-formula id="IEq31"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M74"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq31.gif"/></alternatives></inline-formula>2.16</td><td align="left"><inline-formula id="IEq32"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M76"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq32.gif"/></alternatives></inline-formula>1.78</td><td align="left">31.60</td><td align="left">14.21</td></tr><tr><td align="left">Min</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">Max</td><td align="left">300,416</td><td align="left">100,158</td><td align="left">97,894</td><td align="left">106,091</td><td align="left">68,192</td></tr><tr><td align="left">Range</td><td align="left">300,416</td><td align="left">100,158</td><td align="left">97,894</td><td align="left">106,091</td><td align="left">68,192</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Statistical properties of the daily data of the COVID-19 cases for Russia, Italy, Spain, Turkey, and Germany</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Country</th><th align="left">Russia</th><th align="left">Italy</th><th align="left">Spain</th><th align="left">Turkey</th><th align="left">Germany</th></tr></thead><tbody><tr><td align="left">Sample size</td><td align="left">423</td><td align="left">423</td><td align="left">422</td><td align="left">383</td><td align="left">427</td></tr><tr><td align="left">Mean</td><td align="left">10,566</td><td align="left">8,351</td><td align="left">7,965</td><td align="left">8,376</td><td align="left">6,521</td></tr><tr><td align="left">Median</td><td align="left">8,764</td><td align="left">2,843</td><td align="left">1,931</td><td align="left">2,026</td><td align="left">1,898</td></tr><tr><td align="left">Mode</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">987</td><td align="left">0</td></tr><tr><td align="left">Standard deviation</td><td align="left">8,348.8</td><td align="left">9,959.9</td><td align="left">12,779.9</td><td align="left">42,585.6</td><td align="left">8,722.4</td></tr><tr><td align="left">Skewness</td><td align="left">0.68</td><td align="left">1.16</td><td align="left">2.85</td><td align="left">18.45</td><td align="left">1.76</td></tr><tr><td align="left">Standard error of skewness</td><td align="left">0.12</td><td align="left">0.12</td><td align="left">0.12</td><td align="left">0.13</td><td align="left">0.12</td></tr><tr><td align="left"><inline-formula id="IEq33"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z_{Skewness}}$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi mathvariant="italic">Skewness</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq33.gif"/></alternatives></inline-formula></td><td align="left">5.75</td><td align="left">9.77</td><td align="left">23.97</td><td align="left">147.56</td><td align="left">14.88</td></tr><tr><td align="left">Kurtosis</td><td align="left"><inline-formula id="IEq34"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M80"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq34.gif"/></alternatives></inline-formula>0.57</td><td align="left">0.41</td><td align="left">11.14</td><td align="left">353.45</td><td align="left">2.90</td></tr><tr><td align="left">Standard error of kurtosis</td><td align="left">0.24</td><td align="left">0.24</td><td align="left">0.24</td><td align="left">0.25</td><td align="left">0.24</td></tr><tr><td align="left"><inline-formula id="IEq35"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z_{Kurtosis}}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi mathvariant="italic">Kurtosis</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq35.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq36"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M84"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq36.gif"/></alternatives></inline-formula>2.41</td><td align="left">1.74</td><td align="left">47.00</td><td align="left">1,419.46</td><td align="left">12.29</td></tr><tr><td align="left">Min</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">Max</td><td align="left">29,499</td><td align="left">40,902</td><td align="left">93,822</td><td align="left">823,225</td><td align="left">49,044</td></tr><tr><td align="left">Range</td><td align="left">29,499</td><td align="left">40,902</td><td align="left">93,822</td><td align="left">823,225</td><td align="left">49,044</td></tr></tbody></table></table-wrap></p>
              </sec>
              <sec id="Sec14">
                <title>Measures of evaluation</title>
                <p id="Par40">The two forecasting performance measures used in the comparison are (1) symmetric mean absolute percentage error (SMAPE) which is defined as<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {SMAPE}=\frac{1}{n}\sum _{t=1}^{n}\frac{\left| y_t-f_t\right| }{\frac{\left( \left| y_t\right| +\left| f_t\right| \right) }{2}}\times 100 , \end{aligned}$$\end{document}</tex-math><mml:math id="M86" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">SMAPE</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mfenced close="|" open="|"><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced><mml:mfrac><mml:mfenced close=")" open="("><mml:mfenced close="|" open="|"><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mn>2</mml:mn></mml:mfrac></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>and (2) the root mean square error (RMSE) which is obtained by<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} RMSE=\sqrt{\frac{1}{n}\sum _{t=1}^{n}\left( y_t-f_t\right) ^2} \end{aligned}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="521_2021_6548_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq37"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_t$$\end{document}</tex-math><mml:math id="M90"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq37.gif"/></alternatives></inline-formula> and <inline-formula id="IEq38"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_t$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq38.gif"/></alternatives></inline-formula> are the predicted and observed values at time point <italic>t</italic>, respectively.</p>
              </sec>
              <sec id="Sec15">
                <title>Hyperparameter selection</title>
                <p id="Par41">Table <xref rid="Tab4" ref-type="table">4</xref> illustrates the domain of all hyperparameters utilized in the implemented models. The lag hyperparameter exploited in transforming input time series into samples suitable for deep learning techniques has a significant impact on obtaining models that can forecast future values with minimum error [<xref ref-type="bibr" rid="CR49">49</xref>]. Another important hyperparameter is the learning rate that regulates how the weights are adjusted during the model training. Additionally, the utilized models throughout this study have different key hyperparameters that influence the forecasting accuracy of obtained models. The hyperparameters specific to each model are provided in Table <xref rid="Tab4" ref-type="table">4</xref>. Also, as outlined in the previous section (see Table <xref rid="Tab1" ref-type="table">1</xref>), each utilized deep learning techniques contain a dense layer that follows the sequence capturing layer (e.g., LSTM, CNN, or GRU) and an output layer, which produces the outputs. These layers are common in all the utilized models, and their ranges are also given in Table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Range of the hyperparameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Hyperparameter</th><th align="left">Range</th></tr></thead><tbody><tr><td align="left" rowspan="4">Common hyperparameters</td><td align="left">Lag: [10, 11, 12, 13, 14, 15]</td></tr><tr><td align="left">Learning rate: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]</td></tr><tr><td align="left">Dense activation function: [ReLU, Linear]</td></tr><tr><td align="left">Output activation function: [ReLU, Linear]</td></tr><tr><td align="left" rowspan="3">LSTM</td><td align="left">Activation function: [ReLU, Linear]</td></tr><tr><td align="left">Dropout rate: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]</td></tr><tr><td align="left">Number of units: [4, 8, 16, 32, 64, 128]</td></tr><tr><td align="left" rowspan="2">CNN_FE</td><td align="left">Kernel size: [2, 3, 4]</td></tr><tr><td align="left">Number of filters: [32, 64, 128, 256]</td></tr><tr><td align="left" rowspan="3">GRU</td><td align="left">Activation function: [ReLU, Linear]</td></tr><tr><td align="left">Number of units: [4, 8, 16, 32, 64, 128]</td></tr><tr><td align="left">Dropout rate: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]</td></tr></tbody></table></table-wrap></p>
              </sec>
              <sec id="Sec16">
                <title>Data preprocessing</title>
                <p id="Par42">According to the methodology shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref> and the procedure described in Algorithm 1, firstly, a new time series is generated via augmentation. Next, the original and the augmented series are transformed into samples with the input–output format. Then, the resulted samples are split into train set, validation set, and test set following the holdout procedure. Finally, a new validation set is created by concatenating the validation samples corresponding to the original series and the augmented one. It should be noted that this study adopts multi-output forecasting, and the sample generation process is performed using the lag (size of the input window) and the output window. In all experiments, the output window is set to 7 days. Following the multi-output forecasting strategy, for time series <inline-formula id="IEq39"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T:t_1,t_2,t_3,t_4,t_5,t_6,t_7,t_8,\ldots ,t_n$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mi>T</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq39.gif"/></alternatives></inline-formula> and Lag <inline-formula id="IEq40"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=5$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq40.gif"/></alternatives></inline-formula>, Output_window <inline-formula id="IEq41"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=2$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq41.gif"/></alternatives></inline-formula>, the created instances are shown in Table <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>An example of the sample generation function</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Input</th><th align="left">Output</th></tr></thead><tbody><tr><td align="left"><inline-formula id="IEq42"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_1, t_2, t_3, t_4, t_5$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq42.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq43"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_6, t_7$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq43.gif"/></alternatives></inline-formula></td></tr><tr><td align="left"><inline-formula id="IEq44"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_2, t_3, t_4, t_5, t_6$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq44.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq45"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_7, t_8$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq45.gif"/></alternatives></inline-formula></td></tr><tr><td align="left"><inline-formula id="IEq46"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_3, t_4, t_5, t_6, t_7$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq46.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq47"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_8, t_9$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>9</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq47.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">.</td><td align="left">.</td></tr><tr><td align="left"><inline-formula id="IEq48"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_{n-(L+O)+1}, t_{n-(L+O)+2}, t_{n-(L+O)+3}, t_{n-(L+O)+4}, t_{n-(L+O)+5}$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq48.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq49"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_{n-1}, t_{n}$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="521_2021_6548_Article_IEq49.gif"/></alternatives></inline-formula></td></tr></tbody></table></table-wrap></p>
              </sec>
              <sec id="Sec17">
                <title>Evaluation of the effectiveness of the proposed approach</title>
                <p id="Par43">In this section, we investigate whether our proposed approach is able to enhance the forecasting accuracy of the deep learning models based on LSTM, CNN, and GRU. We run our experiments on the data of the before-mentioned ten countries. All experiments are repeated ten times, and the average performance measures are reported.</p>
                <sec id="Sec18">
                  <title>LSTM</title>
                  <p id="Par44">Table <xref rid="Tab6" ref-type="table">6</xref> shows the results obtained using the deep learning model based on LSTM. As it can be seen, the model obtained using the proposed approach (LSTM_Aug) leads to a lower error in terms of SMAPE and RMSE for eight countries. LSTM_Aug achieves superior results for the USA, Brazil, India, France, Russia, the UK, Spain, and Turkey. Also, the mean SMAPE for LSTM_Aug is 0.82, which is lower than the one for LSTM, which has a mean of 1.30. Also, regarding RMSE, the mean RMSE measure for LSTM_Aug is significantly lower than the mean RMSE of LSTM. The experiments indicate that the results of LSTM_Aug are excellent, and the proposed approach significantly impacts the performance of LSTM.<table-wrap id="Tab6"><label>Table 6</label><caption><p>LSTM results for ten countries in terms of SMAPE and RMSE for regular and augmentation approaches—LSTM_Aug is obtained following the proposed approach</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Country</th><th align="left" colspan="2">SMAPE</th><th align="left" colspan="2">RMSE</th></tr><tr><th align="left">LSTM_Aug</th><th align="left">LSTM</th><th align="left">LSTM_Aug</th><th align="left">LSTM</th></tr></thead><tbody><tr><td align="left">USA</td><td align="left"><bold>0.87</bold></td><td align="left">1.73</td><td align="left"><bold>291935.32</bold></td><td align="left">570575.55</td></tr><tr><td align="left">Brazil</td><td align="left"><bold>0.62</bold></td><td align="left">0.76</td><td align="left"><bold>83105.03</bold></td><td align="left">101239.26</td></tr><tr><td align="left">India</td><td align="left"><bold>0.50</bold></td><td align="left">0.77</td><td align="left"><bold>81741.89</bold></td><td align="left">112064.7</td></tr><tr><td align="left">France</td><td align="left"><bold>0.60</bold></td><td align="left">0.66</td><td align="left"><bold>40719.57</bold></td><td align="left">45009.65</td></tr><tr><td align="left">Russia</td><td align="left"><bold>0.86</bold></td><td align="left">1.23</td><td align="left"><bold>41813.95</bold></td><td align="left">59394.68</td></tr><tr><td align="left">UK</td><td align="left"><bold>0.46</bold></td><td align="left">2.32</td><td align="left"><bold>22279.42</bold></td><td align="left">106723.4</td></tr><tr><td align="left">Italy</td><td align="left">0.62</td><td align="left"><bold>0.44</bold></td><td align="left">22976.17</td><td align="left"><bold>18834.81</bold></td></tr><tr><td align="left">Spain</td><td align="left"><bold>1.29</bold></td><td align="left">2.37</td><td align="left"><bold>51592.85</bold></td><td align="left">86450.37</td></tr><tr><td align="left">Turkey</td><td align="left"><bold>1.71</bold></td><td align="left">2.13</td><td align="left"><bold>66494.82</bold></td><td align="left">78740.49</td></tr><tr><td align="left">Germany</td><td align="left">0.73</td><td align="left"><bold>0.65</bold></td><td align="left">24476.58</td><td align="left"><bold>21542.29</bold></td></tr><tr><td align="left">Mean</td><td align="left"><bold>0.82</bold></td><td align="left">1.31</td><td align="left"><bold>72713.56</bold></td><td align="left">120057.52</td></tr></tbody></table><table-wrap-foot><p>Bold values indicate the best results</p></table-wrap-foot></table-wrap></p>
                </sec>
                <sec id="Sec19">
                  <title>Convolution model</title>
                  <p id="Par45">The results of experiments using deep learning model based on CNN are given in Table <xref rid="Tab7" ref-type="table">7</xref>. The best values are shown in boldface. In terms of SMAPE, the CNN_Aug model achieves better performance in 9 countries out of 10. Also, in terms of RMSE, we see a similar performance where CNN_Aug beats CNN in 9 cases. To give a comprehensive report on the performance of models, the mean SMAPE and mean RMSE measures also are computed. The mean SMAPE for CNN_Aug is 0.63, which is lower than that for CNN (0.73). Also, this is true for mean RMSE in which the CNN_Aug achieves lower error than CNN. The results indicate that CNN_Aug outperforms CNN, and that using the proposed data preparation strategy considerably enhances the accuracy of CNN-based deep learning models.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Results of CNN for ten countries in terms of SMAPE and RMSE for regular and augmentation approaches. CNN_Aug is obtained following the proposed approach</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Country</th><th align="left" colspan="2">SMAPE</th><th align="left" colspan="2">RMSE</th></tr><tr><th align="left">CNN_Aug</th><th align="left">CNN</th><th align="left">CNN_Aug</th><th align="left">CNN</th></tr></thead><tbody><tr><td align="left">USA</td><td align="left"><bold>0.24</bold></td><td align="left">0.32</td><td align="left"><bold>81157.55</bold></td><td align="left">104899.61</td></tr><tr><td align="left">Brazil</td><td align="left"><bold>0.54</bold></td><td align="left">0.67</td><td align="left"><bold>73194.5</bold></td><td align="left">88552.5</td></tr><tr><td align="left">India</td><td align="left">0.66</td><td align="left"><bold>0.52</bold></td><td align="left">102096.63</td><td align="left"><bold>81892.72</bold></td></tr><tr><td align="left">France</td><td align="left"><bold>0.49</bold></td><td align="left">0.51</td><td align="left"><bold>30442.08</bold></td><td align="left">33311.58</td></tr><tr><td align="left">Russia</td><td align="left"><bold>0.32</bold></td><td align="left">0.48</td><td align="left"><bold>15545.49</bold></td><td align="left">23084.47</td></tr><tr><td align="left">UK</td><td align="left"><bold>0.89</bold></td><td align="left">0.93</td><td align="left"><bold>39566.78</bold></td><td align="left">41051.61</td></tr><tr><td align="left">Italy</td><td align="left"><bold>0.35</bold></td><td align="left">0.50</td><td align="left"><bold>14129.16</bold></td><td align="left">19769.05</td></tr><tr><td align="left">Spain</td><td align="left"><bold>1.16</bold></td><td align="left">1.41</td><td align="left"><bold>47599.51</bold></td><td align="left">54012.22</td></tr><tr><td align="left">Turkey</td><td align="left"><bold>1.04</bold></td><td align="left">1.22</td><td align="left"><bold>39365.34</bold></td><td align="left">45970.07</td></tr><tr><td align="left">Germany</td><td align="left"><bold>0.62</bold></td><td align="left">0.73</td><td align="left"><bold>22090.1</bold></td><td align="left">26058.33</td></tr><tr><td align="left">Mean</td><td align="left"><bold>0.63</bold></td><td align="left">0.73</td><td align="left"><bold>46518.71</bold></td><td align="left">51860.22</td></tr></tbody></table><table-wrap-foot><p>Bold values indicate the best results</p></table-wrap-foot></table-wrap></p>
                </sec>
                <sec id="Sec20">
                  <title>GRU model</title>
                  <p id="Par46">Table <xref rid="Tab8" ref-type="table">8</xref> provides the results of experiments using the deep learning method based on GRU. Similar to the previously mentioned models, we compare the model obtained using the regular experimental setting (GRU model) with the model obtained using the proposed augmentation approach (GRU_aug). GRU_Aug and GRU perform similarly as each of them achieves minimum error in terms of SMAPE and RMSE in 5 cases out of 10 countries. This can be attributed to the fact that the GRU model uses different gating units and use less training parameters and this prevents it from overfitting.<table-wrap id="Tab8"><label>Table 8</label><caption><p>GRU results for ten countries in terms of SMAPE and RMSE for regular and augmentation approaches—GRU_Aug is obtained following the proposed approach</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Country</th><th align="left" colspan="2">SMAPE</th><th align="left" colspan="2">RMSE</th></tr><tr><th align="left">GRU_Aug</th><th align="left">GRU</th><th align="left">GRU_Aug</th><th align="left">GRU</th></tr></thead><tbody><tr><td align="left">USA</td><td align="left"><bold>0.28</bold></td><td align="left">0.44</td><td align="left"><bold>97535.03</bold></td><td align="left">152119.5</td></tr><tr><td align="left">Brazil</td><td align="left"><bold>0.75</bold></td><td align="left">0.77</td><td align="left"><bold>99050.16</bold></td><td align="left">101927.57</td></tr><tr><td align="left">India</td><td align="left"><bold>0.53</bold></td><td align="left">0.72</td><td align="left"><bold>83931.57</bold></td><td align="left">109283.11</td></tr><tr><td align="left">France</td><td align="left">0.60</td><td align="left"><bold>0.55</bold></td><td align="left">40609.86</td><td align="left"><bold>36529.07</bold></td></tr><tr><td align="left">Russia</td><td align="left"><bold>0.54</bold></td><td align="left">0.93</td><td align="left"><bold>26955.22</bold></td><td align="left">47059.80</td></tr><tr><td align="left">UK</td><td align="left"><bold>0.51</bold></td><td align="left">0.52</td><td align="left"><bold>24700.29</bold></td><td align="left">24989.78</td></tr><tr><td align="left">Italy</td><td align="left">0.43</td><td align="left"><bold>0.36</bold></td><td align="left">18585.69</td><td align="left"><bold>15907.25</bold></td></tr><tr><td align="left">Spain</td><td align="left">4.1</td><td align="left"><bold>1.25</bold></td><td align="left">145384.97</td><td align="left"><bold>50215.92</bold></td></tr><tr><td align="left">Turkey</td><td align="left">1.64</td><td align="left"><bold>1.48</bold></td><td align="left">64087.62</td><td align="left"><bold>57313.48</bold></td></tr><tr><td align="left">Germany</td><td align="left">0.76</td><td align="left"><bold>0.66</bold></td><td align="left">25800.14</td><td align="left"><bold>21647.34</bold></td></tr><tr><td align="left">Mean</td><td align="left">1.01</td><td align="left"><bold>0.77</bold></td><td align="left">62664.06</td><td align="left"><bold>61699.28</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values indicate the best results</p></table-wrap-foot></table-wrap></p>
                </sec>
              </sec>
              <sec id="Sec21">
                <title>Overall comparison of the proposed method with regular approach</title>
                <p id="Par47">To provide an overall description of the results, we summarize the results of experiments and show the top augmentation model for each country in Table <xref rid="Tab9" ref-type="table">9</xref>. As can be seen from the table, for all ten countries, the models based on the proposed augmentation approach show the top accuracy in terms of both SMAPE and RMSE. This demonstrates the effectiveness of the proposed approach in increasing the forecasting accuracy of the deep learning methods. Also, CNN Aug performs excellently and reaches the best model for eight countries including, the USA, Brazil, France, Russia, Italy, Spain, Turkey, and Germany. Besides, LSTM_Aug achieves the best accuracy for two countries. Furthermore, as illustrated in Table <xref rid="Tab9" ref-type="table">9</xref>, in no country is GRU_Aug superior.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Top model for each country</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Country</th><th align="left">Top model</th></tr></thead><tbody><tr><td align="left">USA</td><td align="left">CNN_Aug</td></tr><tr><td align="left">Brazil</td><td align="left">CNN_Aug</td></tr><tr><td align="left">India</td><td align="left">LSTM_Aug</td></tr><tr><td align="left">France</td><td align="left">CNN_Aug</td></tr><tr><td align="left">Russia</td><td align="left">CNN_Aug</td></tr><tr><td align="left">UK</td><td align="left">LSTM_Aug</td></tr><tr><td align="left">Italy</td><td align="left">CNN_Aug</td></tr><tr><td align="left">Spain</td><td align="left">CNN_Aug</td></tr><tr><td align="left">Turkey</td><td align="left">CNN_Aug</td></tr><tr><td align="left">Germany</td><td align="left">CNN_Aug</td></tr></tbody></table></table-wrap></p>
              </sec>
              <sec id="Sec22">
                <title>Visualizing the results</title>
                <p id="Par48">To further demonstrate the forecasting ability of the obtained models, in this section, the actual and forecasts for each country are visualized in Figs. <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref>, <xref rid="Fig10" ref-type="fig">10</xref>, <xref rid="Fig11" ref-type="fig">11</xref>, <xref rid="Fig12" ref-type="fig">12</xref>, <xref rid="Fig13" ref-type="fig">13</xref>, <xref rid="Fig14" ref-type="fig">14</xref>, <xref rid="Fig15" ref-type="fig">15</xref>, and <xref rid="Fig16" ref-type="fig">16</xref>. The actual values are shown in red in all figures, while the forecasts are shown in black using the best deep learning model (Figs. <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref>, <xref rid="Fig10" ref-type="fig">10</xref>, <xref rid="Fig11" ref-type="fig">11</xref>, <xref rid="Fig12" ref-type="fig">12</xref>, <xref rid="Fig13" ref-type="fig">13</xref>, <xref rid="Fig14" ref-type="fig">14</xref>, <xref rid="Fig15" ref-type="fig">15</xref> and <xref rid="Fig16" ref-type="fig">16</xref>). As Figs. <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref>, <xref rid="Fig10" ref-type="fig">10</xref>, <xref rid="Fig11" ref-type="fig">11</xref>, <xref rid="Fig12" ref-type="fig">12</xref>, <xref rid="Fig13" ref-type="fig">13</xref>, <xref rid="Fig14" ref-type="fig">14</xref>, <xref rid="Fig15" ref-type="fig">15</xref>, and <xref rid="Fig16" ref-type="fig">16</xref> indicate, the predicted cases for the USA, Brazil, France, Russia, UK, Italy, Turkey, and Germany are very close to the actual values; and there is a minimum error. Also, there are overlaps at some of the time points that demonstrate the power of the proposed approach. The plot for India (Figure <xref rid="Fig9" ref-type="fig">9</xref>) indicates that from time point 1 to time point 15, the forecasted values are very close to the real values but after time point 15, the error increases. Furthermore, as shown in Fig. <xref rid="Fig14" ref-type="fig">14</xref>, the inaccuracy is rather substantial in various time points for Spain. This is primarily due to the noise in the country’s input data.<fig id="Fig7"><label>Fig. 7</label><caption><p>Actual and forecasted number of cases for test set—USA</p></caption><graphic xlink:href="521_2021_6548_Fig7_HTML" id="MO16"/></fig><fig id="Fig8"><label>Fig. 8</label><caption><p>Actual and forecasted number of cases for test set—Brazil</p></caption><graphic xlink:href="521_2021_6548_Fig8_HTML" id="MO17"/></fig><fig id="Fig9"><label>Fig. 9</label><caption><p>Actual and forecasted number of cases for test set—India</p></caption><graphic xlink:href="521_2021_6548_Fig9_HTML" id="MO18"/></fig><fig id="Fig10"><label>Fig. 10</label><caption><p>Actual and forecasted number of cases for test set—France</p></caption><graphic xlink:href="521_2021_6548_Fig10_HTML" id="MO19"/></fig><fig id="Fig11"><label>Fig. 11</label><caption><p>Actual and forecasted number of cases for test set—Russia</p></caption><graphic xlink:href="521_2021_6548_Fig11_HTML" id="MO20"/></fig><fig id="Fig12"><label>Fig. 12</label><caption><p>Actual and forecasted number of cases for test set—UK</p></caption><graphic xlink:href="521_2021_6548_Fig12_HTML" id="MO21"/></fig><fig id="Fig13"><label>Fig. 13</label><caption><p>Actual and forecasted number of cases for test set—Italy</p></caption><graphic xlink:href="521_2021_6548_Fig13_HTML" id="MO22"/></fig><fig id="Fig14"><label>Fig. 14</label><caption><p>Actual and forecasted number of cases for test set—Spain</p></caption><graphic xlink:href="521_2021_6548_Fig14_HTML" id="MO23"/></fig><fig id="Fig15"><label>Fig. 15</label><caption><p>Actual and forecasted number of cases for test set—Turkey</p></caption><graphic xlink:href="521_2021_6548_Fig15_HTML" id="MO24"/></fig><fig id="Fig16"><label>Fig. 16</label><caption><p>Actual and forecasted number of cases for test set—Germany</p></caption><graphic xlink:href="521_2021_6548_Fig16_HTML" id="MO25"/></fig></p>
              </sec>
            </sec>
            <sec id="Sec23">
              <title>Discussion</title>
              <p id="Par49">In this study, we proposed a method that uses augmentation techniques to enhance time series forecasting. To conduct experimental study and to test the effectiveness of the proposed idea, we selected three deep learning methods, LSTM, GRU, and CNN. Furthermore, due to the importance of accurate forecasting of COVID-19 infections, data of ten countries with highest cases of infections have been chosen. The results of experiments demonstrated that the models obtained employing LSTM and the proposed idea greatly outperforms the regular LSTM model. Similarly, the proposed method significantly improves the performance of the CNN models. Besides, for GRU, the proposed method achieves an average performance.</p>
              <sec id="Sec24">
                <title>Assumptions</title>
                <p id="Par50">Similar to any time series forecasting task, in this study, we utilize the series past values to train the models. Also, we assume that an optimal hyperparameters for the utilized models have been chosen.</p>
              </sec>
              <sec id="Sec25">
                <title>Implications of the results</title>
                <p id="Par51">Unlike the one-step-ahead forecasting, where a forecasting model uses the previous observations to predict a single time step, the multi-step-ahead forecasting strategy [<xref ref-type="bibr" rid="CR24">24</xref>], which was used in this study, allows forecasting two or more steps. In the COVID-19 forecasting, the multi-step-ahead forecasting is attractive to policymakers. In fact, a longer window forecasting uncovers the trend of pandemic effectively and thus appeal more significant for governments. Also, in terms of SMAPE, the models generated following the proposed idea demonstrate excellent performance. Besides, the Mean SMAPE values for LSTM_Aug, CNN_Aug, and GRU_Aug are 0.82, 0.63, and 1.01, respectively indicating the forecasting power of the proposed method.</p>
              </sec>
              <sec id="Sec26">
                <title>Practical implications</title>
                <p id="Par52">As we mentioned previously, in this study, we formulate forecasting the number of infected cases as a time series forecasting problem in which the data of past observations of a series is used for predicting the future time points. The proposed models forecast the number of infected cases for a longer horizon with minimum error in comparison to their regular counterparts. The forecasts can be utilized by governments to take appropriate decisions in controlling the pandemic.</p>
              </sec>
              <sec id="Sec27">
                <title>Limitations</title>
                <p id="Par53">In this study, we did not access to the other sources of information such as the interventions implemented by each country or vaccination of COVID-19. The models only were learned using the time series of the infections. Another limitation of this study is related to the hyperparameter selection for deep learning methods. As these methods contains a complex architecture, they require more computation. Therefore, investigating every hyperparameter configuration, similar to way performed in the grid search method, may not practicable. Therefore, in this study we used the Bayesian optimization algorithm to search the optimal hyperparameters.</p>
              </sec>
            </sec>
            <sec id="Sec28">
              <title>Conclusion and future work</title>
              <p id="Par54">A new schema based on time series augmentation was suggested in this study to improve the performance of deep learning techniques in time series forecasting. The proposed method’s main idea is to use a time series augmentation technique to create a new time series with the same properties in the original series. Then, we use the generated series to obtain enough samples to train the deep learning methods optimally. The proposed method is implemented in the context of COVID-19 time series forecasting data of the 10 most affected countries using the LSTM, GRU, and CNN models. According to the findings of the experiments, in the majority of countries, the LSTM_Aug model outperformed the standard LSTM model and the CNN_Aug model achieved significant performance than the regular CNN. In addition, GRU_Aug obtained an average performance when compared to the regular GRU. Overall, the models’ performance following the proposed idea is excellent and significantly improves the regular models. As future work, we intend to evaluate the proposed method using other time series augmentation approaches such as dynamic time warping barycentric averaging.</p>
            </sec>
          </body>
          <back>
            <fn-group>
              <fn id="Fn1">
                <label>1</label>
                <p id="Par32"><ext-link ext-link-type="uri" xlink:href="https://pkg.robjhyndman.com/forecast/">https://pkg.robjhyndman.com/forecast/</ext-link>.</p>
              </fn>
              <fn>
                <p>
                  <bold>Publisher's Note</bold>
                </p>
                <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
            </fn-group>
            <notes notes-type="author-contribution">
              <title>Author Contributions</title>
              <p>HA performed conceptualization, methodology design, software development, validation, writing the original draft, and writing, reviewing, and editing. RP had contributed to software development, data curation, and visualization. AB took part in writing, reviewing, and editing.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Availability of data and material</title>
              <p>The data is publicly available at the Humanitarian Data Exchange (HDX) <ext-link ext-link-type="uri" xlink:href="https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases">https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases</ext-link>.</p>
            </notes>
            <notes>
              <title>Declarations</title>
              <notes id="FPar1" notes-type="COI-statement">
                <title>Conflict of interest</title>
                <p id="Par58">The authors declare that they have no conflict of interest.</p>
              </notes>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <mixed-citation publication-type="other">Gorbalenya AE, Baker SC, Baric RS, de Groot RJ, Drosten C, Gulyaeva AA, Haagmans BL, Lauber C, Leontovich AM, Neuman BW, Penzar D, Perlman S, Poon LLM, Samborskiy DV, Sidorov IA, Sola I, Ziebuhr J, Coronaviridae Study Group of the International Committee on Taxonomy of V (2020) The species Severe acute respiratory syndrome-related coronavirus: classifying 2019-nCoV and naming it SARS-CoV-2. Nature Microbiology 5(4):536–544. 10.1038/s41564-020-0695-z</mixed-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nikolopoulos</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Punia</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Schäfers</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Tsinopoulos</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Vasilakis</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Forecasting and planning during a pandemic: COVID-19 growth rates, supply chain disruptions, and governmental decisions</article-title>
                  <source>Eur J Oper Res</source>
                  <year>2021</year>
                  <volume>290</volume>
                  <issue>1</issue>
                  <fpage>99</fpage>
                  <lpage>115</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.ejor.2020.08.001</pub-id>
                  <pub-id pub-id-type="pmid">32836717</pub-id>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <mixed-citation publication-type="other">Torrealba-Rodriguez O, Conde-Gutiérrez R, Hernández-Javier A (2020) Modeling and prediction of COVID-19 in Mexico applying mathematical and computational models. Chaos, Solitons &amp; Fractals:109946</mixed-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Abbasimehr</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Paki</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Prediction of COVID-19 confirmed cases combining deep learning methods and Bayesian optimization</article-title>
                  <source>Chaos, Solitons &amp; Fractals</source>
                  <year>2021</year>
                  <volume>142</volume>
                  <fpage>110511</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110511</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <mixed-citation publication-type="other">Abbasimehr H, Paki R, Bahrini A (2021) Improving the performance of deep learning models using statistical features: The case study of COVID-19 forecasting. Mathematical Methods in the Applied Sciences. 10.1002/mma.7500</mixed-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <mixed-citation publication-type="other">Sultana J, Usha Rani M, Farquad MAH An Extensive Survey on Some Deep-Learning Applications. In: Venkata Krishna P, Obaidat MS (eds) Emerging Research in Data Engineering Systems and Computer Communications, Singapore, 2020. Springer Singapore, pp 511-519</mixed-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <mixed-citation publication-type="other">Dev K, Khowaja SA, Bist AS, Saini V, Bhatia S (2021) Triage of potential covid-19 patients from chest x-ray images using hierarchical convolutional networks. Neural Comput &amp; Applic:1-16</mixed-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <mixed-citation publication-type="other">Singh RK, Pandey R, Babu RN (2021) COVIDScreen: Explainable deep learning framework for differential diagnosis of COVID-19 using chest X-Rays. Neural Comput &amp; Applic:1-22</mixed-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <mixed-citation publication-type="other">Abbasimehr H, Shabani M, Yousefi M (2020) An optimized model using LSTM network for demand forecasting. Comput Ind Eng:106435. 10.1016/j.cie.2020.106435</mixed-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ntakaris</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Mirone</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Kanniainen</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Gabbouj</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Iosifidis</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <article-title>Feature engineering for mid-price prediction with deep learning</article-title>
                  <source>IEEE Access</source>
                  <year>2019</year>
                  <volume>7</volume>
                  <fpage>82390</fpage>
                  <lpage>82412</lpage>
                  <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2924353</pub-id>
                </element-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <mixed-citation publication-type="other">Abbasimehr H, Paki R (2021) Improving time series forecasting using LSTM and attention models. Journal of Ambient Intelligence and Humanized Computing. 10.1007/s12652-020-02761-x</mixed-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bandara</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Bergmeir</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Smyl</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Forecasting across time series databases using recurrent neural networks on groups of similar series: clustering approach</article-title>
                  <source>Expert Syst Appl</source>
                  <year>2020</year>
                  <volume>140</volume>
                  <fpage>112896</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.eswa.2019.112896</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <mixed-citation publication-type="other">Olah C (2015) Understanding lstm networks, 2015 Retrieved from:<ext-link ext-link-type="uri" xlink:href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</ext-link></mixed-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Arora</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Kumar</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Panigrahi</surname>
                      <given-names>BK</given-names>
                    </name>
                  </person-group>
                  <article-title>Prediction and analysis of COVID-19 positive cases using deep learning models: a descriptive case study of India</article-title>
                  <source>Chaos, Solitons &amp; Fractals</source>
                  <year>2020</year>
                  <volume>139</volume>
                  <fpage>110017</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110017</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kırbaş</surname>
                      <given-names>İ</given-names>
                    </name>
                    <name>
                      <surname>Sözen</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Tuncer</surname>
                      <given-names>AD</given-names>
                    </name>
                    <name>
                      <surname>Kazancğolu</surname>
                      <given-names>FŞ</given-names>
                    </name>
                  </person-group>
                  <article-title>Comparative analysis and forecasting of COVID-19 cases in various European countries with ARIMA, NARNN and LSTM approaches</article-title>
                  <source>Chaos, Solitons &amp; Fractals</source>
                  <year>2020</year>
                  <volume>138</volume>
                  <fpage>110015</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110015</pub-id>
                </element-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shahid</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Zameer</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Muneeb</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Predictions for COVID-19 with deep learning models of LSTM, GRU and Bi-LSTM</article-title>
                  <source>Chaos, Solitons &amp; Fractals</source>
                  <year>2020</year>
                  <volume>140</volume>
                  <fpage>110212</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110212</pub-id>
                </element-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wang</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Zheng</surname>
                      <given-names>X</given-names>
                    </name>
                    <name>
                      <surname>Ai</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Liu</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Zhu</surname>
                      <given-names>B</given-names>
                    </name>
                  </person-group>
                  <article-title>Time series prediction for the epidemic trends of COVID-19 using the improved LSTM deep learning method: case studies in Russia, Peru and Iran</article-title>
                  <source>Chaos, Solitons &amp; Fractals</source>
                  <year>2020</year>
                  <volume>140</volume>
                  <fpage>110214</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.chaos.2020.110214</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tran</surname>
                      <given-names>DT</given-names>
                    </name>
                    <name>
                      <surname>Iosifidis</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Kanniainen</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Gabbouj</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Temporal attention-augmented bilinear network for financial time-series data analysis</article-title>
                  <source>IEEE Trans Neural Netw Learn Syst</source>
                  <year>2018</year>
                  <volume>30</volume>
                  <issue>5</issue>
                  <fpage>1407</fpage>
                  <lpage>1418</lpage>
                  <pub-id pub-id-type="doi">10.1109/TNNLS.2018.2869225</pub-id>
                  <pub-id pub-id-type="pmid">30281493</pub-id>
                </element-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Yan</surname>
                      <given-names>W</given-names>
                    </name>
                  </person-group>
                  <article-title>Toward automatic time-series forecasting using neural networks</article-title>
                  <source>IEEE Trans Neural Netw Learn Syst</source>
                  <year>2012</year>
                  <volume>23</volume>
                  <issue>7</issue>
                  <fpage>1028</fpage>
                  <lpage>1039</lpage>
                  <pub-id pub-id-type="doi">10.1109/TNNLS.2012.2198074</pub-id>
                  <pub-id pub-id-type="pmid">24807130</pub-id>
                </element-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bergmeir</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Hyndman</surname>
                      <given-names>RJ</given-names>
                    </name>
                    <name>
                      <surname>Benítez</surname>
                      <given-names>JM</given-names>
                    </name>
                  </person-group>
                  <article-title>Bagging exponential smoothing methods using STL decomposition and Box-Cox transformation</article-title>
                  <source>Int J Forecast</source>
                  <year>2016</year>
                  <volume>32</volume>
                  <issue>2</issue>
                  <fpage>303</fpage>
                  <lpage>312</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.ijforecast.2015.07.002</pub-id>
                </element-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <mixed-citation publication-type="other">Bandara K, Hewamalage H, Liu Y-H, Kang Y, Bergmeir C (2020) Improving the Accuracy of Global Forecasting Models using Time Series Data Augmentation. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/200802663">arXiv:200802663</ext-link></mixed-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <mixed-citation publication-type="other">Chung J, Gulcehre C, Cho K, Bengio Y (2014) Empirical evaluation of gated recurrent neural networks on sequence modeling. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/14123555">arXiv:14123555</ext-link></mixed-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <mixed-citation publication-type="other">Goodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT press</mixed-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bao</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Xiong</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Hu</surname>
                      <given-names>Z</given-names>
                    </name>
                  </person-group>
                  <article-title>Multi-step-ahead time series prediction using multiple-output support vector regression</article-title>
                  <source>Neurocomputing</source>
                  <year>2014</year>
                  <volume>129</volume>
                  <fpage>482</fpage>
                  <lpage>493</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neucom.2013.09.010</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ben Taieb</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Sorjamaa</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Bontempi</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>Multiple-output modeling for multi-step-ahead time series forecasting</article-title>
                  <source>Neurocomputing</source>
                  <year>2010</year>
                  <volume>73</volume>
                  <issue>10</issue>
                  <fpage>1950</fpage>
                  <lpage>1957</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neucom.2009.11.030</pub-id>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <mixed-citation publication-type="other">Castillo O, Melin P (2020) Forecasting of COVID-19 Time Series for Countries in the World based on a Hybrid Approach Combining the Fractal Dimension and Fuzzy Logic. Chaos, Solitons &amp; Fractals:110242</mixed-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <mixed-citation publication-type="other">Rahimi I, Chen F, Gandomi AH (2021) A review on COVID-19 forecasting models. Neural Comput &amp; Applic. 10.1007/s00521-020-05626-8</mixed-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Al-Qaness</surname>
                      <given-names>MA</given-names>
                    </name>
                    <name>
                      <surname>Ewees</surname>
                      <given-names>AA</given-names>
                    </name>
                    <name>
                      <surname>Fan</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Abd El Aziz</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Optimization method for forecasting confirmed cases of COVID-19 in China</article-title>
                  <source>J Clinical Med</source>
                  <year>2020</year>
                  <volume>9</volume>
                  <issue>3</issue>
                  <fpage>674</fpage>
                  <pub-id pub-id-type="doi">10.3390/jcm9030674</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <mixed-citation publication-type="other">Jia L, Li K, Jiang Y, Guo X (2020) Prediction and analysis of Coronavirus Disease 2019. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/200305447">arXiv:200305447</ext-link></mixed-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <mixed-citation publication-type="other">Castorina P, Iorio A, Lanteri D (2020) Data analysis on Coronavirus spreading by macroscopic growth laws. International Journal of Modern Physics C:2050103</mixed-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Roosa</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Lee</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Luo</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Kirpich</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Rothenberg</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Hyman</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Yan</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Chowell</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>Real-time forecasts of the COVID-19 epidemic in China from February 5th to February 24th, 2020</article-title>
                  <source>Infectious Dis Modell</source>
                  <year>2020</year>
                  <volume>5</volume>
                  <fpage>256</fpage>
                  <lpage>263</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.idm.2020.02.002</pub-id>
                </element-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ahmadi</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Fadaei</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Shirani</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Rahmani</surname>
                      <given-names>F</given-names>
                    </name>
                  </person-group>
                  <article-title>Modeling and forecasting trend of COVID-19 epidemic in Iran until May 13, 2020</article-title>
                  <source>Med J Islamic Republic of Iran</source>
                  <year>2020</year>
                  <volume>34</volume>
                  <fpage>27</fpage>
                </element-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Melin</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Monica</surname>
                      <given-names>JC</given-names>
                    </name>
                    <name>
                      <surname>Sanchez</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Castillo</surname>
                      <given-names>O</given-names>
                    </name>
                  </person-group>
                  <article-title>Multiple ensemble neural network models with fuzzy response aggregation for predicting COVID-19 time series: the case of Mexico</article-title>
                  <source>Healthcare (Basel)</source>
                  <year>2020</year>
                  <volume>8</volume>
                  <issue>2</issue>
                  <fpage>181</fpage>
                  <pub-id pub-id-type="doi">10.3390/healthcare8020181</pub-id>
                </element-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <mixed-citation publication-type="other">Leila M, Mozhgan S, Marziyeh Sadat S (2020) Exponentially Increasing Trend of Infected Patients with COVID-19 in Iran: A Comparison of Neural Network and ARIMA Forecasting Models. Iranian Journal of Public Health 49 (Supple 1). 10.18502/ijph.v49iS1.3675</mixed-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Petropoulos</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Makridakis</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Forecasting the novel coronavirus COVID-19</article-title>
                  <source>PLoS One</source>
                  <year>2020</year>
                  <volume>15</volume>
                  <issue>3</issue>
                  <fpage>e0231236</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0231236</pub-id>
                  <pub-id pub-id-type="pmid">32231392</pub-id>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hasan</surname>
                      <given-names>N</given-names>
                    </name>
                  </person-group>
                  <article-title>A methodological approach for predicting COVID-19 epidemic using EEMD-ANN hybrid model</article-title>
                  <source>Internet of Things</source>
                  <year>2020</year>
                  <volume>11</volume>
                  <fpage>100228</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.iot.2020.100228</pub-id>
                </element-citation>
              </ref>
              <ref id="CR37">
                <label>37.</label>
                <mixed-citation publication-type="other">Li S, Lin Y, Zhu T, Fan M, Xu S, Qiu W, Chen C, Li L, Wang Y, Yan J (2021) Development and external evaluation of predictions models for mortality of COVID-19 patients using machine learning method. Neural Comput &amp; Applic:1-10</mixed-citation>
              </ref>
              <ref id="CR38">
                <label>38.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hochreiter</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Schmidhuber</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Long short-term memory</article-title>
                  <source>Neural Comput</source>
                  <year>1997</year>
                  <volume>9</volume>
                  <issue>8</issue>
                  <fpage>1735</fpage>
                  <lpage>1780</lpage>
                  <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
                  <pub-id pub-id-type="pmid">9377276</pub-id>
                </element-citation>
              </ref>
              <ref id="CR39">
                <label>39.</label>
                <mixed-citation publication-type="other">Cho K, van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In, Doha, Qatar, Oct 2014. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pp 1724-1734. 10.3115/v1/D14-1179</mixed-citation>
              </ref>
              <ref id="CR40">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rawat</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Wang</surname>
                      <given-names>Z</given-names>
                    </name>
                  </person-group>
                  <article-title>Deep convolutional neural networks for image classification: a comprehensive review</article-title>
                  <source>Neural Comput</source>
                  <year>2017</year>
                  <volume>29</volume>
                  <issue>9</issue>
                  <fpage>2352</fpage>
                  <lpage>2449</lpage>
                  <pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id>
                  <pub-id pub-id-type="pmid">28599112</pub-id>
                </element-citation>
              </ref>
              <ref id="CR41">
                <label>41.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cleveland</surname>
                      <given-names>RB</given-names>
                    </name>
                    <name>
                      <surname>Cleveland</surname>
                      <given-names>WS</given-names>
                    </name>
                    <name>
                      <surname>McRae</surname>
                      <given-names>JE</given-names>
                    </name>
                    <name>
                      <surname>Terpenning</surname>
                      <given-names>I</given-names>
                    </name>
                  </person-group>
                  <article-title>STL: A seasonal-trend decomposition</article-title>
                  <source>J Official statistics</source>
                  <year>1990</year>
                  <volume>6</volume>
                  <issue>1</issue>
                  <fpage>3</fpage>
                  <lpage>73</lpage>
                </element-citation>
              </ref>
              <ref id="CR42">
                <label>42.</label>
                <mixed-citation publication-type="other">Lahiri SN (2013) Resampling methods for dependent data. Springer Science &amp; Business Media</mixed-citation>
              </ref>
              <ref id="CR43">
                <label>43.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wu</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Chen</surname>
                      <given-names>X-Y</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Xiong</surname>
                      <given-names>L-D</given-names>
                    </name>
                    <name>
                      <surname>Lei</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Deng</surname>
                      <given-names>S-H</given-names>
                    </name>
                  </person-group>
                  <article-title>Hyperparameter optimization for machine learning models based on bayesian optimizationb</article-title>
                  <source>J Electron Sci Technol</source>
                  <year>2019</year>
                  <volume>17</volume>
                  <issue>1</issue>
                  <fpage>26</fpage>
                  <lpage>40</lpage>
                  <pub-id pub-id-type="doi">10.11989/JEST.1674-862X.80904120</pub-id>
                </element-citation>
              </ref>
              <ref id="CR44">
                <label>44.</label>
                <mixed-citation publication-type="other">Brochu E, Cora VM, De Freitas N (2010) A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/10122599">arXiv:10122599</ext-link></mixed-citation>
              </ref>
              <ref id="CR45">
                <label>45.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Calandra</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Seyfarth</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Peters</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Deisenroth</surname>
                      <given-names>MP</given-names>
                    </name>
                  </person-group>
                  <article-title>Bayesian optimization for learning gaits under uncertainty</article-title>
                  <source>Ann Math Artificial Intell</source>
                  <year>2016</year>
                  <volume>76</volume>
                  <issue>1</issue>
                  <fpage>5</fpage>
                  <lpage>23</lpage>
                  <pub-id pub-id-type="doi">10.1007/s10472-015-9463-9</pub-id>
                </element-citation>
              </ref>
              <ref id="CR46">
                <label>46.</label>
                <mixed-citation publication-type="other">Chollet F (2015) Keras Accessed January 2020. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link></mixed-citation>
              </ref>
              <ref id="CR47">
                <label>47.</label>
                <mixed-citation publication-type="other">Novel Coronavirus (COVID-19) Cases Data (2020). Retrieved from <ext-link ext-link-type="uri" xlink:href="https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases">https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases</ext-link></mixed-citation>
              </ref>
              <ref id="CR48">
                <label>48.</label>
                <mixed-citation publication-type="other">CRITICAL TRENDS: TRACKING CRITICAL DATA, (2020) Accessed on: June 2020 Retrieved from: <ext-link ext-link-type="uri" xlink:href="https://coronavirus.jhu.edu/data">https://coronavirus.jhu.edu/data</ext-link></mixed-citation>
              </ref>
              <ref id="CR49">
                <label>49.</label>
                <mixed-citation publication-type="other">Ribeiro GHT, Neto PSGdM, Cavalcanti GDC, Tsang IR Lag selection for time series forecasting using Particle Swarm Optimization. In: The 2011 International Joint Conference on Neural Networks, 31 July-5 Aug. 2011 2011. pp 2437-2444.10.1109/IJCNN.2011.6033535</mixed-citation>
              </ref>
              <ref id="CR50">
                <label>50.</label>
                <mixed-citation publication-type="other">Gravetter FJ, Wallnau LB, Forzano LAB, Witnauer JE (2020) Essentials of statistics for the behavioral sciences. Cengage Learning</mixed-citation>
              </ref>
              <ref id="CR51">
                <label>51.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bland</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Altman</surname>
                      <given-names>DG</given-names>
                    </name>
                  </person-group>
                  <source>Measurement error. BMJ (Clinical research ed.)</source>
                  <year>1996</year>
                  <volume>313</volume>
                  <issue>7059</issue>
                  <fpage>744</fpage>
                  <pub-id pub-id-type="doi">10.1136/bmj.313.7059.744</pub-id>
                </element-citation>
              </ref>
              <ref id="CR52">
                <label>52.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Groeneveld</surname>
                      <given-names>RA</given-names>
                    </name>
                    <name>
                      <surname>Meeden</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>Measuring skewness and kurtosis</article-title>
                  <source>J Royal Statistical Soc: Series D (The Statistician)</source>
                  <year>1984</year>
                  <volume>33</volume>
                  <issue>4</issue>
                  <fpage>391</fpage>
                  <lpage>399</lpage>
                </element-citation>
              </ref>
              <ref id="CR53">
                <label>53.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kim</surname>
                      <given-names>HY</given-names>
                    </name>
                  </person-group>
                  <article-title>Statistical notes for clinical researchers: assessing normal distribution (2) using skewness and kurtosis</article-title>
                  <source>Restorative dentistry &amp; endodontics</source>
                  <year>2013</year>
                  <volume>38</volume>
                  <issue>1</issue>
                  <fpage>52</fpage>
                  <pub-id pub-id-type="doi">10.5395/rde.2013.38.1.52</pub-id>
                  <pub-id pub-id-type="pmid">23495371</pub-id>
                </element-citation>
              </ref>
              <ref id="CR54">
                <label>54.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Parmezan</surname>
                      <given-names>ARS</given-names>
                    </name>
                    <name>
                      <surname>Souza</surname>
                      <given-names>VM</given-names>
                    </name>
                    <name>
                      <surname>Batista</surname>
                      <given-names>GE</given-names>
                    </name>
                  </person-group>
                  <article-title>Evaluation of statistical and machine learning models for time series prediction: identifying the state-of-the-art and the best conditions for the use of each model</article-title>
                  <source>Inform Sci</source>
                  <year>2019</year>
                  <volume>484</volume>
                  <fpage>302</fpage>
                  <lpage>337</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.ins.2019.01.076</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
